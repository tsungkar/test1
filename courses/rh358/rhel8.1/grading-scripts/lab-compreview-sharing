#!/bin/bash
#
# Copyright 2020 Red Hat, Inc.
#
# NAME
#     compreview-sharing - grading script for RH358 Configuring File Sharing
#                           and Printers lab
#
# SYNOPSIS
#     compreview-sharing {start|grade|finish}
#
#        start   - prepare the system for starting the lab
#        grade   - perform evaluation steps on the system
#        finish  - perform post-lab cleanup
#
# DESCRIPTION
#     This script, based on singular argument, either does start, grading,
#     or finish for the Configuring File Sharing and Printers lab.
#
# CHANGELOG
#   * Tue Jun 23 2020 Herve Quatremain <hquatrem@redhat.com>
#   - original code


PATH=/usr/bin:/bin:/usr/sbin:/sbin

# Initialize and set some variables
run_as_root='true'
target_a='servera'
target_c='serverc'
ippsample_server='serverd'
queue_name='office-printer'

# CUPS
ansible_dir='printing-review'
lab_playbook_dir='classroom/ansible'
lab_playbook_tar="${ansible_dir}.tar"
lab_playbook_start="${ansible_dir}-start.yml"
lab_playbook_finish="${ansible_dir}-finish.yml"

# Samba
sambamount_user='sambamount'
linux_user='manager1'
linux_group='managers'
smb_share_dir="/srv/${linux_group}"
smb_mount_point='/managers_reports'

# NFS
nfs_user='operator1'
nfs_group='operators'
nfs_share_dir="/srv/${nfs_group}"
nfs_mount_point='/operators_data'

# iSCSI
second_disk_device='vdb'
blockstore='rack1.disk1'
targetIQN='iqn.2014-06.com.example:rack1'
iscsi_mount='/data_prod'


# This defines which subcommands are supported (start, grade, etc.).
# Corresponding lab_COMMAND functions must be defined.
declare -a valid_commands=(start grade finish)


function disk_cleanup {
  for i in {5..1}
  do
    if ${ssh} ${target_c} "test -b /dev/${second_disk_device}$i"
    then
      ${ssh} ${target_c} "umount -l /dev/${second_disk_device}$i"
      ${ssh} ${target_c} "swapoff /dev/${second_disk_device}$i"
      ${ssh} ${target_c} "dd if=/dev/zero of=/dev/${second_disk_device}$i bs=512 count=10"
      ${ssh} ${target_c} "parted --script /dev/${second_disk_device} rm $i"
    fi
  done
  ${ssh} ${target_c} "wipefs --all --force /dev/${second_disk_device}"
  ${ssh} ${target_c} "test -b /dev/${second_disk_device} && dd if=/dev/zero of=/dev/${second_disk_device} bs=1024 count=2048"
}


function printer_cleanup {
  ${ssh} ${target_a} "lpstat -p | cut -d' ' -f 2 | xargs lpadmin -x ;
                      systemctl disable --now cups ;
                      yum -y remove cups cups-ipptool ;
                      rm -rf /etc/cups /var/log/cups /var/run/cups /var/spool/cups ;
                      firewall-cmd --remove-service=mdns --permanent ;
                      firewall-cmd --reload"
}


function lab_start {

  print_header 'Starting lab.'

  print_line "Preparing ${target_a} and ${target_c} for lab exercise work:"
  print_line
  host_reachable ${target_a} ${target_c} ${ippsample_server}


  #
  # CUPS preparation
  #


  pad ' · Downloading lab playbooks'
  cd /tmp
  cmd1="curl -O http://materials.example.com/${lab_playbook_dir}/${lab_playbook_tar}"
  cmd2="tar xf ${lab_playbook_tar}"
  if ${cmd1} && ${cmd2}
  then
    rm -f ${lab_playbook_tar}
    print_SUCCESS

    pad ' · Running lab preparation playbook'
    cd "/tmp/${ansible_dir}"
    if ansible-playbook ${lab_playbook_start} & spinner $! 3
    then
      cd
      rm -rf "/tmp/${ansible_dir}"
      print_SUCCESS
    else
      print_FAIL
    fi
  else
    print_FAIL
  fi


  #
  # NFS and SMB preparation
  #


  pad " · Backing up /etc/fstab on ${target_a}"
  if ${ssh} ${target_a} "test -s /var/tmp/lab-${problem_name}-fstab || cp /etc/fstab /var/tmp/lab-${problem_name}-fstab"
  then
    print_SUCCESS
  else
    print_FAIL
  fi

  pad " · Backing up /etc/samba/smb.conf on ${target_c}"
  if ${ssh} ${target_c} "test -s /var/tmp/lab-${problem_name}-smb.conf || cp /etc/samba/smb.conf /var/tmp/lab-${problem_name}-smb.conf"
  then
    print_SUCCESS
  else
    print_FAIL
  fi

  for target in ${target_a} ${target_c}
  do
    pad " · Creating the ${nfs_group} group on ${target}"
    ${ssh} ${target} "groupadd -g 2020 ${nfs_group}"
    print_SUCCESS

    pad " · Creating the ${nfs_user} user account on ${target}"
    ${ssh} ${target} "useradd -u 2040 -G ${nfs_group} ${nfs_user} ; echo redhat | passwd --stdin ${nfs_user}"
    print_SUCCESS
  done

  #
  # workstation
  #

  if [ -e "/home/student/${problem_name}" ]
  then
    pad " · Saving existing ~/${problem_name}"
    if mv /home/student/${problem_name} /home/student/${problem_name}.$(date +%m-%d-%H:%M:%S)
    then
      print_SUCCESS
    else
      print_FAIL
    fi
  fi

  rhel_install_ansible

  pad " · Installing the rhel-system-roles package on workstation"
  if yum install -y rhel-system-roles
  then
    print_SUCCESS
  else
    print_FAIL
  fi

  pad " · Deploying the Ansible files on workstation"
  mkdir -p /home/student/${problem_name}/{templates,solution}

  cat <<'EOF' > /home/student/${problem_name}/ansible.cfg
[defaults]
inventory=inventory
remote_user=devops
EOF

  cat <<EOF > /home/student/${problem_name}/inventory
[servers]
${target_c}.lab.example.com

[clients]
${target_a}.lab.example.com

[initiators]
${target_a}.lab.example.com
EOF

  cat <<EOF > /home/student/${problem_name}/solution/printer-create.yml
---
- name: Install CUPS and create a print queue
  hosts: ${target_a}.lab.example.com
  become: true
  vars:
    queue_name: "${queue_name}"
    device_uri: "ipp://serverc.lab.example.com:631/printers/rht-printer"

  tasks:
    - name: the package for creating print queues is installed
      yum:
        name: cups
        state: present

    - name: the printing service is running and enabled
      service:
        name: cups
        state: started
        enabled: yes

    - name: check if print queue already exists
      command: lpstat -p "{{ queue_name }}"
      register: cmdout
      ignore_errors: true
      changed_when: false

    - name: the print queue exists
      command: lpadmin -p "{{ queue_name }}" -v "{{ device_uri }}"
                       -m everywhere -E
      when: cmdout.rc != 0

    - name: check default printer
      command: lpstat -d
      register: curr_dest
      changed_when: false

    - name: the new print queue is the default queue
      command: lpadmin -d "{{ queue_name }}"
      when: curr_dest['stdout'] | regex_replace('^(.*):.') != queue_name
EOF


  cat <<EOF > /home/student/${problem_name}/printer-create.yml
---
- name: Install CUPS and create a print queue
  hosts: ${target_a}.lab.example.com
  become: true
  vars:
    queue_name: "${queue_name}"
    device_uri: "ipp://serverc.lab.example.com:631/printers/rht-printer"

  tasks:
    - name: the package for creating print queues is installed
      yum:
        name: #FIXME#
        state: present

    - name: the printing service is running and enabled
      service:
        name: #FIXME#
        state: started
        enabled: yes

    - name: check if print queue already exists
      command: lpstat -p "{{ queue_name }}"
      register: cmdout
      ignore_errors: true
      changed_when: false

    - name: the print queue exists
      #FIXME: declare the {{ queue_name}} queue with {{ device_uri }} for the
      #       IPP Everywhere printer. Enable the printer.
      command: #FIXME#
      when: cmdout.rc != 0

    - name: check default printer
      command: lpstat -d
      register: curr_dest
      changed_when: false

    - name: the new print queue is the default queue
      command: #FIXME: define the {{ queue_name }} queue as the default printer
      when: curr_dest['stdout'] | regex_replace('^(.*):.') != queue_name
EOF

  cat <<EOF > /home/student/${problem_name}/solution/printer-reject.yml
---
- name: Reject print jobs
  hosts: ${target_a}.lab.example.com
  become: true
  vars:
    queue_name: "${queue_name}"

  tasks:
    - name: check if print queue already exists
      command: lpstat -p "{{ queue_name }}"
      register: cmdout
      ignore_errors: true
      changed_when: false

    - name: print jobs are rejected
      command: cupsreject -r "Printer on Fire" "{{ queue_name }}"
      when: cmdout.rc == 0
EOF

  cat <<EOF > /home/student/${problem_name}/printer-reject.yml
---
- name: Reject print jobs
  hosts: ${target_a}.lab.example.com
  become: true
  vars:
    queue_name: "${queue_name}"

  tasks:
    - name: check if print queue already exists
      command: lpstat -p "{{ queue_name }}"
      register: cmdout
      ignore_errors: true
      changed_when: false

    - name: print jobs are rejected
      #FIXME: reject jobs for the {{ queue_name}} queue with the
      #       message: Printer on Fire
      command: #FIXME#
      when: cmdout.rc == 0
EOF

  cat <<EOF > /home/student/${problem_name}/solution/printer-accept.yml
---
- name: Accept print jobs
  hosts: ${target_a}.lab.example.com
  become: true
  vars:
    queue_name: "${queue_name}"

  tasks:
    - name: check if print queue already exists
      command: lpstat -p "{{ queue_name }}"
      register: cmdout
      ignore_errors: true
      changed_when: false

    - name: print jobs are accepted
      command: cupsaccept "{{ queue_name }}"
      when: cmdout.rc == 0
EOF

  cat <<EOF > /home/student/${problem_name}/printer-accept.yml
---
- name: Accept print jobs
  hosts: ${target_a}.lab.example.com
  become: true
  vars:
    queue_name: "${queue_name}"

  tasks:
    - name: check if print queue already exists
      command: lpstat -p "{{ queue_name }}"
      register: cmdout
      ignore_errors: true
      changed_when: false

    - name: print jobs are accepted
      #FIXME: accept jobs for the {{ queue_name}} queue
      command: #FIXME#
      when: cmdout.rc == 0
EOF

  cat <<EOF > /home/student/${problem_name}/solution/nfs_server.yml
---
- name: Share a directory with NFS
  hosts: ${target_c}.lab.example.com
  become: true
  vars:
    shared_dir: ${nfs_share_dir}

  tasks:
    - name: the package for NFS server is installed
      yum:
        name: nfs-utils
        state: present

    - name: the directory exists
      file:
        path: "{{ shared_dir }}"
        owner: root
        group: ${nfs_group}
        mode: '2770'
        state: directory

    - name: the directory is shared
      copy:
        content: "{{ shared_dir }} ${target_a}.lab.example.com(rw)\n"
        dest: /etc/exports.d/share.exports
        owner: root
        group: root
        mode: '0644'
      notify: reload exports

    - name: NFS is started and enabled
      service:
        name: nfs-server
        state: started
        enabled: yes

    - name: the firewall is opened for NFS
      firewalld:
        service: nfs
        state: enabled
        immediate: yes
        permanent: yes

  handlers:
    - name: reload exports
      service:
        name: nfs-server
        state: reloaded
EOF

  cat <<EOF > /home/student/${problem_name}/nfs_server.yml
---
- name: Share a directory with NFS
  hosts: ${target_c}.lab.example.com
  become: true
  vars:
    shared_dir: ${nfs_share_dir}

  tasks:
    - name: the package for NFS server is installed
      yum:
        name: #FIXME: install the required package for an NFS server
        state: present

    - name: the directory exists
      file:
        path: "{{ shared_dir }}"
        owner: root
        group: ${nfs_group}
        mode: '2770'
        state: directory

    - name: the directory is shared
      copy:
        #FIXME: declare the {{ shared_dir }} directory as an NFS share.
        #       Only ${target_a}.lab.example.com must be able to access the share.
        #       ${target_a} has read/write access to the share.
        #       The root user on ${target_a} must have no access to the share.
        content: "{{ shared_dir }} #FIXME#(#FIXME#)\n"
        dest: /etc/exports.d/share.exports
        owner: root
        group: root
        mode: '0644'
      notify: reload exports

    - name: NFS is started and enabled
      service:
        name: #FIXME: the NFS server service must be started and enabled
        state: started
        enabled: yes

    - name: the firewall is opened for NFS
      firewalld:
        service: #FIXME: configure the firewall to allow NFS traffic
        state: enabled
        immediate: yes
        permanent: yes

  handlers:
    - name: reload exports
      service:
        name: #FIXME: the NFS server service must be reloaded
        state: reloaded
EOF

  cat <<EOF > /home/student/${problem_name}/solution/nfs_client.yml
---
- name: Access an NFS share
  hosts: ${target_a}.lab.example.com
  become: true
  vars:
    shared_dir: ${nfs_share_dir}
    mount_point: ${nfs_mount_point}

  tasks:
    - name: the package for NFS client is installed
      yum:
        name: nfs-utils
        state: present

    - name: the NFS share is mounted and in /etc/fstab
      mount:
        path: "{{ mount_point }}"
        src: ${target_c}.lab.example.com:{{ shared_dir }}
        state: mounted
        fstype: nfs
EOF

  cat <<EOF > /home/student/${problem_name}/nfs_client.yml
---
- name: Access an NFS share
  hosts: ${target_a}.lab.example.com
  become: true
  vars:
    shared_dir: ${nfs_share_dir}
    mount_point: ${nfs_mount_point}

  tasks:
    - name: the package for NFS client is installed
      yum:
        name: #FIXME: install the required package for an NFS client
        state: present

    - name: the NFS share is mounted and in /etc/fstab
      mount:
        #FIXME: persistently mount {{ shared_dir }} from ${target_c}.lab.example.com
        #       into the {{ mount_point }} directory
        path: #FIXME#
        src: #FIXME#
        state: mounted
        fstype: #FIXME#
EOF

  cat <<EOF > /home/student/${problem_name}/smb_vars.yml
---
shared_dir: ${smb_share_dir}
share_name: managerdata
mount_point: ${smb_mount_point}

# User account for mounting the share
samba_usermount: ${sambamount_user}
samba_passmount: redhat

allowed_group: ${linux_group}
samba_user: ${linux_user}
samba_user_password: redhat
EOF

  cat <<'EOF' > /home/student/${problem_name}/solution/smb.conf.j2
[global]
        workgroup = MANAGERGROUP
        server min protocol = SMB3
        smb encrypt = required

[{{ share_name }}]
        path = {{ shared_dir }}
        valid users = {{ samba_usermount }}, @{{ allowed_group }}
        write list = @{{ allowed_group }}
EOF

  cat <<'EOF' > /home/student/${problem_name}/templates/smb.conf.j2
# See smb.conf.example for a more detailed config file or
# read the smb.conf manpage.
# Run 'testparm' to verify the config is correct after
# you modified it.

[global]
        workgroup = SAMBA
        security = user

        passdb backend = tdbsam

        printing = cups
        printcap name = cups
        load printers = yes
        cups options = raw

[homes]
        comment = Home Directories
        valid users = %S, %D%w%S
        browseable = No
        read only = No
        inherit acls = Yes

[printers]
        comment = All Printers
        path = /var/tmp
        printable = Yes
        create mask = 0600
        browseable = No

[print$]
        comment = Printer Drivers
        path = /var/lib/samba/drivers
        write list = @printadmin root
        force group = @printadmin
        create mask = 0664
        directory mask = 0775
EOF

  cat <<'EOF' > /home/student/${problem_name}/solution/smb_server.yml
---
- name: Share a directory with SMB
  hosts: serverc.lab.example.com
  become: true
  vars_files:
    - smb_vars.yml

  tasks:
    - name: the package for a Samba server is installed
      yum:
        name: samba
        state: present

    - name: the Linux group for Samba users exists
      group:
        name: "{{ allowed_group }}"

    - name: the Linux user for Samba exists
      user:
        name: "{{ samba_user }}"
        password: "{{ samba_user_password | password_hash('sha512', 'secretsalt') }}"
        groups:
          - "{{ allowed_group }}"

    - name: the Linux user is in Samba database
      command: smbpasswd -s -a {{ samba_user }}
      args:
        stdin: "{{ samba_user_password }}\n{{ samba_user_password }}"

    - name: the Linux user for Samba mount exists
      user:
        name: "{{ samba_usermount }}"
        shell: /sbin/nologin
        create_home: no
        system: yes

    - name: the Samba user for Samba mount exists
      command: smbpasswd -s -a {{ samba_usermount }}
      args:
        stdin: "{{ samba_passmount }}\n{{ samba_passmount }}"

    - name: the directory exists
      file:
        path: "{{ shared_dir }}"
        owner: "{{ samba_usermount }}"
        group: "{{ allowed_group }}"
        mode: '2570'
        state: directory
        setype: samba_share_t

    - name: the directory is shared
      template:
        src: templates/smb.conf.j2
        dest: /etc/samba/smb.conf
        owner: root
        group: root
        mode: '0644'
        setype: samba_etc_t
      notify: reload smb

    - name: the SMB service is started and enabled
      service:
        name: smb
        state: started
        enabled: yes

    - name: the firewall is opened for SMB
      firewalld:
        service: samba
        state: enabled
        immediate: yes
        permanent: yes

  handlers:
    - name: reload smb
      service:
        name: smb
        state: reloaded
EOF

  cat <<EOF > /home/student/${problem_name}/smb_server.yml
---
- name: Share a directory with SMB
  hosts: ${target_c}.lab.example.com
  become: true
  vars_files:
    - smb_vars.yml

  tasks:
    - name: the package for a Samba server is installed
      yum:
        name: #FIXME: install the required package for a Samba server
        state: present

    - name: the Linux group for Samba users exists
      group:
        name: "{{ allowed_group }}"

    - name: the Linux user for Samba exists
      user:
        name: "{{ samba_user }}"
        password: "{{ samba_user_password | password_hash('sha512', 'secretsalt') }}"
        groups:
          - "{{ allowed_group }}"

    - name: the Linux user is in Samba database
      command: smbpasswd -s -a {{ samba_user }}
      args:
        stdin: "{{ samba_user_password }}\n{{ samba_user_password }}"

    - name: the Linux user for Samba mount exists
      user:
        name: "{{ samba_usermount }}"
        shell: /sbin/nologin
        create_home: no
        system: yes

    - name: the Samba user for Samba mount exists
      command: smbpasswd -s -a {{ samba_usermount }}
      args:
        stdin: "{{ samba_passmount }}\n{{ samba_passmount }}"

    - name: the directory exists
      file:
        #FIXME: create the ${smb_share_dir} directory as follows:
        #             Directory ownership: ${sambamount_user}
        #       Directory group ownership: ${linux_group}
        #                    Owner access: read
        #                    Group access: read/write
        #              Other users access: none
        #       All contents created in the directory must automatically
        #       belong to the ${linux_group} group.
        #       Set the correct SELinux context type.
        path: #FIXME#
        owner: #FIXME#
        group: #FIXME#
        mode: #FIXME#
        state: directory
        setype: #FIXME#


    - name: the directory is shared
      template:
        #FIXME: edit templates/smb.conf.j2 to declare the ${smb_share_dir}
        #       directory as an SMB share as follows:
        #                          Work group: MANAGERGROUP
        #        SMB minimum protocol version: 3
        #                  Traffic encryption: Always required
        #                          Share name: managerdata
        #                   Access allowed to: ${sambamount_user} and the
        #                                      members of the ${linux_group} group
        #                   Read/write access: Members of the ${linux_group} group
        #       For your convenience, the default Samba configuration file is
        #       available under the templates/ directory but must be updated
        #       according to the preceding requirements.
        src: templates/smb.conf.j2
        dest: /etc/samba/smb.conf
        owner: root
        group: root
        mode: '0644'
        setype: samba_etc_t
      notify: reload smb

    - name: the SMB service is started and enabled
      service:
        name: #FIXME: the service must be started and enabled
        state: started
        enabled: yes

    - name: the firewall is opened for SMB
      firewalld:
        service: #FIXME: configure the firewall to allow SMB traffic
        state: enabled
        immediate: yes
        permanent: yes

  handlers:
    - name: reload smb
      service:
        name: #FIXME: the service must be reloaded
        state: reloaded
EOF

  cat <<'EOF' > /home/student/${problem_name}/solution/smb_client.yml
---
- name: Access an SMB share
  hosts: servera.lab.example.com
  become: true
  vars_files:
   - smb_vars.yml

  tasks:
    - name: the package to mount SMB shares is installed
      yum:
        name: cifs-utils
        state: present

    - name: the Linux group for Samba users exists
      group:
        name: "{{ allowed_group }}"

    - name: the Linux user for Samba exists
      user:
        name: "{{ samba_user }}"
        password: "{{ samba_user_password | password_hash('sha512', 'secretsalt') }}"
        groups:
          - "{{ allowed_group }}"

    - name: the credential file exists
      copy:
        content: "username={{ samba_usermount }}\n\
                  password={{ samba_passmount }}\n"
        dest: /etc/samba/creds.txt
        owner: root
        group: root
        mode: '0600'
      no_log: true

    - name: the SMB share is mounted
      mount:
        path: "{{ mount_point }}"
        src: "//serverc.lab.example.com/{{ share_name }}"
        opts: "credentials=/etc/samba/creds.txt,multiuser,seal"
        state: mounted
        fstype: cifs
EOF

  cat <<EOF > /home/student/${problem_name}/smb_client.yml
---
- name: Access an SMB share
  hosts: ${target_a}.lab.example.com
  become: true
  vars_files:
   - smb_vars.yml

  tasks:
    - name: the package to mount SMB shares is installed
      yum:
        name: #FIXME: install the required package to mount SMB shares
        state: present

    - name: the Linux group for Samba users exists
      group:
        name: "{{ allowed_group }}"

    - name: the Linux user for Samba exists
      user:
        name: "{{ samba_user }}"
        password: "{{ samba_user_password | password_hash('sha512', 'secretsalt') }}"
        groups:
          - "{{ allowed_group }}"

    - name: the credential file exists
      copy:
        #FIXME: create the /etc/samba/creds.txt credential file for the
        #       multiuser mount option.
        #       Use the ${sambamount_user} user account with redhat for
        #       the password.
        content: "#FIXME#={{ samba_usermount }}\n\
                  #FIXME#={{ samba_passmount }}\n"
        dest: #FIXME#
        owner: root
        group: root
        mode: '0600'
      no_log: true

    - name: the SMB share is mounted
      mount:
        #FIXME: persistently mount the managerdata SMB share from
        #       ${target_c}.lab.example.com into the ${smb_mount_point}
        #       directory.
        #       Use the credential file, the multiuser option, and activate
        #       traffic encryption.
        path: #FIXME#
        src: #FIXME#
        opts: #FIXME#
        state: mounted
        fstype: #FIXME#
EOF

  cat <<EOF > /home/student/${problem_name}/templates/initiatorname.iscsi.j2
InitiatorName=iqn.2014-06.com.example:{{ ansible_facts['hostname'] }}
EOF

  cat <<EOF > /home/student/${problem_name}/initiator.yml
---
- name: Ensure ${iscsi_mount} is mounted from ${target_c} iSCSI target
  hosts: initiators
  become: true

  tasks:
    - name: the iSCSI initiator software is installed
      yum:
        name: #FIXME: install the required package
        state: present

    - name: the IQN is set for the initiator
      copy:
        #FIXME: set the initiator IQN to iqn.2014-06.com.example:${target_a}
        dest: #FIXME#
        content: "#FIXME#=iqn.2014-06.com.example:{{ ansible_facts['hostname'] }}\n"
        mode: '644'
        owner: root
        group: root
      notify: restart iscsid

    # Forces the handler to run so that the iscsid service is restarted
    # and is aware of the new initiator IQN
    - meta: flush_handlers

    - name: the iSCSI target is discovered and available
      open_iscsi:
        #FIXME: discover and log into the target.
        #       Target IQN: ${targetIQN}
        #       Portal: 172.25.250.12 (port 3260)
        portal: #FIXME#
        port: #FIXME#
        target: #FIXME#
        discover: yes
        login: yes
      register: target

    - name: display the discovered devices
      debug:
        msg: The new device is {{ target['devicenodes'][0] }}

    - name: the new device is formatted and mounted under ${iscsi_mount}
      include_role:
        name: rhel-system-roles.storage
      vars:
        #FIXME: mount target['devicenodes'][0] into ${iscsi_mount}
        #       If the device is not yet formatted in ext4, format it.
        #       Use the proper mount option for an iSCSI disk.
        storage_volumes:
          - name: devdata
            state: present
            type: disk
            disks:
              - "{{ target['devicenodes'][0] }}"
            mount_point: #FIXME#
            fs_type: #FIXME#
            mount_options: #FIXME#

  handlers:
    - name: restart iscsid
      service:
        name: iscsid
        state: restarted
EOF

  cat <<EOF > /home/student/${problem_name}/solution/initiator.yml
---
- name: Ensure ${iscsi_mount} is mounted from ${target_c} iSCSI target
  hosts: initiators
  become: true

  tasks:
    - name: the iSCSI initiator software is installed
      yum:
        name: iscsi-initiator-utils
        state: present

    - name: the IQN is set for the initiator
      copy:
        dest: /etc/iscsi/initiatorname.iscsi
        content: "InitiatorName=iqn.2014-06.com.example:{{ ansible_facts['hostname'] }}\n"
        mode: '644'
        owner: root
        group: root
      notify: restart iscsid

    # Forces the handler to run so that the iscsid service is restarted
    # and is aware of the new initiator IQN
    - meta: flush_handlers

    - name: the iSCSI target is discovered and available
      open_iscsi:
        portal: 172.25.250.12
        port: '3260'
        target: ${targetIQN}
        discover: yes
        login: yes
      register: target

    - name: display the discovered devices
      debug:
        msg: The new device is {{ target['devicenodes'][0] }}

    - name: the new device is formatted and mounted under ${iscsi_mount}
      include_role:
        name: rhel-system-roles.storage
      vars:
        storage_volumes:
          - name: devdata
            state: present
            type: disk
            disks:
              - "{{ target['devicenodes'][0] }}"
            mount_point: ${iscsi_mount}
            fs_type: ext4
            mount_options: '_netdev'

  handlers:
    - name: restart iscsid
      service:
        name: iscsid
        state: restarted
EOF

  cat <<EOF > /home/student/${problem_name}/solution/target.yml
---
- name: Ensure the iSCSI target is prepared
  hosts: ${target_c}.lab.example.com
  become: true

  tasks:
    - name: the target command line tool is installed
      yum:
        name: targetcli
        state: present

    - name: the target service is started and enabled
      service:
        name: target
        state: started
        enabled: yes

    - name: the firewall is opened for iSCSI
      firewalld:
        service: iscsi-target
        state: enabled
        immediate: yes
        permanent: yes

    - name: check if the target already exists
      command: targetcli ls /iscsi/${targetIQN}
      register: cmdout
      ignore_errors: true
      changed_when: false

    - name: the iSCSI target is prepared
      shell: |
        targetcli /backstores/block create ${blockstore} /dev/${second_disk_device}1
        targetcli /iscsi create ${targetIQN}
        targetcli /iscsi/${targetIQN}/tpg1/acls create iqn.2014-06.com.example:${target_a}
        targetcli /iscsi/${targetIQN}/tpg1/luns create /backstores/block/${blockstore}
        targetcli /iscsi/${targetIQN}/tpg1/portals delete 0.0.0.0 3260
        targetcli /iscsi/${targetIQN}/tpg1/portals create 172.25.250.12 3260
        targetcli saveconfig
      when: cmdout.rc != 0
EOF

  cat <<EOF > /home/student/${problem_name}/target.yml
---
- name: Ensure the iSCSI target is prepared
  hosts: ${target_c}.lab.example.com
  become: true

  tasks:
    - name: the target command line tool is installed
      yum:
        name: #FIXME: install the required package
        state: present

    - name: the target service is started and enabled
      service:
        name: #FIXME: start and enable the service
        state: started
        enabled: yes

    - name: the firewall is opened for iSCSI
      firewalld:
        service: #FIXME: enable the firewalld service for an iSCSI target
        state: enabled
        immediate: yes
        permanent: yes

    - name: check if the target already exists
      command: targetcli ls /iscsi/${targetIQN}
      register: cmdout
      ignore_errors: true
      changed_when: false

    - name: the iSCSI target is prepared
      shell: |
        targetcli /backstores/block create ${blockstore} /dev/${second_disk_device}1
        targetcli /iscsi create ${targetIQN}
        targetcli /iscsi/${targetIQN}/tpg1/acls create iqn.2014-06.com.example:${target_a}
        targetcli /iscsi/${targetIQN}/tpg1/luns create /backstores/block/${blockstore}
        targetcli /iscsi/${targetIQN}/tpg1/portals delete 0.0.0.0 3260
        targetcli /iscsi/${targetIQN}/tpg1/portals create 172.25.250.12 3260
        targetcli saveconfig
      when: cmdout.rc != 0
EOF

  cat <<EOF > /home/student/${problem_name}/solution/initiator_cleanup.yml
---
- name: Ensure ${iscsi_mount} is not mounted
  hosts: initiators
  become: true

  tasks:
    - name: the ${iscsi_mount} file system is unmounted
      mount:
        path: ${iscsi_mount}
        state: absent

    - name: the iSCSI target is disconnected
      open_iscsi:
        portal: 172.25.250.12
        port: '3260'
        target: ${targetIQN}
        discover: no
        login: no
        auto_node_startup: no
      ignore_errors: yes

    - name: the iscsi-initiator-utils package is not installed
      yum:
        name: iscsi-initiator-utils
        state: absent

    - name: the iSCSI configuration files are not present
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/iscsi
        - /var/lib/iscsi
        - /var/lock/iscsi
EOF
  print_SUCCESS
  chown -R student: /home/student/${problem_name}


  #
  # iSCSI preparation
  #

  pad " · Deploying the help file on ${target_a}"
  if ${ssh} ${target_a} "mkdir -p /root/${problem_name}"
  then
    cat <<EOF | ${ssh} ${target_a} "cat > /root/${problem_name}/iscsi.txt"
     Initiator IQN: iqn.2014-06.com.example:${target_a}
        Target IQN: ${targetIQN}
    Portal address: 172.25.250.12 (port 3260)
       Mount point: ${iscsi_mount}
EOF
    print_SUCCESS
  else
    print_FAIL
  fi

  #
  # serverc: preparing the block device
  #

  pad " · Cleaning up spare disk ${second_disk_device} on ${target_c}"
  disk_cleanup
  print_SUCCESS

  pad " · Preparing ${second_disk_device} on ${target_c}"
  if ${ssh} ${target_c} parted -s /dev/${second_disk_device} mklabel msdos mkpart primary 0 2G
  then
    ${ssh} ${target_c} partprobe /dev/${second_disk_device}
    print_SUCCESS
  else
    print_FAIL
  fi

  pad " · Deploying the help file on ${target_c}"
  if ${ssh} ${target_c} "mkdir -p /root/${problem_name}"
  then
    cat <<EOF | ${ssh} ${target_c} "cat > /root/${problem_name}/iscsi.txt"
      Block device: /dev/${second_disk_device}1
Backing store name: ${blockstore}
        Target IQN: ${targetIQN}
     Initiator IQN: iqn.2014-06.com.example:${target_a}
    Portal address: 172.25.250.12 (port 3260)
EOF
    print_SUCCESS
  else
    print_FAIL
  fi

  print_line
}


function lab_grade {

  print_header "Grading the student's work on ${target_a} and ${target_c}:"
  host_reachable ${target_a} ${target_c} ${ippsample_server}

  TMP_FILE=$(mktemp)

  pad " · Removing CUPS from ${target_a}"
  printer_cleanup
  print_SUCCESS

  pad " · The printer-create.yml playbook runs from workstation"
  su student -c "cd /home/student/${problem_name} && ansible-playbook printer-create.yml"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The ${queue_name} print queue exists on ${target_a}"
  ${ssh} ${target_a} "lpoptions -p ${queue_name}" > "${TMP_FILE}"
  if grep -wiq "${queue_name}" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The ${queue_name} jobs go to the correct destination"
  if grep -iq "ipp://serverc.*:631/printers/rht-printer" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The ${queue_name} print queue uses the correct PPD"
  if grep -iq "Everywhere Basic Printer Simulator" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${queue_name} is the default print queue on ${target_a}"
  ${ssh} ${target_a} "lpstat -d" | grep -wiq "${queue_name}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The printer-reject.yml playbook runs from workstation"
  su student -c "cd /home/student/${problem_name} && ansible-playbook printer-reject.yml"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${queue_name} is rejecting jobs on ${target_a}"
  ${ssh} ${target_a} "lpoptions -p ${queue_name}" | grep -wiq printer-is-accepting-jobs=false
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${queue_name} rejecting message is set on ${target_a}"
  ${ssh} ${target_a} "lpstat -p ${queue_name}" | grep -iq "Printer[[:space:]]*on[[:space:]]*Fire"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The printer-accept.yml playbook runs from workstation"
  su student -c "cd /home/student/${problem_name} && ansible-playbook printer-accept.yml"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${queue_name} is accepting jobs on ${target_a}"
  ${ssh} ${target_a} "lpoptions -p ${queue_name}" | grep -wiq printer-is-accepting-jobs=true
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · Removing CUPS from ${target_a}"
  printer_cleanup
  print_SUCCESS

  if ! ${ssh} ${target_c} "rpm -q nfs-utils"
  then
    pad " · The package for NFS server is installed on ${target_c}"
    print_FAIL
  fi

  pad " · The NFS service is running on ${target_c}"
  if ${ssh} ${target_c} "systemctl is-active nfs-server"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The NFS service is enabled on ${target_c}"
  if ${ssh} ${target_c} "systemctl is-enabled nfs-server"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · Firewall port for NFS is opened on ${target_c}"
  ${ssh} ${target_c} "firewall-cmd --list-all" | grep -wq -e 2049 -e nfs
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${nfs_share_dir} exists on ${target_c}"
  ${ssh} ${target_c} "stat --format 'd=%F g=%G a=%A' ${nfs_share_dir}" > "${TMP_FILE}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${nfs_share_dir} is a directory"
  if grep -wq d=directory "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${nfs_share_dir} group is ${nfs_group}"
  if grep -wq g=${nfs_group} "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${nfs_share_dir} group has read/write access"
  if grep -q " a=....rw" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${nfs_share_dir} other users have no access"
  if grep -q " a=.......---" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${nfs_share_dir} contents automatically belong to the group"
  if grep -iq " a=......s" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${nfs_share_dir} is shared with NFS on ${target_c}"
  ${ssh} ${target_c} "exportfs -r ; exportfs -v | grep ${nfs_share_dir}" > "${TMP_FILE}"
  if grep -wq "^${nfs_share_dir}" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${target_a} can access the NFS share on ${target_c}"
  if grep -wiq "${target_a}" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${target_a} has read/write access to the NFS share on ${target_c}"
  if grep -w rw "${TMP_FILE}" | grep -wiq "${target_a}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · root on ${target_a} has no access to the NFS share on ${target_c}"
  if grep -w root_squash "${TMP_FILE}"  | grep -wiq "${target_a}"
  then
    print_PASS
  else
    print_FAIL
  fi

  if ! ${ssh} ${target_a} "rpm -q nfs-utils"
  then
    pad " · The package for NFS client is installed on ${target_a}"
    print_FAIL
  fi

  pad " · The ${nfs_mount_point} directory exists on ${target_a}"
  if ${ssh} ${target_a} "test -d ${nfs_mount_point}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The NFS share is mounted under ${nfs_mount_point}"
  ${ssh} ${target_a} mount | grep -Eqi "(${target_c}|172.25.250.12).* ${nfs_mount_point} .*nfs"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The NFS share is declared in /etc/fstab on ${target_a}"
  ${ssh} ${target_a} cat /etc/fstab | grep -Eqi "(${target_c}|172.25.250.12).*${nfs_mount_point}.*nfs"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The package for Samba server is installed on ${target_c}"
  if ${ssh} ${target_c} "rpm -q samba"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The SMB service is running on ${target_c}"
  if ${ssh} ${target_c} "systemctl is-active smb"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The SMB service is enabled on ${target_c}"
  if ${ssh} ${target_c} "systemctl is-enabled smb"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · Firewall ports for SMB are opened on ${target_c}"
  ${ssh} ${target_c} "firewall-cmd --list-all" | grep -wq -e 445 -e samba
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The ${linux_group} group exists on ${target_c}"
  ${ssh} ${target_c} "grep -w ${linux_group} /etc/group" > "${TMP_FILE}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The ${linux_user} user account exists on ${target_c}"
  ${ssh} ${target_c} "grep -w ${linux_user} /etc/passwd" > "${TMP_FILE}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${linux_user} is a member of ${linux_group} on ${target_c}"
  ${ssh} ${target_c} "grep '${linux_group}.*${linux_user}' /etc/group" > "${TMP_FILE}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${linux_user} is defined in Samba database on ${target_c}"
  if ${ssh} ${target_c} "yum -y install samba-common-tools ; pdbedit -L | grep -wq ${linux_user}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The ${sambamount_user} user account exists on ${target_c}"
  ${ssh} ${target_c} "grep -w ${sambamount_user} /etc/passwd" > "${TMP_FILE}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${sambamount_user} cannot login on ${target_c}"
  if grep -qw -e nologin -e false -e true "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${sambamount_user} is defined in Samba database on ${target_c}"
  if ${ssh} ${target_c} "pdbedit -L | grep -wq ${sambamount_user}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${smb_share_dir} exists on ${target_c}"
  ${ssh} ${target_c} "stat --format 'd=%F g=%G a=%A s=%C u=%U' ${smb_share_dir}" > "${TMP_FILE}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${smb_share_dir} is a directory"
  if grep -wq d=directory "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${smb_share_dir} owner is ${sambamount_user}"
  if grep -wq u=${sambamount_user} "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${smb_share_dir} group is ${linux_group}"
  if grep -wq g=${linux_group} "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${smb_share_dir} group has read/write access"
  if grep -q " a=....rw" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${smb_share_dir} other users have no access"
  if grep -q " a=.......---" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${smb_share_dir} contents automatically belong to the group"
  if grep -iq " a=......s" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The SMB work group is MANAGERGROUP on ${target_c}"
  ${ssh} ${target_c} cat /etc/samba/smb.conf > "${TMP_FILE}"
  if grep -qEi "workgroup\s*=\s*MANAGERGROUP\s*$" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The SMB min protocol version is 3 on ${target_c}"
  if grep -qEi "server\s+min\s+protocol\s*=\s*SMB3" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · SMB traffic encryption is required on ${target_c}"
  if grep -qEi "smb\s+encrypt\s*=\s*(required|mandatory)" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The managerdata SMB share is declared on ${target_c}"
  if grep -qEi "\[\s*managerdata\s*\]" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · managerdata shares the ${smb_share_dir} directory"
  if grep -qEi "path\s*=\s*${smb_share_dir}" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The ${linux_group} group exists on ${target_a}"
  ${ssh} ${target_a} "grep -w ${linux_group} /etc/group" > "${TMP_FILE}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The ${linux_user} user account exists on ${target_a}"
  ${ssh} ${target_a} "grep -w ${linux_user} /etc/passwd" > "${TMP_FILE}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${linux_user} is a member of ${linux_group} on ${target_a}"
  ${ssh} ${target_a} "grep '${linux_group}.*${linux_user}' /etc/group" > "${TMP_FILE}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The package to mount SMB shares is installed on ${target_a}"
  if ${ssh} ${target_a} "rpm -q cifs-utils"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The /etc/samba/creds.txt credential file exists on ${target_a}"
  ${ssh} ${target_a} cat /etc/samba/creds.txt > "${TMP_FILE}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The ${sambamount_user} user is used for SMB credentials"
  if grep -qEi "username\s*=\s*${sambamount_user}" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The password is redhat in the credential file on ${target_a}"
  if grep -qEi "password\s*=\s*redhat" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The ${smb_mount_point} directory exists on ${target_a}"
  if ${ssh} ${target_a} "test -d ${smb_mount_point}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The SMB share is mounted under ${smb_mount_point}"
  ${ssh} ${target_a} mount | grep -Ei "(${target_c}|172.25.250.12).* ${smb_mount_point} .*cifs" > "${TMP_FILE}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The SMB share is mounted with the multiuser option"
  if grep -qw multiuser "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The SMB share is mounted with traffic encryption"
  if grep -qw seal "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The SMB share is declared in /etc/fstab on ${target_a}"
  ${ssh} ${target_a} cat /etc/fstab | grep -Ei "(${target_c}|172.25.250.12).*${smb_mount_point}.*cifs" > "${TMP_FILE}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The share is using the /etc/samba/creds.txt credential file"
  if grep -q "credentials=/etc/samba/creds.txt" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The targetcli package is installed on ${target_c}"
  if ${ssh} ${target_c} "rpm -q targetcli"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The target service is running on ${target_c}"
  if ${ssh} ${target_c} "systemctl is-active target"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The target service is enabled on ${target_c}"
  if ${ssh} ${target_c} "systemctl is-enabled target"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · Firewall port 3260 is opened on ${target_c}"
  ${ssh} ${target_c} "firewall-cmd --list-all" | grep -wq -e 3260 -e iscsi-target
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The target blockstore is ${blockstore}"
  TMP_FILE=$(mktemp)
  ${ssh} ${target_c} "targetcli ls" > "${TMP_FILE}"
  if grep -A 3 " block " "${TMP_FILE}" | grep -wq "${blockstore}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The blockstore device is ${second_disk_device}1"
  if grep -q "/dev/${second_disk_device}1 " "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The target IQN is ${targetIQN}"
  if grep -wq "${targetIQN}" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The target has an ACL for iqn.2014-06.com.example:${target_a}"
  if grep -A 1000 " acls " "${TMP_FILE}" | grep -wq "iqn.2014-06.com.example:${target_a}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The LUN is mapped to the ${blockstore} blockstore"
  if grep -A 1000 " luns " "${TMP_FILE}" | grep -wq "block/${blockstore}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The portal is set to 172.25.250.12:3260"
  if grep -wq "172.25.250.12:3260" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The target configuration is saved"
  if ${ssh} ${target_c} "test -s /etc/target/saveconfig.json"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The iscsi-initiator-utils package is installed on ${target_a}"
  if ${ssh} ${target_a} "rpm -q iscsi-initiator-utils"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The initiator IQN is iqn.2014-06.com.example:${target_a}"
  if ${ssh} ${target_a} grep iqn.2014-06.com.example:${target_a} /etc/iscsi/initiatorname.iscsi
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${target_a} is using the 172.25.250.12:3260 portal"
  ${ssh} ${target_a} "iscsiadm -m session -P 3" > "${TMP_FILE}"
  if grep -q "Persistent Portal:.*172.25.250.12:3260" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · ${target_a} is connected to ${targetIQN}"
  if grep -qw "${targetIQN}" "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The ${iscsi_mount} directory exists"
  if ${ssh} ${target_a} "test -d ${iscsi_mount}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The file system is mounted on ${iscsi_mount}"
  DEVICE=$(awk '/Attached scsi disk/ { print $4 }' "${TMP_FILE}")
  if [ -z "${DEVICE}" ]
  then
    DEVICE='unkwonw'
  fi
  if ${ssh} ${target_a} "df ${iscsi_mount} | grep -w ${DEVICE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The file system is declared in /etc/fstab"
  ${ssh} ${target_a} "grep ${iscsi_mount} /etc/fstab" > "${TMP_FILE}"
  if [ $? -eq 0 ]
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The correct mount option is used in /etc/fstab"
  if grep -qw _netdev "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  pad " · The correct file system type is used in /etc/fstab"
  if grep -qw ext4 "${TMP_FILE}"
  then
    print_PASS
  else
    print_FAIL
  fi

  rm -rf "${TMP_FILE}"

  # Overall grade
  print_line
  pad 'Overall lab grade'
  if [[ ${fail_count} -eq 0 ]]
  then
    print_PASS
  else
    print_FAIL
  fi

  print_line
}


function lab_finish {

  print_header "Cleaning up the lab on ${target_a} and ${target_c}:"
  host_reachable ${target_a} ${target_c} ${ippsample_server}

  pad " · Unmounting the shares on ${target_a}"
  ${ssh} ${target_a} "umount -afl -t cifs ; umount -afl -t nfs ; umount -afl -t nfs4 ; umount -afl -t ext4 ; sleep 5"
  print_SUCCESS

  pad " · Removing the mount points from ${target_a}"
  ${ssh} ${target_a} "rm -rf ${smb_mount_point} ${nfs_mount_point}" &
  print_SUCCESS

  pad " · Unmounting ${iscsi_mount} from ${target_a}"
  TMP_FILE=$(mktemp)
  ${ssh} ${target_a} "iscsiadm -m session -P 3" > "${TMP_FILE}"
  DEVICE=$(awk '/Attached scsi disk/ { print $4 }' "${TMP_FILE}")
  TARGET=$(awk '/Target:/ { print $2 }' "${TMP_FILE}")
  PORTAL=$(awk '/Persistent Portal:/ { print $NF }' "${TMP_FILE}")
  rm -f "${TMP_FILE}"
  ${ssh} ${target_a} "umount ${iscsi_mount}"
  if [ -n "${DEVICE}" ]
  then
    for i in {5..1}
    do
      if ${ssh} ${target_a} "test -b /dev/${DEVICE}$i"
      then
        ${ssh} ${target_a} "umount -l /dev/${DEVICE}$i"
        ${ssh} ${target_a} "swapoff /dev/${DEVICE}$i"
      fi
    done
    ${ssh} ${target_a} "umount -l /dev/${DEVICE}"
  fi
  ${ssh} ${target_a} "rm -rf ${iscsi_mount}"
  print_SUCCESS

  pad " · Restoring /etc/fstab on ${target_a}"
  ${ssh} ${target_a} "test -s /var/tmp/lab-${problem_name}-fstab && cp /var/tmp/lab-${problem_name}-fstab /etc/fstab ; rm -f /var/tmp/lab-${problem_name}-fstab"
  print_SUCCESS

  pad " · Logging out of the iSCSI target on ${target_a}"
  if [ -n "${TARGET}" -a -n "${PORTAL}" ]
  then
    ${ssh} ${target_a}  "iscsiadm -m node -T ${TARGET} -p ${PORTAL} -u ; iscsiadm -m node -T ${TARGET} -p ${PORTAL} -o delete"
  fi
  print_SUCCESS

  pad " · Removing the help file from ${target_a}"
  ${ssh} ${target_a} "rm -rf /root/${problem_name}"
  print_SUCCESS

  pad " · Removing the credential file from ${target_a}"
  ${ssh} ${target_a} "rm -f /etc/samba/cred*"
  print_SUCCESS

  pad " · Removing the packages from ${target_a}"
  ${ssh} ${target_a} "yum remove -y cifs-utils samba-client iscsi-initiator-utils cups-ipptool; rm -rf /var/lib/iscsi /var/lock/iscsi /etc/iscsi"
  print_SUCCESS

  pad " · Closing the firewall ports on ${target_a}"
  ${ssh} ${target_a} 'firewall-cmd --remove-service=samba-client --remove-service=samba-dc --remove-service=samba --permanent ; firewall-cmd --remove-port=137/udp --remove-port=138/udp --remove-port=139/tcp --remove-port=445/tcp --permanent ; firewall-cmd --reload'
  print_SUCCESS

  pad " · Removing the ${linux_user} user account from ${target_a}"
  ${ssh} ${target_a} "userdel --force --remove ${linux_user}"
  print_SUCCESS

  pad " · Removing the ${linux_group} group from ${target_a}"
  ${ssh} ${target_a} "groupdel ${linux_group}"
  print_SUCCESS

  for target in ${target_a} ${target_c}
  do
    pad " · Removing the ${nfs_user} user account from ${target}"
    ${ssh} ${target} "userdel --force --remove ${nfs_user}"
    print_SUCCESS

    pad " · Removing the ${nfs_group} group from ${target}"
    ${ssh} ${target} "groupdel ${nfs_group}"
    print_SUCCESS
  done

  pad " · Cleaning up the target on ${target_c}"
  ${ssh} ${target_c} "targetcli clearconfig confirm=True"
  print_SUCCESS

  pad " · Cleaning up spare disk ${second_disk_device} on ${target_c}"
  disk_cleanup
  print_SUCCESS

  pad " · Removing the help file from ${target_c}"
  ${ssh} ${target_c} "rm -rf /root/${problem_name}"
  print_SUCCESS

  pad " · Stopping the services on ${target_c}"
  ${ssh} ${target_c} "systemctl disable --now smb.service nmb.service nfs-server.service target.service"
  print_SUCCESS

  pad " · Removing the packages from ${target_c}"
  ${ssh} ${target_c} "yum remove -y samba samba-libs samba-client samba-common-tools targetcli target-restore ; rm -rf /etc/target /var/target"
  print_SUCCESS

  pad " · Removing the samba password file from ${target_c}"
  ${ssh} ${target_c} "rm -rf /var/lib/samba/private/*"
  print_SUCCESS

  pad " · Restoring /etc/samba/smb.conf on ${target_c}"
  ${ssh} ${target_c} "test -s /var/tmp/lab-${problem_name}-smb.conf && cp /var/tmp/lab-${problem_name}-smb.conf /etc/samba/smb.conf ; rm -f /var/tmp/lab-${problem_name}-smb.conf"
  print_SUCCESS

  pad " · Cleaning up NFS exports on ${target_c}"
  ${ssh} ${target_c} "rm -f /etc/exports.d/* ; > /etc/exports"
  print_SUCCESS

  pad " · Removing the shared directories from ${target_c}"
  ${ssh} ${target_c} "rm -rf ${smb_share_dir} ${nfs_share_dir}"
  print_SUCCESS

  pad " · Removing the ${linux_user} user account from ${target_c}"
  ${ssh} ${target_c} "userdel --force --remove ${linux_user}"
  print_SUCCESS

  pad " · Removing the ${linux_group} group from ${target_c}"
  ${ssh} ${target_c} "groupdel ${linux_group}"
  print_SUCCESS

  pad " · Removing the ${sambamount_user} user account from ${target_c}"
  ${ssh} ${target_c} "userdel --force --remove ${sambamount_user}"
  print_SUCCESS

  pad " · Removing the SELinux rule from ${target_c}"
  ${ssh} ${target_c} "semanage fcontext -d -t samba_share_t '/srv/managers(/.*)?'"
  print_SUCCESS

  pad " · Closing the firewall ports on ${target_c}"
  ${ssh} ${target_c} 'firewall-cmd --remove-service=samba-client --remove-service=samba-dc --remove-service=samba --permanent ; firewall-cmd --remove-port=137/udp --remove-port=138/udp --remove-port=139/tcp --remove-port=445/tcp --permanent ; firewall-cmd --remove-service=nfs --remove-service=nfs3 --permanent ; firewall-cmd --remove-port=2049/tcp --remove-port=2049/udp --permanent ; firewall-cmd --remove-service=iscsi-target --permanent ; firewall-cmd --remove-port=3260/tcp --permanent; firewall-cmd --reload'
  print_SUCCESS

  pad " · Removing the rhel-system-roles package from workstation"
  yum remove -y rhel-system-roles
  print_SUCCESS


  #
  # CUPS cleanup
  #

  pad " · Removing CUPS from ${target_a}"
  printer_cleanup
  print_SUCCESS

  pad ' · Downloading lab playbooks'
  cd /tmp
  cmd1="curl -O http://materials.example.com/${lab_playbook_dir}/${lab_playbook_tar}"
  cmd2="tar xf ${lab_playbook_tar}"
  if ${cmd1} && ${cmd2}
  then
    rm -f ${lab_playbook_tar}
    print_SUCCESS

    pad ' · Running lab cleanup playbook'
    cd "/tmp/${ansible_dir}"
    if ansible-playbook ${lab_playbook_finish} & spinner $! 3
    then
      cd
      rm -rf "/tmp/${ansible_dir}"
      print_SUCCESS
    else
      print_FAIL
    fi
  else
    print_FAIL
  fi

  print_line
  print_line 'Lab finished.'
  print_line
}

############### Don't EVER change anything below this line ###############

# Source library of functions
source /usr/local/lib/${function_lib}
source /usr/local/lib/${platform_lib}

grading_main_program "$@"
