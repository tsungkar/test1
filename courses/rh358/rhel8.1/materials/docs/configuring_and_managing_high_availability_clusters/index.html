<!DOCTYPE html>
<html lang="en" dir="ltr" prefix="content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema# article: http://ogp.me/ns/article# book: http://ogp.me/ns/book# profile: http://ogp.me/ns/profile# video: http://ogp.me/ns/video#" >

<head profile="http://www.w3.org/1999/xhtml/vocab">
	  <!--[if IE]><![endif]-->
<meta charset="utf-8" />
<meta name="revision" title="70b8fe18-dde2-4462-95b7-ef99fd8af68e" product="f321f43a-4a79-48fe-8aae-117b9d973ec0" revision="b9796f39cfdde395bad8c9c028af0864c6296540:en-us" page="71b2322f-552c-46e3-8464-4fdf3f54a152" />
<meta name="generator" content="Drupal 7 (http://drupal.org)" />
<link rel="canonical" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/index" />
<link rel="shortlink" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/index" />
<meta property="og:site_name" content="Red Hat Customer Portal" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/index" />
<meta property="og:title" content="Configuring and managing high availability clusters Red Hat Enterprise Linux 8 | Red Hat Customer Portal" />
<meta property="og:description" content="This guide provides information about installing, configuring, and managing the Red Hat High Availability Add-On for Red Hat Enterprise Linux 8." />
<meta property="og:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:url" content="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/index" />
<meta name="twitter:title" content="Configuring and managing high availability clusters Red Hat Enterprise Linux 8 | Red Hat Customer Portal" />
<meta name="twitter:description" content="This guide provides information about installing, configuring, and managing the Red Hat High Availability Add-On for Red Hat Enterprise Linux 8." />
<meta name="twitter:image:src" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
  <title>Configuring and managing high availability clusters Red Hat Enterprise Linux 8 | Red Hat Customer Portal</title>
  <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<!--[if IEMobile]><meta http-equiv="cleartype" content="on"><![endif]-->

<!-- metaInclude -->
<meta name="avalon-host-info" content="kcs06.web.prod.ext.phx2.redhat.com" />
<meta name="avalon-version" content="2070d6b" />
<meta name="cp-chrome-build-date" content="2020-03-09T20:28:54.189Z" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- Windows Phone -->
<meta name="msapplication-navbutton-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-status-bar-style" content="#000000">
<link rel="manifest" href="https://access.redhat.com/webassets/avalon/j/manifest.json">
<!-- Open Search - Tap to search -->
<link rel="search" type="application/opensearchdescription+xml" title="Red Hat Customer Portal" href="https://access.redhat.com/webassets/avalon/j/opensearch.xml" />

 

<script type="text/javascript">
    window.portal = {
        analytics : {},
        host      : "https://access.redhat.com",
        idp_url   : "https://sso.redhat.com",
        lang      : "en",  
        version   : "2070d6b",
        builddate : "2020-03-09T20:28:54.189Z",
        fetchdate : "2020-03-11T07:53:05-0400",
        nrid      : "14615289",
        nrlk      : "2a497fa56f"
    };
</script>
<script type="text/javascript">
    if (!/\/logout.*/.test(location.pathname) && portal.host === location.origin && document.cookie.indexOf('rh_sso_session') >= 0 && !(document.cookie.indexOf('rh_jwt') >= 0)) window.location = '/login?redirectTo=' + encodeURIComponent(window.location.href);
</script>
<!-- cssInclude -->

<link rel="shortcut icon" href="favicon.ico" />

<link media="all" rel="stylesheet" type="text/css" href="bootstrap.css%3Fv=2070d6b.css" />
<link media="all" rel="stylesheet" type="text/css" href="main.css%3Fv=2070d6b.css" />
<link media="all" rel="stylesheet" type="text/css" href="components.css%3Fv=2070d6b.css" />
<link media="all" rel="stylesheet" type="text/css" href="pages.css%3Fv=2070d6b.css" />

<link href="chosen.css%3Fv=2070d6b.css" rel="stylesheet" type="text/css" />

<!--[if lte IE 9]>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/ie.css" />
<![endif]-->

<noscript>
    <style type="text/css" media="screen"> .primary-nav { display: block; } </style>
</noscript>

<!-- /cssInclude -->
<script type="text/javascript" src="require.js%3Fv=2070d6b" data-main="/webassets/avalon/j/"></script>

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<script src="https://access.redhat.com/chrome_themes/nimbus/js/ie8.js"></script>
<![endif]-->
  <link type="text/css" rel="stylesheet" href="css__c2Nkkx_5vYh8rvZbfBAGB4EMzMtH5ouFJDBlDsnhSR8__-ax0_8bam15ZDJT9j7LilCfJrDyEhGGAgc0KC8HYvJg__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="all" />
<link type="text/css" rel="stylesheet" href="css__Jy3BSr8TrxptaufAzQDT1skBUlX2CnL_wm6BizzYuGw__-YZvqB8yuA4kK_iKklbk5HdZCoG2qAgz1l-8Qi2NFH4__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="all" />
<link type="text/css" rel="stylesheet" href="messages.css" media="all" />
<link type="text/css" rel="stylesheet" href="css__m76iIFREtc70Nmw5xe1ZwHbNBlwOP2Zjc3DcacNWnFQ__STEh8aY3w8E_bKzhB4Xke2WOQ9XMDQquHIP5B8SeDIY__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="all" />
<link type="text/css" rel="stylesheet" href="css__4sM4s6XOQ2Nm0pdkrWRLvrgtwpTmHAFzR_LcdesKYj8__vJ0jjVEhoPh3KBeLZfkqg51T3AFxhQCuXg1reipa__k__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="print" />
  <link rel="stylesheet" type="text/css" media="all" href="list.css" />
  <script src="js__1YIht1y1Jlm9D0PyxEuhS3DlWdYAlEsNpsFDOaUsfRw__ZnC7278ZsqBG-1FOUqflDl8B3Y1kLiAWXAIkoLrHbYg__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script src="js__5ysXPc5KIyMmizxqRY68ILfrEGrj0P29WBIifnPTJvQ__Cap0DACEVMsefumg1gS1APLLd8stDkdGfp6c1uswMo4__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script>chrometwo_require(["analytics/attributes"], function(attributes) {
attributes.set('ResourceID',       '70b8fe18-dde2-4462-95b7-ef99fd8af68e');
attributes.set('ResourceTitle',    'Configuring and managing high availability clusters');
attributes.set('Language',         'en-us');
attributes.set('RevisionId',       'b9796f39cfdde395bad8c9c028af0864c6296540:en-us');
attributes.set('PublicationState', ['active','published']);
attributes.set('Product',          'Red Hat Enterprise Linux');
attributes.set('ProductVersion',   '8');
attributes.set('ProductId',        'Red Hat Enterprise Linux 8');
});</script>
<script>breadcrumbs = [ ["Products &amp; Services", "/products/"], ["Product Documentation", "/documentation/"], ["Red Hat Enterprise Linux", "/documentation/en-us/red_hat_enterprise_linux/"], ["8", "/documentation/en-us/red_hat_enterprise_linux/8/"], ["Configuring and managing high availability clusters", "/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/"] ];</script>
<script>window.siteMapState = "products";</script>
<script src="js__i6ieGBO-OPmIKm_f0srsb6gM7QELIWrBpKh6ub_yj8A__wjRg_duSK4rEZCRV2uwrCIPMl80Z_LJ9ew61H5hE-ZI__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script src="js__vGeEtorfXGI3aPAp71YA74N5p8wfpYVhnoIqwaJrOiQ__veGxEiPQW6fkzAPZnAFA23SFz1CyJy2dlu6x8XuHM4s__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script>jQuery.extend(Drupal.settings, {"basePath":"\/","pathPrefix":"","ajaxPageState":{"theme":"kcs","theme_token":"n07G-G6vE__byWpbN1cvyF5FIFfGwIfjaa0v7IzEtlk","js":{"misc\/jquery-extend-3.4.0.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"sites\/all\/libraries\/chosen\/chosen\/chosen.jquery.min.js":1,"modules\/chosen\/chosen.js":1,"sites\/all\/modules\/custom\/rh_doc_fetcher\/js\/prism.js":1,"sites\/all\/modules\/custom\/rh_doc_fetcher\/js\/underscore.js":1,"sites\/all\/themes\/kcs\/js\/kcs_base.js":1,"sites\/all\/themes\/kcs\/js\/showdown.js":1,"sites\/all\/themes\/kcs\/js\/case_links.js":1},"css":{"modules\/system\/system.base.css":1,"modules\/system\/system.menus.css":1,"modules\/system\/system.messages.css":1,"modules\/book\/book.css":1,"modules\/comment\/comment.css":1,"modules\/date\/date_api\/date.css":1,"modules\/date\/date_popup\/themes\/datepicker.1.7.css":1,"modules\/field\/theme\/field.css":1,"modules\/node\/node.css":1,"modules\/search\/search.css":1,"modules\/user\/user.css":1,"sites\/all\/modules\/contrib\/user_prune\/css\/user_prune.css":1,"modules\/views\/css\/views.css":1,"sites\/all\/libraries\/chosen\/chosen\/chosen.css":1,"modules\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/contrib\/panels\/css\/panels.css":1,"sites\/all\/modules\/custom\/rate\/rate.css":1,"sites\/all\/modules\/custom\/rh_doc_fetcher\/plugins\/layouts\/docs_page\/docs_page.css":1,"https:\/\/access.redhat.com\/webassets\/avalon\/s\/messages.css":1,"sites\/all\/themes\/zen\/system.base.css":1,"sites\/all\/themes\/zen\/system.menus.css":1,"sites\/all\/themes\/zen\/system.messages.css":1,"sites\/all\/themes\/zen\/system.theme.css":1,"sites\/all\/themes\/zen\/comment.css":1,"sites\/all\/themes\/zen\/node.css":1,"sites\/all\/themes\/kcs\/css\/html-reset.css":1,"sites\/all\/themes\/kcs\/css\/wireframes.css":1,"sites\/all\/themes\/kcs\/css\/layout-fixed.css":1,"sites\/all\/themes\/kcs\/css\/page-backgrounds.css":1,"sites\/all\/themes\/kcs\/css\/tabs.css":1,"sites\/all\/themes\/kcs\/css\/pages.css":1,"sites\/all\/themes\/kcs\/css\/blocks.css":1,"sites\/all\/themes\/kcs\/css\/navigation.css":1,"sites\/all\/themes\/kcs\/css\/views-styles.css":1,"sites\/all\/themes\/kcs\/css\/nodes.css":1,"sites\/all\/themes\/kcs\/css\/comments.css":1,"sites\/all\/themes\/kcs\/css\/forms.css":1,"sites\/all\/themes\/kcs\/css\/fields.css":1,"sites\/all\/themes\/kcs\/css\/kcs.css":1,"sites\/all\/themes\/kcs\/css\/print.css":1}},"chosen":{"selector":"#edit-field-kcs-component-select-und, #edit-field-kcs-sbr-select-und, #edit-field-kcs-product-select-und, #edit-field-kcs-type-select-und, #edit-language, #edit-field-kcs-a-category-select-und, #edit-field-kcs-state-select-und, #edit-field-kcs-tags-select-und, #edit-field-kcs-state-select-value, #edit-field-vid-reference-product-und, #edit-state, #edit-product, #edit-category, #edit-field-kcs-product-select-tid, #edit-field-kcs-type-select-tid, #edit-field-kcs-a-category-select-tid, #edit-sort-bef-combine, #edit-kcs-state, #edit-field-category-tid, #edit-field-product-tid, #edit-field-tags-tid, #edit-field-category-und, #edit-field-product-und, #edit-field-tags-und, #views-exposed-form-questions-list-questions-filter-block #edit-field-category, #edit-field-tags-und, #views-exposed-form-questions-list-questions-filter-block #edit-field-product, #views-exposed-form-questions-list-questions-filter-block #edit-field-tags, #edit-field-mega-menu-tab-und, #edit-field-internal-tags-tid, #edit-tags, #edit-field-kcs-tags-select-tid, #edit-subscriptions-and-choose-field-select, #edit-subscriptions-and-type, #edit-vid-13, #edit-vid-4, #edit-vid-5, #edit-vid-53, #edit-vid-1, #edit-vid-3, #edit-group-blog, #edit-group-group, #edit-subscriptions-and-choose-field-select-two, #edit-subscriptions-and-type-two, #edit-vid-13-two, #edit-vid-4-two, #edit-vid-5-two, #edit-vid-53-two, #edit-vid-1-two, #edit-vid-3-two, #edit-group-blog-two, #edit-group-group-two, #edit-subscriptions-and-edit-choose-field-select, #edit-subscriptions-and-type-edit, #edit-subscriptions-and-edit-type-two, #edit-vid-13-edit-two, #edit-vid-4-edit-two, #edit-vid-5-edit-two, #edit-vid-53-edit-two, #edit-vid-1-edit-two, #edit-vid-3-edit-two, #edit-group-group-edit-two, #edit-subscriptions-and-edit-choose-field-select-two, #edit-vid-13-edit, #edit-vid-4-edit, #edit-vid-5-edit, #edit-vid-53-edit, #edit-vid-1-edit, #edit-vid-3-edit, #edit-group-group-edit, #edit-group-blog-edit, #edit-field-supported-languages-tid, #edit-field-geography-und, #edit-field-supported-products-und, #edit-field-supported-languages-und, #edit-field-vendor-und, #edit-field-errata-type-text-und, #edit-field-errata-severity-text-und, #edit-field-software-partner-level-und, #edit-field-scert-product-category-und, #edit-field-certifications-und-0-field-product-und, #edit-kcs-article-type, #edit-field-eco-industry-tag-select-tid, #edit-field-eco-software-catego-select-tid, #edit-field-ecosystem-tag-select-und, #edit-field-eco-industry-tag-select-und, #edit-field-eco-software-catego-select-und, #edit-field-vendor-tsanet-member-ref-und, #edit-field-og-vendor-ref-und-0-default, #edit-field-eco-products-enabled-col-und-0-field-eco-subscription-model-ref-und, #edit-field-eco-products-enabled-col-und-0-field-eco-support-level-ref-und, #edit-field-eco-products-enabled-col-und-0-field-eco-product-select-und, #edit-field-eco-certifications-select-und-0-field-eco-product-select-und, #edit-field-eco-certifications-select-und-0-field-eco-product-archite-select-und, #edit-field-eco-certifications-select-und-0-field-eco-certificati-lvl-select-und, #edit-field-eco-certifications-select-und-0-field-eco-hypervisor-str-und, #edit-field-eco-supported-language-ref-und, #edit-field-eco-region-ref-und, #edit-field-cs-product-category-str-und, #edit-field-eco-certifications-select-und-0-field-eco-format-ref-und, #edit-field-eco-group-access-ref-und, #edit-field-hw-category-tag-ref-und, #edit-field-profile-industry-und, #edit-field-profile-tech-interests-und, #edit-field-product-page-features-ref-und, #edit-field-eco-product-select-und, #edit-field-base-product-ref-und, #edit-field-eco-product-archite-select-und, #edit-field-eco-format-ref-und, #edit-field-certification-status-ref-und, #edit-field-certification-result-ref-und, #edit-field-og-certified-product-ref-und-0-default, #edit-field-eco-subscription-model-ref-und, #edit-field-eco-support-level-ref-und, #edit-field-eco-certificati-lvl-select-und, #edit-field-eco-hypervisor-str-und, #edit-field-ccp-thirdparty-cert-select-und, .use_, #edit-field-eco-cert-product-tag-und, #edit-field-documentation-location-ref-und, #edit-field-documentation-title-und, #edit-field-accelerator-products-und","minimum":"0"},"rh_doc_fetcher":{"page_type":"single"},"section":"","kcs":{"nodeType":null,"nodeId":null}});</script>
    <!--[if lt IE 9]>
  <script src="https://access.redhat.com/sites/all/themes/kcs/js/html5shiv.js"></script>
  <![endif]-->
  
      


    <!--kcs07-->
<script type="text/javascript">
  Drupal.portal = {"version":{"redhat_portal":"package drupal7-redhat_portal is not installed"}};
  Drupal.portal.currentUser = {};
  </script>

</head>

<body class="portal-page admin-bar  kcs_external" >

  
  <div id="page-wrap" class="page-wrap">
    <div class="top-page-wrap">

        <!--googleoff: all-->
        <header class="masthead" id="masthead">

            <!-- Accessibility Nav & Header -->
<div class="accessibility-nav sr-only">
    <a href="https://access.redhat.com/">
        <h1>Red Hat <span>Customer </span><span>Portal</span></h1>
    </a>
    <p><a href="index.html#cp-main">Skip to main content</a></p>
    <nav aria-labelledby="accessibility-nav-heading">
        <h2 id="accessibility-nav-heading">Main Navigation</h2>
        <ul>
    <li>
        <a href="index.html#" class="has-subnav"><span>Products &amp; Services</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/products">View All Products</a></li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Infrastructure and Management</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-virtualization/">Red Hat Virtualization</a></li>
                    <li><a href="https://access.redhat.com/products/identity-management/">Red Hat Identity Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-directory-server/">Red Hat Directory Server</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-certificate-system/">Red Hat Certificate System</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-update-infrastructure/">Red Hat Update Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|p|im|rhaijan2016&">Red Hat Insights</a></li>
                    <li><a href="https://access.redhat.com/products/ansible-tower-red-hat/">Red Hat Ansible Tower</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ansible-engine/">Red Hat Ansible Engine</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Cloud Computing</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloudforms/">Red Hat CloudForms</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloud-suite/">Red Hat Cloud Suite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-online-red-hat/">Red Hat OpenShift Online</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Storage</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-storage">Red Hat Openshift Container Storage</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Runtimes</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true">
                      <a href="index.html#" class="back">Back</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/thorntail/">Red Hat build of Thorntail</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/eclipse-vertx/">Red Hat build of Eclipse Vert.x</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/openjdk/">Red Hat build of OpenJDK</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/open-liberty/">Open Liberty</a>
                    </li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Integration and Automation</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-integration/">Red Hat Integration</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                    </li>
                    <li>
                      <a href="https://access.redhat.com/products/red-hat-process-automation/">Red Hat Automation</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                    </li>
                </ul>
            </li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/support/">Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/production/">Production Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/developer/">Development Support</a></li>
            <li><a href="https://access.redhat.com/support/policy/update_policies/">Product Life Cycle &amp; Update Policies</a></li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/documentation/">Documentation</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux">Red Hat Enterprise Linux</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform">Red Hat OpenStack Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform">Red Hat OpenShift Container Platform</a></li>

            <li class="heading">Services</li>
            <li><a href="https://www.redhat.com/en/services/consulting">Consulting</a></li>
            <li><a href="https://access.redhat.com/support/offerings/tam/">Technical Account Management</a></li>
            <li><a href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications</a></li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/ecosystem/">Ecosystem</a></li>
            <li><a href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources</a></li>
            <li><a href="https://access.redhat.com/public-cloud">Red Hat in the Public Cloud</a></li>
          </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Tools</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|t|c1|rhaidec2015&">Red Hat Insights</a></li>
            <li class="heading">Tools</li>
            <li><a href="https://access.redhat.com/solution-engine">Solution Engine</a></li>
            <li><a href="https://access.redhat.com/downloads/content/package-browser">Packages</a></li>
            <li><a href="https://access.redhat.com/errata/">Errata</a></li>
            <li class="heading">Customer Portal Labs</li>
            <li><a href="https://access.redhat.com/labs/">Explore Labs</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=config">Configuration</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=deploy">Deployment</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=security">Security</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=troubleshoot">Troubleshooting</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Security</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/security/">Product Security Center</a></li>
            <li class="heading">Security Updates</li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs</a></li>
            <li class="heading">Resources</li>
            <li><a href="https://access.redhat.com/security/overview/">Overview</a></li>
            <li><a href="https://access.redhat.com/blogs/product-security">Security Blog</a></li>
            <li><a href="https://www.redhat.com/security/data/metrics/">Security Measurement</a></li>
            <li><a href="https://access.redhat.com/security/updates/classification/">Severity Ratings</a></li>
            <li><a href="https://access.redhat.com/security/updates/backporting/">Backporting Policies</a></li>
            <li><a href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Community</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="heading">Customer Portal Community</li>
            <li><a href="https://access.redhat.com/discussions?keyword=&name=&product=All&category=All&tags=All">Discussions</a></li>
            <li><a href="https://access.redhat.com/blogs/">Blogs</a></li>
            <li><a href="https://access.redhat.com/groups/">Private Groups</a></li>
            <li><a href="https://access.redhat.com/community/">Community Activity</a></li>
            <li class="heading">Customer Events</li>
            <li><a href="https://access.redhat.com/convergence/">Red Hat Convergence</a></li>
            <li><a href="http://www.redhat.com/summit/">Red Hat Summit</a></li>
            <li class="heading">Stories</li>
            <li><a href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value</a></li>
            <li><a href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.</a></li>
            <li><a href="http://www.redhat.com/en/open-source">Open Source Communities</a></li>
        </ul>
    </li>
    <li><a href="https://access.redhat.com/management/">Subscriptions</a></li>
    <li><a href="https://access.redhat.com/downloads/">Downloads</a></li>
    <li><a href="https://catalog.redhat.com/software/containers/explore/">Containers</a></li>
    <li><a href="https://access.redhat.com/support/cases/">Support Cases</a></li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Account</span></a>
        <ul class="utility mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://access.redhat.com/login" id="accessibility-accountLogin">Log In</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://www.redhat.com/wapps/ugc/register.html">Register</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated">
                <div class="account-info" id="accessibility-accountUser">
                    <div class="account-name"><strong id="accessibility-userFullName"></strong></div>
                    <div class="account-org"><span id="accessibility-userOrg"></span></div>
                    <div class="account-number mobile-accountNumber"><span class="property">Red Hat Account Number:</span> <span class="value"></span></div>
                </div>
            </li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/personalInfo.html">Account Details</a></li>
            <!-- <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/prefs.html">Newsletter and Contact Preferences</a></li> -->
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/usermgt/userList.html" class="user-management-link">User Management</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/account.html">Account Maintenance</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user">My Profile</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user" id="accessibility-userNotificationsLink">Notifications</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/help/">Help</a></li>
            <li class="logout mobile-accountLinksLoggedIn authenticated"><a href="https://sso.redhat.com/auth/realms/redhat-external/logout?redirectUrl=https%3A%2F%2Faccess.redhat.com%2Flogout">Log Out</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Language</span></a>
        <ul class="utility mm-hide" id="accessibility-localesMenu">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=en" id="accessibility-en">English</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ko" id="accessibility-ko">한국어</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ja" id="accessibility-ja">日本語</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" id="accessibility-zh_CN">中文 (中国)</a></li>
		</ul>
    </li>
</ul>
    </nav>
</div>

<!-- Mobile Header -->
<nav class="mobile-nav-bar hidden-sm hidden-md hidden-lg" aria-hidden="true">
    <button id="menu-btn" class="menu menu-white" type="button" aria-label="Toggle Navigation">
        <span class="lines"></span>
    </button>
    <a class="logo" href="https://access.redhat.com/">
        <span class="logo-crop">
          <!-- logo -->
          <svg class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 613 145">
            <defs>
              <style>
                .rh-logo-hat {
                  fill: #e00;
                }
                .rh-logo-type {
                  fill: #fff;
                }
              </style>
            </defs>
            <title>Red Hat</title>
            <path
              class="rh-logo-hat"
              d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/>
              <path class="rh-logo-band"
              d="M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45,12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/>
              <path
              class="rh-logo-type"
              d="M579.74,92.8c0,11.89,7.15,17.67,20.19,17.67a52.11,52.11,0,0,0,11.89-1.68V95a24.84,24.84,0,0,1-7.68,1.16c-5.37,0-7.36-1.68-7.36-6.73V68.3h15.56V54.1H596.78v-18l-17,3.68V54.1H568.49V68.3h11.25Zm-53,.32c0-3.68,3.69-5.47,9.26-5.47a43.12,43.12,0,0,1,10.1,1.26v7.15a21.51,21.51,0,0,1-10.63,2.63c-5.46,0-8.73-2.1-8.73-5.57m5.2,17.56c6,0,10.84-1.26,15.36-4.31v3.37h16.82V74.08c0-13.56-9.14-21-24.39-21-8.52,0-16.94,2-26,6.1l6.1,12.52c6.52-2.74,12-4.42,16.83-4.42,7,0,10.62,2.73,10.62,8.31v2.73a49.53,49.53,0,0,0-12.62-1.58c-14.31,0-22.93,6-22.93,16.73,0,9.78,7.78,17.24,20.19,17.24m-92.44-.94h18.09V80.92h30.29v28.82H506V36.12H487.93V64.41H457.64V36.12H439.55ZM370.62,81.87c0-8,6.31-14.1,14.62-14.1A17.22,17.22,0,0,1,397,72.09V91.54A16.36,16.36,0,0,1,385.24,96c-8.2,0-14.62-6.1-14.62-14.09m26.61,27.87h16.83V32.44l-17,3.68V57.05a28.3,28.3,0,0,0-14.2-3.68c-16.19,0-28.92,12.51-28.92,28.5a28.25,28.25,0,0,0,28.4,28.6,25.12,25.12,0,0,0,14.93-4.83ZM320,67c5.36,0,9.88,3.47,11.67,8.83H308.47C310.15,70.3,314.36,67,320,67M291.33,82c0,16.2,13.25,28.82,30.28,28.82,9.36,0,16.2-2.53,23.25-8.42l-11.26-10c-2.63,2.74-6.52,4.21-11.14,4.21a14.39,14.39,0,0,1-13.68-8.83h39.65V83.55c0-17.67-11.88-30.39-28.08-30.39a28.57,28.57,0,0,0-29,28.81M262,51.58c6,0,9.36,3.78,9.36,8.31S268,68.2,262,68.2H244.11V51.58Zm-36,58.16h18.09V82.92h13.77l13.89,26.82H292l-16.2-29.45a22.27,22.27,0,0,0,13.88-20.72c0-13.25-10.41-23.45-26-23.45H226Z"/>
          </svg>
        </span>
        <span class="site-title"><span>Customer </span><span>Portal</span></span>
    </a>
    <button class="btn btn-search btn-utility" data-target="#site-search">
        <span class="web-icon-search"></span>
        <span class="link-text">Search</span>
    </button>
</nav>

<!-- Mobile Menu Drawer -->
<div class="nav-drawer mobile-nav-drawer hidden-sm hidden-md hidden-lg" aria-hidden="true">
    <nav class="nav-container">
        <ul>
    <li>
        <a href="index.html#" class="has-subnav"><span>Products &amp; Services</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/products">View All Products</a></li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Infrastructure and Management</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-virtualization/">Red Hat Virtualization</a></li>
                    <li><a href="https://access.redhat.com/products/identity-management/">Red Hat Identity Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-directory-server/">Red Hat Directory Server</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-certificate-system/">Red Hat Certificate System</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-update-infrastructure/">Red Hat Update Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|p|im|rhaijan2016&">Red Hat Insights</a></li>
                    <li><a href="https://access.redhat.com/products/ansible-tower-red-hat/">Red Hat Ansible Tower</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ansible-engine/">Red Hat Ansible Engine</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Cloud Computing</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloudforms/">Red Hat CloudForms</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloud-suite/">Red Hat Cloud Suite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-online-red-hat/">Red Hat OpenShift Online</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Storage</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-storage">Red Hat Openshift Container Storage</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Runtimes</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true">
                      <a href="index.html#" class="back">Back</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/thorntail/">Red Hat build of Thorntail</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/eclipse-vertx/">Red Hat build of Eclipse Vert.x</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/openjdk/">Red Hat build of OpenJDK</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/open-liberty/">Open Liberty</a>
                    </li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Integration and Automation</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-integration/">Red Hat Integration</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                    </li>
                    <li>
                      <a href="https://access.redhat.com/products/red-hat-process-automation/">Red Hat Automation</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                    </li>
                </ul>
            </li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/support/">Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/production/">Production Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/developer/">Development Support</a></li>
            <li><a href="https://access.redhat.com/support/policy/update_policies/">Product Life Cycle &amp; Update Policies</a></li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/documentation/">Documentation</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux">Red Hat Enterprise Linux</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform">Red Hat OpenStack Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform">Red Hat OpenShift Container Platform</a></li>

            <li class="heading">Services</li>
            <li><a href="https://www.redhat.com/en/services/consulting">Consulting</a></li>
            <li><a href="https://access.redhat.com/support/offerings/tam/">Technical Account Management</a></li>
            <li><a href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications</a></li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/ecosystem/">Ecosystem</a></li>
            <li><a href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources</a></li>
            <li><a href="https://access.redhat.com/public-cloud">Red Hat in the Public Cloud</a></li>
          </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Tools</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|t|c1|rhaidec2015&">Red Hat Insights</a></li>
            <li class="heading">Tools</li>
            <li><a href="https://access.redhat.com/solution-engine">Solution Engine</a></li>
            <li><a href="https://access.redhat.com/downloads/content/package-browser">Packages</a></li>
            <li><a href="https://access.redhat.com/errata/">Errata</a></li>
            <li class="heading">Customer Portal Labs</li>
            <li><a href="https://access.redhat.com/labs/">Explore Labs</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=config">Configuration</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=deploy">Deployment</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=security">Security</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=troubleshoot">Troubleshooting</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Security</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/security/">Product Security Center</a></li>
            <li class="heading">Security Updates</li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs</a></li>
            <li class="heading">Resources</li>
            <li><a href="https://access.redhat.com/security/overview/">Overview</a></li>
            <li><a href="https://access.redhat.com/blogs/product-security">Security Blog</a></li>
            <li><a href="https://www.redhat.com/security/data/metrics/">Security Measurement</a></li>
            <li><a href="https://access.redhat.com/security/updates/classification/">Severity Ratings</a></li>
            <li><a href="https://access.redhat.com/security/updates/backporting/">Backporting Policies</a></li>
            <li><a href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Community</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="heading">Customer Portal Community</li>
            <li><a href="https://access.redhat.com/discussions?keyword=&name=&product=All&category=All&tags=All">Discussions</a></li>
            <li><a href="https://access.redhat.com/blogs/">Blogs</a></li>
            <li><a href="https://access.redhat.com/groups/">Private Groups</a></li>
            <li><a href="https://access.redhat.com/community/">Community Activity</a></li>
            <li class="heading">Customer Events</li>
            <li><a href="https://access.redhat.com/convergence/">Red Hat Convergence</a></li>
            <li><a href="http://www.redhat.com/summit/">Red Hat Summit</a></li>
            <li class="heading">Stories</li>
            <li><a href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value</a></li>
            <li><a href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.</a></li>
            <li><a href="http://www.redhat.com/en/open-source">Open Source Communities</a></li>
        </ul>
    </li>
    <li><a href="https://access.redhat.com/management/">Subscriptions</a></li>
    <li><a href="https://access.redhat.com/downloads/">Downloads</a></li>
    <li><a href="https://catalog.redhat.com/software/containers/explore/">Containers</a></li>
    <li><a href="https://access.redhat.com/support/cases/">Support Cases</a></li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Account</span></a>
        <ul class="utility mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://access.redhat.com/login" id="mobile-accountLogin">Log In</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://www.redhat.com/wapps/ugc/register.html">Register</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated">
                <div class="account-info" id="mobile-accountUser">
                    <div class="account-name"><strong id="mobile-userFullName"></strong></div>
                    <div class="account-org"><span id="mobile-userOrg"></span></div>
                    <div class="account-number mobile-accountNumber"><span class="property">Red Hat Account Number:</span> <span class="value"></span></div>
                </div>
            </li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/personalInfo.html">Account Details</a></li>
            <!-- <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/prefs.html">Newsletter and Contact Preferences</a></li> -->
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/usermgt/userList.html" class="user-management-link">User Management</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/account.html">Account Maintenance</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user">My Profile</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user" id="mobile-userNotificationsLink">Notifications</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/help/">Help</a></li>
            <li class="logout mobile-accountLinksLoggedIn authenticated"><a href="https://sso.redhat.com/auth/realms/redhat-external/logout?redirectUrl=https%3A%2F%2Faccess.redhat.com%2Flogout">Log Out</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Language</span></a>
        <ul class="utility mm-hide" id="mobile-localesMenu">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=en" id="mobile-en">English</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ko" id="mobile-ko">한국어</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ja" id="mobile-ja">日本語</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" id="mobile-zh_CN">中文 (中国)</a></li>
		</ul>
    </li>
</ul>
    </nav>
</div>

<!-- White Utility Menu for large screens -->
<div class="utility-wrap" aria-hidden="true">
    <div class="utility-container">
        <div class="utility-bar hidden-xs">
            <div role="navigation" class="top-nav">
                <ul>
                    <li id="nav-subscription" data-portal-tour-1="1"><a class="top-nav-subscriptions" href="https://access.redhat.com/management/">Subscriptions</a></li>
                    <li id="nav-downloads" data-portal-tour-1="2"><a class="top-nav-downloads" href="https://access.redhat.com/downloads/">Downloads</a></li>
                    <li id="nav-containers"><a class="top-nav-containers" href="https://catalog.redhat.com/software/containers/explore/">Containers</a></li>
                    <li id="nav-support" data-portal-tour-1="3"><a class="top-nav-support-cases" href="https://access.redhat.com/support/cases/">Support Cases</a></li>
                </ul>
            </div>

            <div role="navigation" class="utility-nav">
                <ul>
                    <li id="searchABTestHide">
                        <a class="btn-search" data-target="#site-search" title="Search" data-toggle="tooltip" data-placement="bottom">
                            <span class="web-icon-search" aria-label="search"></span>
                            <span class="link-text">Search</span>
                        </a>
                    </li>
                    <!-- AB Test -->
                    <li id="searchABTestShow">
                        <form id="topSearchFormABTest" name="topSearchFormABTest">
                            <label for="topSearchInputABTest" class="sr-only">Search</label>
                            <input id="topSearchInputABTest" name="keyword" placeholder="Search" value="" type="text" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" class="form-control">
                            <button type="submit" class="btn btn-app btn-sm btn-link"><span class="web-icon-search" aria-label="search"></span></button>
                        </form>
                    </li>
                    <!-- End of AB Test -->
                    <li>
                        <a class="btn-profile" data-target="#account-info" data-portal-tour-1="4a" title="Account" data-toggle="tooltip" data-placement="bottom">
                            <span class="web-icon-user" aria-label="log in"></span>
                            <span class="link-text">Log In</span>
                            <span class="account-user" id="accountUserName"></span>
                        </a>
                    </li>
                    <li>
                        <a class="btn-language" data-target="#language" data-portal-tour-1="6" title="Language" data-toggle="tooltip" data-placement="bottom">
                            <span class="web-icon-globe" aria-label="language"></span>
                            <span class="link-text">Language</span>
                        </a>
                    </li>
                </ul>
            </div>
        </div>
        <!-- Utility Tray -->
        <div class="utility-tray-container">
            <div class="utility-tray">
                <div id="site-search" class="utility-link site-search">
                    <div class="content">
                        <form class="ng-pristine ng-valid topSearchForm" id="topSearchForm" name="topSearchForm" action="https://access.redhat.com/search/browse/search/" method="get" enctype="application/x-www-form-urlencoded">
                            <div class="input-group push-bottom">

                                <input class="form-control searchField" id="topSearchInput" name="keyword" value="" placeholder="Enter your search term" type="text">
                                <span class="input-group-btn">
                                    <button type="submit" class="btn btn-primary">Search</button>
                                </span>
                            </div>
                            <div>Troubleshooting an issue? Try <a href="https://access.redhat.com/solution-engine">Solution Engine</a>&mdash;our new support tool.</div>
                        </form>
                    </div>
                </div>
                <div id="account-info" class="utility-link account-info">
                    <div class="content">

                        <!-- Account Unauthenticated -->
                        <div id="accountLinksLoggedOut" class="unauthenticated">
                            <h2 class="utility-header">Log in to Your Red Hat Account</h2>
                            <div class="row col-border-row">
                                <div class="col-sm-6 col-border">
                                    <p><a href="https://access.redhat.com/login" id="accountLogin" class="btn btn-primary">Log In</a></p>
                                    <p>Your Red Hat account gives you access to your profile, preferences, and services, depending on your status.</p>
                                </div>
                                <div class="col-sm-6 col-border col-border-left">
                                    <p><a href="https://www.redhat.com/wapps/ugc/register.html" class="btn btn-primary">Register</a></p>
                                    <p>If you are a new customer, register now for access to product evaluations and purchasing capabilities. </p>

                                    <strong>Need access to an account?</strong><p>If your company has an existing Red Hat account, your organization administrator can grant you access.</p>

                                    <p><a href="https://access.redhat.com/support/contact/customerService/">If you have any questions, please contact customer service.</a></p>
                                </div>
                            </div>
                        </div>

                        <!-- Account Authenticated -->
                        <div id="accountLinksLoggedIn" class="authenticated">
                            <h2 class="utility-header"><span id="userFirstName"></span></h2>
                            <div class="row col-border-row">
                                <div class="col-sm-6 col-border col-border-right">
                                    <div class="account-info" id="accountUser">
                                        <div class="avatar"><!-- placeholder--></div>
                                        <div class="account-name"><strong id="userFullName"></strong></div>
                                        <div class="account-org"><span id="userOrg"></span></div>
                                        <div class="account-number accountNumber"><span class="property">Red Hat Account Number:</span> <span class="value"></span></div>
                                    </div>

                                    <div class="row account-settings">
                                        <div class="col-md-6" data-portal-tour-1="4">
                                            <h3>Red Hat Account</h3>
                                            <ul class="reset">
                                                <li><a href="https://www.redhat.com/wapps/ugc/protected/personalInfo.html">Account Details</a></li>
                                                <!-- <li><a href="https://www.redhat.com/wapps/ugc/protected/prefs.html">Newsletter and Contact Preferences</a></li> -->
                                                <li><a href="https://www.redhat.com/wapps/ugc/protected/usermgt/userList.html" class="user-management-link">User Management</a></li>
                                                <li><a href="https://www.redhat.com/wapps/ugc/protected/account.html">Account Maintenance</a></li>
                                                <li><a href="https://access.redhat.com/account-team">Account Team</a></li>
                                            </ul>
                                        </div>
                                        <div class="col-md-6" data-portal-tour-1="5">
                                            <h3>Customer Portal</h3>
                                            <ul class="reset">
                                                <li><a href="https://access.redhat.com/user">My Profile</a></li>
                                                <li><a href="https://access.redhat.com/user" id="userNotificationsLink">Notifications</a></li>
                                                <li><a href="https://access.redhat.com/help/">Help</a></li>
                                            </ul>
                                        </div>
                                    </div>

                                </div>
                                <div class="col-sm-6 col-border">
                                    <p>For your security, if you’re on a public computer and have finished using your Red Hat services, please be sure to log out.</p>
                                    <p><a href="https://sso.redhat.com/auth/realms/redhat-external/logout?redirectUrl=https%3A%2F%2Faccess.redhat.com%2Flogout" id="accountLogout" class="btn btn-primary">Log Out</a></p>
                                </div>
                            </div>
                        </div>

                    </div>
                </div>
                <div id="language" class="utility-link language">
                    <div class="content">
                        <h2 class="utility-header">Select Your Language</h2>
                        <div class="row" id="localesMenu">
                            <div class="col-sm-2">
                                <ul class="reset">
                                    <li><a href="https://access.redhat.com/changeLanguage?language=en" data-lang="en" id="en">English</a></li>
                                    <li><a href="https://access.redhat.com/changeLanguage?language=ko" data-lang="ko" id="ko">한국어</a></li>
                                </ul>
                            </div>
                            <div class="col-sm-2">
                                <ul class="reset">
                                    <li><a href="https://access.redhat.com/changeLanguage?language=ja"    data-lang="ja"    id="ja">日本語</a></li>
                                    <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" data-lang="zh_CN" id="zh_CN">中文 (中国)</a></li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div id="scroll-anchor"></div>

<!-- Main Menu for large screens -->
<div class="header-nav visible-sm visible-md visible-lg" aria-hidden="true">
    <div id="header-nav">
        <div class="container">
            <div class="row">
                <div class="col-xs-12">

                    <a href="https://access.redhat.com/" class="logo">
                        <span class="logo-crop">
                          <svg class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 613 145">
                            <defs>
                              <style>
                                .rh-logo-hat {
                                  fill: #e00;
                                }
                                .rh-logo-type {
                                  fill: #fff;
                                }
                              </style>
                            </defs>
                            <title>Red Hat</title>
                            <path
                              class="rh-logo-hat"
                              d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/>
                              <path class="rh-logo-band"
                              d="M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45,12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/>
                              <path
                              class="rh-logo-type"
                              d="M579.74,92.8c0,11.89,7.15,17.67,20.19,17.67a52.11,52.11,0,0,0,11.89-1.68V95a24.84,24.84,0,0,1-7.68,1.16c-5.37,0-7.36-1.68-7.36-6.73V68.3h15.56V54.1H596.78v-18l-17,3.68V54.1H568.49V68.3h11.25Zm-53,.32c0-3.68,3.69-5.47,9.26-5.47a43.12,43.12,0,0,1,10.1,1.26v7.15a21.51,21.51,0,0,1-10.63,2.63c-5.46,0-8.73-2.1-8.73-5.57m5.2,17.56c6,0,10.84-1.26,15.36-4.31v3.37h16.82V74.08c0-13.56-9.14-21-24.39-21-8.52,0-16.94,2-26,6.1l6.1,12.52c6.52-2.74,12-4.42,16.83-4.42,7,0,10.62,2.73,10.62,8.31v2.73a49.53,49.53,0,0,0-12.62-1.58c-14.31,0-22.93,6-22.93,16.73,0,9.78,7.78,17.24,20.19,17.24m-92.44-.94h18.09V80.92h30.29v28.82H506V36.12H487.93V64.41H457.64V36.12H439.55ZM370.62,81.87c0-8,6.31-14.1,14.62-14.1A17.22,17.22,0,0,1,397,72.09V91.54A16.36,16.36,0,0,1,385.24,96c-8.2,0-14.62-6.1-14.62-14.09m26.61,27.87h16.83V32.44l-17,3.68V57.05a28.3,28.3,0,0,0-14.2-3.68c-16.19,0-28.92,12.51-28.92,28.5a28.25,28.25,0,0,0,28.4,28.6,25.12,25.12,0,0,0,14.93-4.83ZM320,67c5.36,0,9.88,3.47,11.67,8.83H308.47C310.15,70.3,314.36,67,320,67M291.33,82c0,16.2,13.25,28.82,30.28,28.82,9.36,0,16.2-2.53,23.25-8.42l-11.26-10c-2.63,2.74-6.52,4.21-11.14,4.21a14.39,14.39,0,0,1-13.68-8.83h39.65V83.55c0-17.67-11.88-30.39-28.08-30.39a28.57,28.57,0,0,0-29,28.81M262,51.58c6,0,9.36,3.78,9.36,8.31S268,68.2,262,68.2H244.11V51.58Zm-36,58.16h18.09V82.92h13.77l13.89,26.82H292l-16.2-29.45a22.27,22.27,0,0,0,13.88-20.72c0-13.25-10.41-23.45-26-23.45H226Z"/>
                          </svg>
                        </span>
                        <span class="site-title"><span>Customer </span><span>Portal</span></span>
                    </a>

                    <nav class="primary-nav hidden-sm">
                        <ul>
                            <li id="nav-products"><a class="products" data-link="mega" data-target="products-menu" href="https://access.redhat.com/products/" id="products-menu">Products &amp; Services</a></li>
                            <li id="nav-tools"><a class="tools" data-link="mega" data-target="tools-menu" href="https://access.redhat.com/labs/" id="tools-menu">Tools</a></li>
                            <li id="nav-security"><a class="security" data-link="mega" data-target="security-menu" href="https://access.redhat.com/security/" id="security-menu">Security</a></li>
                            <li id="nav-community"><a class="community" data-link="mega" data-target="community-menu" href="https://access.redhat.com/community/" id="community-menu">Community</a></li>
                        </ul>
                    </nav>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Main Mega Menu for large screens -->
<div class="mega-wrap visible-sm visible-md visible-lg" aria-hidden="true">
    <nav class="mega">
        <div class="container">
            <div class="mega-menu-wrap">

                <!-- Products Menu -->
                <div aria-labelledby="products-menu" class="products-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row">
                        <div class="col-md-6 col-sm-8">
                            <div class="root clearfix" data-portal-tour-1="7">
                                <ul class="subnav subnav-root">
                                    <li data-target="infrastructure-menu" class="active">
                                        <h3>Infrastructure and Management</h3>
                                    </li>
                                    <li data-target="cloud-menu">
                                        <h3>Cloud Computing</h3>
                                    </li>
                                    <li data-target="storage-menu">
                                        <h3>Storage</h3>
                                    </li>
                                    <li data-target="jboss-dev-menu">
                                        <h3>Runtimes</h3>
                                    </li>
                                    <li data-target="jboss-int-menu">
                                        <h3>Integration and Automation</h3>
                                    </li>
                                </ul>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="infrastructure-menu" style="display: block;">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-virtualization/">Red Hat Virtualization</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/identity-management/">Red Hat Identity Management</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-directory-server/">Red Hat Directory Server</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-certificate-system/">Red Hat Certificate System</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-update-infrastructure/">Red Hat Update Infrastructure</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-insights/">Red Hat Insights</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/ansible-tower-red-hat/">Red Hat Ansible Tower</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-ansible-engine/">Red Hat Ansible Engine</a>
                                        </li>
                                    </ul>
                                </div>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="cloud-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-cloudforms/">Red Hat CloudForms</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-cloud-suite/">Red Hat Cloud Suite</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/openshift-online-red-hat/">Red Hat OpenShift Online</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay</a>
                                        </li>
                                    </ul>
                                </div>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="storage-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage</a>
                                        </li>
                                        <li>
                                          <a href="https://access.redhat.com/products/red-hat-openshift-container-storage">Red Hat Openshift Container Storage</a>
                                        </li>
                                    </ul>
                                </div>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="jboss-dev-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/thorntail/">Red Hat build of Thorntail</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/eclipse-vertx/">Red Hat build of Eclipse Vert.x</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/openjdk/">Red Hat build of OpenJDK</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/open-liberty/">Open Liberty</a>
                                        </li>
                                    </ul>
                                </div>
                                <!-- <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="jboss-int-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                                        </li>
                                    </ul>
                                </div> -->
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="jboss-int-menu">
                                    <ul class="border-bottom" id="portal-menu-border-bottom">
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-integration/">Red Hat Integration</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                                      </li>
                                    </ul>
                                    <ul>
                                      <li>
                                        <a href="https://access.redhat.com/products/red-hat-process-automation/">Red Hat Automation</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                                      </li>
                                    </ul>
                                </div>
                            </div>
                            <a href="https://access.redhat.com/products" class="btn btn-primary">View All Products</a>
                        </div>

                        <div class="col-md-6 col-sm-4 pull-right" data-portal-tour-1="8">
                            <div class="row">
                                <div class="col-md-6">
                                    <ul>
                                        <li><a href="https://access.redhat.com/support" class="cta-link cta-link-darkbg">Support</a></li>
                                        <li><a href="https://access.redhat.com/support/offerings/production/">Production Support</a></li>
                                        <li><a href="https://access.redhat.com/support/offerings/developer/">Development Support</a></li>
                                        <li><a href="https://access.redhat.com/support/policy/update_policies/">Product Life Cycle &amp; Update Policies</a></li>
                                    </ul>
                                    <h4 class="nav-title">Services</h4>
                                    <ul>
                                        <li><a href="https://www.redhat.com/en/services/consulting">Consulting</a></li>
                                        <li><a href="https://access.redhat.com/support/offerings/tam/">Technical Account Management</a></li>
                                        <li><a href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications</a></li>
                                    </ul>
                                </div>
                                <div class="col-md-6">
                                    <ul>
                                        <li><a href="https://access.redhat.com/documentation" class="cta-link cta-link-darkbg">Documentation</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/red_hat_enterprise_linux">Red Hat Enterprise Linux</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/red_hat_openstack_platform">Red Hat OpenStack Platform</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/openshift_container_platform">Red Hat OpenShift Container Platform</a></li>
                                    </ul>
                                    <ul>
                                        <li><a href="https://access.redhat.com/ecosystem/" class="cta-link cta-link-darkbg">Ecosystem</a></li>
                                        <li><a href="https://access.redhat.com/public-cloud">Red Hat in the Public Cloud</a></li>
                                        <li><a href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources</a></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Tools Menu -->
                <div aria-labelledby="tools-menu" class="tools-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row" data-portal-tour-1="9">
                        <div class="col-sm-12">
                            <div class="row">
                                <div class="col-sm-4">
                                    <h3 class="nav-title">Tools</h3>
                                    <ul>
                                        <li><a href="https://access.redhat.com/solution-engine">Solution Engine</a></li>
                                        <li><a href="https://access.redhat.com/downloads/content/package-browser">Packages</a></li>
                                        <li><a href="https://access.redhat.com/errata/">Errata</a></li>
                                    </ul>
                                </div>
                                <div class="col-sm-4">
                                    <ul class="list-flat">
                                        <li><a href="https://access.redhat.com/labs/" class="cta-link cta-link-darkbg">Customer Portal Labs</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=config">Configuration</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=deploy">Deployment</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=security">Security</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=troubleshoot">Troubleshooting</a></li>
                                    </ul>
                                </div>

                                <div class="col-sm-4">
                                    <div class="card card-dark-grey">
                                        <h4 class="card-heading">Red Hat Insights</h4>
                                        <p class="text-white">Increase visibility into IT operations to detect and resolve technical issues before they impact your business.</p>
                                        <ul class="list-flat rh-l-grid rh-m-gutters rh-m-all-6-col-on-md">
                                          <li><a href="https://www.redhat.com/en/technologies/management/insights" class="cta-link cta-link-md cta-link-darkbg">Learn more</a></li>
                                          <li><a href="https://cloud.redhat.com/insights" class="cta-link cta-link-md cta-link-darkbg">Go to Insights</a></li>
                                        </ul>
                                    </div>
                                </div>

                            </div>
                        </div>

                    </div>
                </div>

                <!-- Security Menu -->
                <div aria-labelledby="security-menu" class="security-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row">
                        <div class="col-sm-12 basic" data-portal-tour-1="10">

                            <div class="row">
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Red Hat Product Security Center</h2>
                                    <p class="text-white">Engage with our Red Hat Product Security team, access security updates, and ensure your environments are not exposed to any known security vulnerabilities.</p>
                                    <p><a href="https://access.redhat.com/security/" class="btn btn-primary">Product Security Center</a></p>
                                </div>

                                <div class="col-sm-4">
                                    <h2 class="nav-title">Security Updates</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories</a></li>
                                        <li><a href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database</a></li>
                                        <li><a href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs</a></li>
                                    </ul>
                                    <p class="text-white">Keep your systems secure with Red Hat&#039;s specialized responses for high-priority security vulnerabilities.</p>
                                    <ul>
                                        <li class="more-link"><a href="https://access.redhat.com/security/vulnerability">View Responses</a></li>
                                    </ul>
                                </div>

                                <div class="col-sm-4">
                                    <h2 class="nav-title">Resources</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/security/overview/">Overview</a></li>
                                        <li><a href="https://www.redhat.com/en/blog/channel/security">Security Blog</a></li>
                                        <li><a href="https://www.redhat.com/security/data/metrics/">Security Measurement</a></li>
                                        <li><a href="https://access.redhat.com/security/updates/classification/">Severity Ratings</a></li>
                                        <li><a href="https://access.redhat.com/security/updates/backporting/">Backporting Policies</a></li>
                                        <li><a href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys</a></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Community Menu -->
                <div aria-labelledby="community-menu" class="community-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row">
                        <div class="col-sm-12 basic" data-portal-tour-1="11">
                            <div class="row">
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Customer Portal Community</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/discussions?keyword=&name=&product=All&category=All&tags=All">Discussions</a></li>
                                        <li><a href="https://access.redhat.com/blogs/">Blogs</a></li>
                                        <li><a href="https://access.redhat.com/groups/">Private Groups</a></li>
                                        <p class="push-top"><a href="https://access.redhat.com/community/" class="btn btn-primary">Community Activity</a></p>
                                    </ul>
                                </div>
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Customer Events</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/convergence/">Red Hat Convergence</a></li>
                                        <li><a href="http://www.redhat.com/summit/">Red Hat Summit</a></li>
                                    </ul>
                                </div>
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Stories</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value</a></li>
                                        <li><a href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.</a></li>
                                        <li><a href="http://www.redhat.com/en/open-source">Open Source Communities</a></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <!--<a href="https://access.redhat.com/community/" class="btn btn-primary">Explore Community</a>-->
                    </div>
                </div>
            </div>
        </div>
    </nav>
</div>

            <!--[if IE 8]>
            <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                    <span class="icon-warning alert-icon" aria-hidden="true"></span>
                    You are using an unsupported web browser. Update to a supported browser for the best experience. <a href="/announcements/2120951">Read the announcement</a>.
                </div>
            </div>
            <![endif]-->
            <!--[if IE 9]>
            <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                    <span class="icon-warning alert-icon" aria-hidden="true"></span>
                    As of March 1, 2016, the Red Hat Customer Portal will no longer support Internet Explorer 9. See our new <a href="/help/browsers">browser support policy</a> for more information.
                </div>
            </div>
            <![endif]-->
            <div id="site-section"></div>
        </header>
        <!--googleon: all-->

        <main id="cp-main" class="portal-content-area">
            <div id="cp-content" class="main-content">
  <script type="text/javascript">
    chrometwo_require(['jquery-ui']);
  </script>


      <article class="rh_docs">
  <div class="container">
        <!-- Display: Book Page Content -->
    
  

<div class="row">
  <div itemscope="" itemtype="https://schema.org/TechArticle" itemref="techArticle-md1 techArticle-md2 techArticle-md3"></div>
  <div itemscope="" itemtype="https://schema.org/SoftwareApplication" itemref="softwareApplication-md1 softwareApplication-md2 softwareApplication-md3 softwareApplication-md4"></div>
  


<a class="toc-toggle toc-show" data-toggle="collapse" data-target="#toc-main" aria-expanded="false" aria-controls="toc-main">
  <span class="sr-only">Show Table of Contents</span>
  <span class="web-icon-mobile-menu" aria-hidden="true"></span>
</a>
<nav id="toc-main" class="toc-main collapse in">
  <div class="toc-menu affix-top">
    <a class="toc-toggle toc-hide" data-toggle="collapse" data-target="#toc-main" aria-expanded="false" aria-controls="toc-main">
      <span class="sr-only">Hide Table of Contents</span>
      <span class="icon-remove" aria-hidden="true"></span>
    </a>
    <div class="doc-options">
      <div class="doc-language btn-group">
        <button type="button" class="btn btn-app dropdown-toggle" data-toggle="dropdown" aria-expanded="false">
          English <span class="caret"></span>
        </button>
        <ul class="dropdown-menu" role="menu">
                      <li><a href="index.html">English</a></li>
                      <li><a href="https://access.redhat.com/documentation/ja-jp/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/">日本語</a></li>
                  </ul>
      </div>
      <div class="doc-format btn-group">
        <button type="button" class="btn btn-app dropdown-toggle" data-toggle="dropdown" aria-expanded="false">
          Single-page HTML <span class="caret"></span>
        </button>
        <ul class="dropdown-menu" role="menu">
                      <li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/">Multi-page HTML</a></li>
                                <li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/pdf/configuring_and_managing_high_availability_clusters/Red_Hat_Enterprise_Linux-8-Configuring_and_managing_high_availability_clusters-en-US.pdf">PDF</a></li>
                      <li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/epub/configuring_and_managing_high_availability_clusters/Red_Hat_Enterprise_Linux-8-Configuring_and_managing_high_availability_clusters-en-US.epub">ePub</a></li>
                  </ul>
      </div>
    </div>
    <ol class="menu"><li class=" leaf"><a href="index.html">Configuring and managing high availability clusters</a></li><li class=" leaf"><a href="index.html#proc_providing-feedback-on-red-hat-documentation_configuring-and-managing-high-availability-clusters">Providing feedback on Red Hat documentation</a></li><li class=" leaf"><a href="index.html#assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters">1. High Availability Add-On overview</a><ol class="menu"><li class=" leaf"><a href="index.html#con_high-availability-add-on-concepts-overview-of-high-availability">1.1. High Availability Add-On components</a></li><li class=" leaf"><a href="index.html#s1-Pacemakeroverview-HAAO">1.2. Pacemaker overview</a><ol class="menu"><li class=" leaf"><a href="index.html#s2-Pacemakerarchitecture-HAAO">1.2.1. Pacemaker architecture components</a></li><li class=" leaf"><a href="index.html#s2-Pacemakertools-HAAO">1.2.2. Configuration and management tools</a></li><li class=" leaf"><a href="index.html#s1-configfileoverview-HAAR">1.2.3. The cluster and pacemaker configuration files</a></li></ol></li><li class=" leaf"><a href="index.html#s1-fencing-HAAO">1.3. Fencing overview</a></li><li class=" leaf"><a href="index.html#s1-quorumoverview-HAAO">1.4. Quorum overview</a></li><li class=" leaf"><a href="index.html#s1-resourceoverview-HAAO">1.5. Resource overview</a></li><li class=" leaf"><a href="index.html#con_HA-lvm-shared-volumes-overview-of-high-availability">1.6. LVM logical volumes in a Red Hat high availability cluster</a><ol class="menu"><li class=" leaf"><a href="index.html#choosing_ha_lvm_or_shared_volumes">1.6.1. Choosing HA-LVM or shared volumes</a></li><li class=" leaf"><a href="index.html#configuring_lvm_volumes_in_a_cluster">1.6.2. Configuring LVM volumes in a cluster</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#assembly_getting-started-with-pacemaker-configuring-and-managing-high-availability-clusters">2. Getting started with Pacemaker</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_learning-to-use-pacemaker-getting-started-with-pacemaker">2.1. Learning to use Pacemaker</a></li><li class=" leaf"><a href="index.html#proc_learning-to-configure-failover-getting-started-with-pacemaker">2.2. Learning to configure failover</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_pcs-operation-configuring-and-managing-high-availability-clusters">3. The pcs command line interface</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_pcs-help-pcs-operation">3.1. pcs help display</a></li><li class=" leaf"><a href="index.html#proc_raw-config-pcs-operation">3.2. Viewing the raw cluster configuration</a></li><li class=" leaf"><a href="index.html#proc_configure-testfile-pcs-operation">3.3. Saving a configuration change to a working file</a></li><li class=" leaf"><a href="index.html#proc_cluster-status-pcs-operation">3.4. Displaying cluster status</a></li><li class=" leaf"><a href="index.html#proc_cluster-config-display-pcs-operation">3.5. Displaying the full cluster configuration</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters">4. Creating a Red Hat High-Availability cluster with Pacemaker</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_installing-cluster-software-creating-high-availability-cluster">4.1. Installing cluster software</a></li><li class=" leaf"><a href="index.html#proc_installing-pcp-zeroconf-creating-high-availability-cluster">4.2. Installing the pcp-zeroconf package (recommended)</a></li><li class=" leaf"><a href="index.html#proc_creating-high-availability-cluster-creating-high-availability-cluster">4.3. Creating a high availability cluster</a></li><li class=" leaf"><a href="index.html#proc_configure-multiple-ip-creating-high-availability-cluster">4.4. Creating a high availability cluster with multiple links</a></li><li class=" leaf"><a href="index.html#proc_configuring-fencing-creating-high-availability-cluster">4.5. Configuring fencing</a></li><li class=" leaf"><a href="index.html#proc_cluster-backup-creating-high-availability-cluster">4.6. Backing up and restoring a cluster configuration</a></li><li class=" leaf"><a href="index.html#proc_enabling-ports-for-high-availability-creating-high-availability-cluster">4.7. Enabling ports for the High Availability Add-On</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters">5. Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http">5.1. Configuring an LVM volume with an ext4 file system in a Pacemaker cluster</a></li><li class=" leaf"><a href="index.html#proc_configuring-apache-http-web-server-configuring-ha-http">5.2. Configuring an Apache HTTP Server</a></li><li class=" leaf"><a href="index.html#proc_configuring-resources-for-http-server-in-a-cluster-configuring-ha-http">5.3. Creating the resources and resource groups</a></li><li class=" leaf"><a href="index.html#proc_testing-resource-configuration-in-a-cluster-configuring-ha-http">5.4. Testing the resource configuration</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters">6. Configuring an active/passive NFS server in a Red Hat High Availability cluster</a><ol class="menu"><li class=" leaf"><a href="index.html#prerequisites">6.1. Prerequisites</a></li><li class=" leaf"><a href="index.html#procedural_overview">6.2. Procedural overview</a></li><li class=" leaf"><a href="index.html#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs">6.3. Configuring an LVM volume with an ext4 file system in a Pacemaker cluster</a></li><li class=" leaf"><a href="index.html#proc_configuring-nfs-share-configuring-ha-nfs">6.4. Configuring an NFS share</a></li><li class=" leaf"><a href="index.html#proc_configuring_resources_for_nfs_server_in_a_cluster-configuring-ha-nfs">6.5. Configuring the resources and resource group for an NFS server in a cluster</a></li><li class=" leaf"><a href="index.html#proc_testing-nfs-resource-configuration-configuring-ha-nfs">6.6. Testing the NFS resource configuration</a><ol class="menu"><li class=" leaf"><a href="index.html#testing_the_nfs_export">6.6.1. Testing the NFS export</a></li><li class=" leaf"><a href="index.html#testing_for_failover">6.6.2. Testing for failover</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters">7. GFS2 file systems in a cluster</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster">7.1. Configuring a GFS2 file system in a cluster</a></li><li class=" leaf"><a href="index.html#proc_migrate-gfs2-rhel7-rhel8-configuring-gfs2-cluster">7.2. Migrating a GFS2 file system from RHEL7 to RHEL8</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters">8. Getting started with the pcsd Web UI</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_installing-cluster-software-getting-started-with-the-pcsd-web-ui">8.1. Installing cluster software</a></li><li class=" leaf"><a href="index.html#proc_setting-up-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui">8.2. Setting up the pcsd Web UI</a></li><li class=" leaf"><a href="index.html#assembly_creating-a-cluster-with-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui">8.3. Creating a cluster with the pcsd Web UI</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_configuring-advanced-cluster-options-with-the-pcsd-web-ui-creating-a-cluster-with-the-pcsd-web-ui">8.3.1. Configuring advanced cluster configuration options with the pcsd Web UI</a></li><li class=" leaf"><a href="index.html#proc_setting-permissions-with-the-pcsd-web-ui-creating-a-cluster-with-the-pcsd-web-ui">8.3.2. Setting cluster management permissions</a></li></ol></li><li class=" leaf"><a href="index.html#proc_configuring-cluster-components-with-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui">8.4. Configuring cluster components with the pcsd Web UI</a><ol class="menu"><li class=" leaf"><a href="index.html#s2-guiclustnodes-HAAR">8.4.1. Configuring cluster nodes with the pcsd Web UI</a></li><li class=" leaf"><a href="index.html#s2-guiclustresources-HAAR">8.4.2. Configuring cluster resources with the pcsd Web UI</a></li><li class=" leaf"><a href="index.html#s2-guifencedevices-HAAR">8.4.3. Configuring fence devices with the pcsd Web UI</a></li><li class=" leaf"><a href="index.html#s2-guiaclset-HAAR">8.4.4. Configuring ACLs with the pcsd Web UI</a></li><li class=" leaf"><a href="index.html#s2-guiclustprops-HAAR">8.4.5. Configuring cluster properties with the pcsd Web UI</a></li></ol></li><li class=" leaf"><a href="index.html#proc_configuring-ha-pcsd-web-ui-getting-started-with-the-pcsd-web-ui">8.5. Configuring a high availability pcsd Web UI</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_configuring-fencing-configuring-and-managing-high-availability-clusters">9. Configuring fencing in a Red Hat High Availability cluster</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_displaying-fence-agents-configuring-fencing">9.1. Displaying available fence agents and their options</a></li><li class=" leaf"><a href="index.html#proc_creating-fence-devices-configuring-fencing">9.2. Creating a fence device</a></li><li class=" leaf"><a href="index.html#ref_general-fence-device-properties-configuring-fencing">9.3. General properties of fencing devices</a></li><li class=" leaf"><a href="index.html#ref_advanced-fence-device-properties-configuring-fencing">9.4. Advanced fencing configuration options</a></li><li class=" leaf"><a href="index.html#proc_testing-fence-devices-configuring-fencing">9.5. Testing a fence device</a></li><li class=" leaf"><a href="index.html#proc_configuring-fencing-levels-configuring-fencing">9.6. Configuring fencing levels</a></li><li class=" leaf"><a href="index.html#proc_configuring-fencing-for-redundant-power-configuring-fencing">9.7. Configuring fencing for redundant power supplies</a></li><li class=" leaf"><a href="index.html#proc_displaying-configuring-fence-devices-configuring-fencing">9.8. Displaying configured fence devices</a></li><li class=" leaf"><a href="index.html#proc_modifying-fence-devices-configuring-fencing">9.9. Modifying and deleting fence devices</a></li><li class=" leaf"><a href="index.html#proc_manually-fencing-a-node-configuring-fencing">9.10. Manually fencing a cluster node</a></li><li class=" leaf"><a href="index.html#proc_disabling-a-fence-device-configuring-fencing">9.11. Disabling a fence device</a></li><li class=" leaf"><a href="index.html#proc_preventing-a-node-from-using-a-fence-device-configuring-fencing">9.12. Preventing a node from using a fence device</a></li><li class=" leaf"><a href="index.html#proc_configuring-acpi-for-fence-devices-configuring-fencing">9.13. Configuring ACPI for use with integrated fence devices</a><ol class="menu"><li class=" leaf"><a href="index.html#s2-bios-setting-CA">9.13.1. Disabling ACPI Soft-Off with the BIOS</a></li><li class=" leaf"><a href="index.html#s2-acpi-disable-logind-CA">9.13.2. Disabling ACPI Soft-Off in the logind.conf file</a></li><li class=" leaf"><a href="index.html#s2-acpi-disable-boot-CA">9.13.3. Disabling ACPI completely in the GRUB 2 File</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters">10. Configuring cluster resources</a><ol class="menu"><li class=" leaf"><a href="index.html#ref_resource-properties.adoc-configuring-cluster-resources">10.1. Resource agent identifiers</a></li><li class=" leaf"><a href="index.html#proc_displaying-resource-specific-parameters-configuring-cluster-resources">10.2. Displaying resource-specific parameters</a></li><li class=" leaf"><a href="index.html#proc_configuring-resource-meta-options-configuring-cluster-resources">10.3. Configuring resource meta options</a><ol class="menu"><li class=" leaf"><a href="index.html#changing_the_default_value_of_a_resource_option">10.3.1. Changing the default value of a resource option</a></li><li class=" leaf"><a href="index.html#displaying_currently_configured_resource_defaults">10.3.2. Displaying currently configured resource defaults</a></li><li class=" leaf"><a href="index.html#setting_meta_options_on_resource_creation">10.3.3. Setting meta options on resource creation</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_resource-groups-configuring-cluster-resources">10.4. Configuring resource groups</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_creating-resource-groups-resourceegroups">10.4.1. Creating a resource group</a></li><li class=" leaf"><a href="index.html#removing_a_resource_group">10.4.2. Removing a resource group</a></li><li class=" leaf"><a href="index.html#displaying_resource_groups">10.4.3. Displaying resource groups</a></li><li class=" leaf"><a href="index.html#s2-group_options-HAAR">10.4.4. Group options</a></li><li class=" leaf"><a href="index.html#s2-group_stickiness-HAAR">10.4.5. Group stickiness</a></li></ol></li><li class=" leaf"><a href="index.html#con_determining-resource-behavior-configuring-cluster-resources">10.5. Determining resource behavior</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters">11. Determining which nodes a resource can run on</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_configuring-location-constraints-determining-which-node-a-resource-runs-on">11.1. Configuring location constraints</a></li><li class=" leaf"><a href="index.html#proc_limiting-resource-discovery-to-a-subset-of-nodes-determining-which-node-a-resource-runs-on">11.2. Limiting resource discovery to a subset of nodes</a></li><li class=" leaf"><a href="index.html#proc_configuring-location-constraint-strategy.adoc-determining-which-node-a-resource-runs-on">11.3. Configuring a location constraint strategy</a><ol class="menu"><li class=" leaf"><a href="index.html#s3-optin-clusters-HAAR">11.3.1. Configuring an &quot;Opt-In&quot; Cluster</a></li><li class=" leaf"><a href="index.html#s3-optout-clusters-HAAR">11.3.2. Configuring an &quot;Opt-Out&quot; Cluster</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters">12. Determining the order in which cluster resources are run</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_configuring-mandatory-ordering.adoc-determining-resource-order">12.1. Configuring mandatory ordering</a></li><li class=" leaf"><a href="index.html#proc_configuring-advisory-ordering.adoc-determining-resource-order">12.2. Configuring advisory ordering</a></li><li class=" leaf"><a href="index.html#proc_configuring-ordered-resource-sets.adocdetermining-resource-order">12.3. Configuring ordered resource sets</a></li><li class=" leaf"><a href="index.html#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order">12.4. Configuring startup order for resource dependencies not managed by Pacemaker</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters">13. Colocating cluster resources</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_specifying-mandatory-placement.adoc-colocating-cluster-resources">13.1. Specifying mandatory placement of resources</a></li><li class=" leaf"><a href="index.html#proc_specifying-advisory-placement.adoc-colocating-cluster-resources">13.2. Specifying advisory placement of resources</a></li><li class=" leaf"><a href="index.html#proc_colocating-resource-sets.adoc-colocating-cluster-resources">13.3. Colocating sets of resources</a></li><li class=" leaf"><a href="index.html#removing_colocation_constraints">13.4. Removing Colocation Constraints</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_displaying-resource-constraints.adoc-configuring-and-managing-high-availability-clusters">14. Displaying resource constraints</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_displaying-resource-constraints.adoc-displaying-resource-constraints">14.1. Displaying all configured constraints</a></li><li class=" leaf"><a href="index.html#displaying_location_constraints">14.2. Displaying location constraints</a></li><li class=" leaf"><a href="index.html#displaying_ordering_constraints">14.3. Displaying ordering constraints</a></li><li class=" leaf"><a href="index.html#displaying_colocation_constraints">14.4. Displaying colocation constraints</a></li><li class=" leaf"><a href="index.html#displaying_resource_specific_constraints">14.5. Displaying resource-specific constraints</a></li><li class=" leaf"><a href="index.html#displaying_resource_dependencies_red_hat_enterprise_linux_8_2_and_later">14.6. Displaying resource dependencies (Red Hat Enterprise Linux 8.2 and later)</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters">15. Determining resource location with rules</a><ol class="menu"><li class=" leaf"><a href="index.html#ref_pacemaker-rules.adoc-determining-resource-location-with-rules">15.1. Pacemaker rules</a><ol class="menu"><li class=" leaf"><a href="index.html#node_attribute_expressions">15.1.1. Node attribute expressions</a></li><li class=" leaf"><a href="index.html#time_date_based_expressions">15.1.2. Time/date based expressions</a></li><li class=" leaf"><a href="index.html#date_specifications">15.1.3. Date specifications</a></li></ol></li><li class=" leaf"><a href="index.html#ref_configuring-constraint-using-rules.adoc-determining-resource-location-with-rules">15.2. Configuring a pacemaker location constraint using rules</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_managing-cluster-resources-configuring-and-managing-high-availability-clusters">16. Managing cluster resources</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_display-configured-resources-managing-cluster-resources">16.1. Displaying configured resources</a></li><li class=" leaf"><a href="index.html#proc_modify-resource-parameters-managing-cluster-resources">16.2. Modifying resource parameters</a></li><li class=" leaf"><a href="index.html#proc_cleanup-cluster-resources-managing-cluster-resources">16.3. Clearing failure status of cluster resources</a></li><li class=" leaf"><a href="index.html#assembly_moving-cluster-resources-managing-cluster-resources">16.4. Moving resources in a cluster</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_move-resource-from-failure-moving-cluster-resources">16.4.1. Moving resources due to failure</a></li><li class=" leaf"><a href="index.html#proc_move-resource-from-connectivity-moving-cluster-resources">16.4.2. Moving resources due to connectivity changes</a></li></ol></li><li class=" leaf"><a href="index.html#proc_disabling-monitor-operationmanaging-cluster-resources">16.5. Disabling a monitor operation</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters">17. Creating cluster resources that are active on multiple nodes (cloned resources)</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_creating-cloned-resource-creating-multinode-resources">17.1. Creating and removing a cloned resource</a></li><li class=" leaf"><a href="index.html#proc_configuring-clone-constraints-creating-multinode-resources">17.2. Configuring clone resource constraints</a></li><li class=" leaf"><a href="index.html#assembly_creating-promotable-clone-resources-creating-multinode-resources">17.3. Creating promotable clone resources</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_-creating-promotable-resource-creating-promotable-clone-resources">17.3.1. Creating a promotable resource</a></li><li class=" leaf"><a href="index.html#proc_-configuring-promotable-resource-constraints-creating-promotable-clone-resources">17.3.2. Configuring promotable resource constraints</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#assembly_clusternode-management-configuring-and-managing-high-availability-clusters">18. Managing cluster nodes</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_cluster-stop-clusternode-management">18.1. Stopping cluster services</a></li><li class=" leaf"><a href="index.html#proc_cluster-enable-clusternode-management">18.2. Enabling and disabling cluster services</a></li><li class=" leaf"><a href="index.html#proc_cluster-nodeadd-clusternode-management">18.3. Adding cluster nodes</a></li><li class=" leaf"><a href="index.html#proc_cluster-noderemove-clusternode-management">18.4. Removing cluster nodes</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_cluster-permissions-configuring-and-managing-high-availability-clusters">19. Setting user permissions for a Pacemaker cluster</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_setting-cluster-access-over-network-cluster-permissions">19.1. Setting permissions for node access over a network</a></li><li class=" leaf"><a href="index.html#proc_setting-local-cluster-permissions-cluster-permissions">19.2. Setting local permissions using ACLs</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_resource-monitoring-operations-configuring-and-managing-high-availability-clusters">20. Resource monitoring operations</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_configuring-resource-monitoring-operations-resource-monitoring-operations">20.1. Configuring resource monitoring operations</a></li><li class=" leaf"><a href="index.html#proc_configuring-global-resource-operation-defaults-resource-monitoring-operations">20.2. Configuring global resource operation defaults</a></li><li class=" leaf"><a href="index.html#proc_configuring-multiple-monitoring-operations-resource-monitoring-operations">20.3. Configuring multiple monitoring operations</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters">21. Pacemaker cluster properties</a><ol class="menu"><li class=" leaf"><a href="index.html#ref_cluster-properties-options-controlling-cluster-behavior">21.1. Summary of cluster properties and options</a></li><li class=" leaf"><a href="index.html#setting-cluster-properties-controlling-cluster-behavior">21.2. Setting and removing cluster properties</a></li><li class=" leaf"><a href="index.html#proc_querying-cluster-property-settings-controlling-cluster-behavior">21.3. Querying cluster property settings</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters">22. Configuring resources to remain stopped on clean node shutdown (RHEL 8.2 and later)</a><ol class="menu"><li class=" leaf"><a href="index.html#ref_cluster-properties-shutdown-lock-configuring-resources-to-remain-stopped">22.1. Cluster properties to configure resources to remain stopped on clean node shutdown</a></li><li class=" leaf"><a href="index.html#proc_setting-shutdown-lock-configuring-resources-to-remain-stopped">22.2. Setting the shutdown-lock cluster property</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters">23. Configuring a node placement strategy</a><ol class="menu"><li class=" leaf"><a href="index.html#configuring-utilization-attributes-configuring-node-placement-strategy">23.1. Utilization attributes and placement strategy</a><ol class="menu"><li class=" leaf"><a href="index.html#configuring_node_and_resource_capacity">23.1.1. Configuring node and resource capacity</a></li><li class=" leaf"><a href="index.html#configuring_placement_strategy">23.1.2. Configuring placement strategy</a></li></ol></li><li class=" leaf"><a href="index.html#pacemaker-resource-allocation-configuring-node-placement-strategy">23.2. Pacemaker resource allocation</a><ol class="menu"><li class=" leaf"><a href="index.html#node_preference">23.2.1. Node Preference</a></li><li class=" leaf"><a href="index.html#node_capacity">23.2.2. Node Capacity</a></li><li class=" leaf"><a href="index.html#resource_allocation_preference">23.2.3. Resource Allocation Preference</a></li></ol></li><li class=" leaf"><a href="index.html#resource-placement-strategy-guidelines-configuring-node-placement-strategy">23.3. Resource placement strategy guidelines</a></li><li class=" leaf"><a href="index.html#node-utilization-resource-agent-configuring-node-placement-strategy">23.4. The NodeUtilization resource agent</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters">24. Configuring a virtual domain as a resource</a><ol class="menu"><li class=" leaf"><a href="index.html#ref_virtual-domain-resource-options-configuring-virtual-domain-as-a-resource">24.1. Virtual domain resource options</a></li><li class=" leaf"><a href="index.html#proc_creating-virtual-domain-resource-configuring-virtual-domain-as-a-resource">24.2. Creating the virtual domain resource</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters">25. Cluster quorum</a><ol class="menu"><li class=" leaf"><a href="index.html#ref_quorum-options-configuring-cluster-quorum">25.1. Configuring quorum options</a></li><li class=" leaf"><a href="index.html#proc_modifying-quorum-options-configuring-cluster-quorum">25.2. Modifying quorum options</a></li><li class=" leaf"><a href="index.html#proc_displaying-quorum-configuration-status-configuring-cluster-quorum">25.3. Displaying quorum configuration and status</a></li><li class=" leaf"><a href="index.html#proc_running-inquorate-clusters-configuring-cluster-quorum">25.4. Running inquorate clusters</a></li><li class=" leaf"><a href="index.html#assembly_configuring-quorum-devices-configuring-cluster-quorum">25.5. Quorum devices</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_installing-quorum-device-packages-configuring-quorum-devices">25.5.1. Installing quorum device packages</a></li><li class=" leaf"><a href="index.html#proc_configuring-quorum-device-configuring-quorum-devices">25.5.2. Configuring a quorum device</a></li><li class=" leaf"><a href="index.html#proc_managing-quorum-device-service-configuring-quorum-devices">25.5.3. Managing the Quorum Device Service</a></li><li class=" leaf"><a href="index.html#proc_managing-quorum-device-settingsconfiguring-quorum-devices">25.5.4. Managing the quorum device settings in a cluster</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#assembly_configuring-pacemaker-alert-agents_configuring-and-managing-high-availability-clusters">26. Triggering scripts for cluster events</a><ol class="menu"><li class=" leaf"><a href="index.html#using-sample-alert-agents-configuring-pacemaker-alert-agents">26.1. Installing and configuring sample alert agents</a></li><li class=" leaf"><a href="index.html#creating_a_cluster_alert">26.2. Creating a cluster alert</a></li><li class=" leaf"><a href="index.html#displaying_modifying_and_removing_cluster_alerts">26.3. Displaying, modifying, and removing cluster alerts</a></li><li class=" leaf"><a href="index.html#configuring_alert_recipients">26.4. Configuring alert recipients</a></li><li class=" leaf"><a href="index.html#alert_meta_options">26.5. Alert meta options</a></li><li class=" leaf"><a href="index.html#alert_configuration_command_examples">26.6. Alert configuration command examples</a></li><li class=" leaf"><a href="index.html#writing_an_alert_agent">26.7. Writing an alert agent</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_configuring-multisite-cluster-configuring-and-managing-high-availability-clusters">27. Configuring multi-site clusters with Pacemaker</a><ol class="menu"><li class=" leaf"><a href="index.html#con_booth-cluster-ticket-manager-configuring-multisite-cluster">27.1. Overview of Booth cluster ticket manager</a></li><li class=" leaf"><a href="index.html#proc-configuring-multisite-with-booth-configuring-multisite-cluster">27.2. Configuring multi-site clusters with Pacemaker</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_remote-node-management-configuring-and-managing-high-availability-clusters">28. Integrating non-corosync nodes into a cluster: the pacemaker_remote service</a><ol class="menu"><li class=" leaf"><a href="index.html#ref_host-and-guest-authentication-of-remote-nodes-remote-node-management">28.1. Host and guest authentication of pacemaker_remote nodes</a></li><li class=" leaf"><a href="index.html#assembly_configuring-kvm-guest-nodes-remote-node-management">28.2. Configuring KVM guest nodes</a><ol class="menu"><li class=" leaf"><a href="index.html#ref_guest-node-resource-options-configuring-kvm-guest-nodes">28.2.1. Guest node resource options</a></li><li class=" leaf"><a href="index.html#proc_integrating-vm-as-guest-node-configuring-kvm-guest-nodes">28.2.2. Integrating a virtual machine as a guest node</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_configuring-remote-nodes-remote-node-management">28.3. Configuring Pacemaker remote nodes</a><ol class="menu"><li class=" leaf"><a href="index.html#ref_remote-node-resource-options-configuring-remote-nodes">28.3.1. Remote node resource options</a></li><li class=" leaf"><a href="index.html#proc-integrating-remote-nodes-configuring-remote-nodes">28.3.2. Remote node configuration overview</a></li></ol></li><li class=" leaf"><a href="index.html#proc_changing-default-port-location-remote-node-management">28.4. Changing the default port location</a></li><li class=" leaf"><a href="index.html#proc_upgrading-systems-with-pacemaker-remote-remote-node-management">28.5. Upgrading systems with pacemaker_remote nodes</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters">29. Performing cluster maintenance</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_stopping-individual-node-cluster-maintenance">29.1. Putting a node into standby mode</a></li><li class=" leaf"><a href="index.html#assembly_manually-move-resources-cluster-maintenance">29.2. Manually moving cluster resources</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_moving-resource-from-node-manually-move-resources">29.2.1. Moving a resource from its current node</a></li><li class=" leaf"><a href="index.html#proc_moving-resource-to-preferred-node-manually-move-resources">29.2.2. Moving a resource to its preferred node</a></li></ol></li><li class=" leaf"><a href="index.html#proc_disabling-resources-cluster-maintenance">29.3. Disabling, enabling, and banning cluster resources</a></li><li class=" leaf"><a href="index.html#proc_unmanaging-resources-cluster-maintenance">29.4. Setting a resource to unmanaged mode</a></li><li class=" leaf"><a href="index.html#proc_setting-maintenance-mode-cluster-maintenance">29.5. Putting a cluster in maintenance mode</a></li><li class=" leaf"><a href="index.html#proc_updating-cluster-packages-cluster-maintenance">29.6. Updating a RHEL high availability cluster</a></li><li class=" leaf"><a href="index.html#proc_upgrading-remote-nodes-cluster-maintenance">29.7. Upgrading remote nodes and guest nodes</a></li></ol></li><li class=" leaf"><a href="index.html#assembly_configuring-disaster-recovery-configuring-and-managing-high-availability-clusters">30. Configuring disaster recovery clusters</a><ol class="menu"><li class=" leaf"><a href="index.html#ref_recovery-considerations-configuring-disaster-recovery">30.1. Considerations for disaster recovery clusters</a></li><li class=" leaf"><a href="index.html#proc_disaster-recovery-display-configuring-disaster-recovery">30.2. Displaying status of recovery clusters (RHEL 8.2 and later)</a></li></ol></li><li class=" leaf"><a href="index.html#idm140319475256848">Legal Notice</a></li></ol>  </div>
</nav>


  <div class="doc-wrapper">
    <div class="panel-pane pane-training-banner"  >
  
      
  
  <div class="card card-light card-md push-bottom">
  <h2 class="card-heading card-heading-red card-heading-sm card-heading-flush">Red Hat Training</h2>
  <p>A Red Hat training course is available for <a href="https://www.redhat.com/en/services/training/courses-by-curriculum?intcmp=701f2000001D4OtAAK&amp;#Red-Hat-Enterprise-Linux" class="cta-link">RHEL 8</a></p>
</div>

  
  </div>
<div class="panel-pane pane-page-title"  >
  
      
  
  <h1 class="title" itemprop="name">Configuring and managing high availability clusters</h1>
  
  </div>
<div class="panel-pane pane-page-body"  >
  
      
  
  <div class="body"><div xml:lang="en-US" class="book" id="idm140319474539200"><div class="titlepage"><div><div class="producttitle"><span class="productname">Red Hat Enterprise Linux</span> <span class="productnumber">8</span></div><div><h2 class="subtitle">Configuring and managing the Red Hat High Availability Add-On</h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat</span> <span class="orgdiv">Customer Content Services</span></div></div><div><a href="index.html#idm140319475256848">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				This guide provides information about installing, configuring, and managing the Red Hat High Availability Add-On for Red Hat Enterprise Linux 8.
			</div></div></div></div><hr/></div><section class="preface" id="proc_providing-feedback-on-red-hat-documentation_configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Providing feedback on Red Hat documentation</h1></div></div></div><p>
			We appreciate your input on our documentation. Please let us know how we could make it better. To do so:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
					For simple comments on specific passages:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Make sure you are viewing the documentation in the <span class="emphasis"><em>Multi-page HTML</em></span> format. In addition, ensure you see the <span class="strong strong"><strong>Feedback</strong></span> button in the upper right corner of the document.
						</li><li class="listitem">
							Use your mouse cursor to highlight the part of text that you want to comment on.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>Add Feedback</strong></span> pop-up that appears below the highlighted text.
						</li><li class="listitem">
							Follow the displayed instructions.
						</li></ol></div></li><li class="listitem"><p class="simpara">
					For submitting more complex feedback, create a Bugzilla ticket:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Go to the <a class="link" href="https://bugzilla.redhat.com/enter_bug.cgi?product=Red%20Hat%20Enterprise%20Linux%208">Bugzilla</a> website.
						</li><li class="listitem">
							As the Component, use <span class="strong strong"><strong>Documentation</strong></span>.
						</li><li class="listitem">
							Fill in the <span class="strong strong"><strong>Description</strong></span> field with your suggestion for improvement. Include a link to the relevant part(s) of documentation.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Submit Bug</strong></span>.
						</li></ol></div></li></ul></div></section><section class="chapter" id="assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 1. High Availability Add-On overview</h1></div></div></div><p>
			The High Availability Add-On is a clustered system that provides reliability, scalability, and availability to critical production services.
		</p><p>
			A cluster is two or more computers (called <span class="emphasis"><em>nodes</em></span> or <span class="emphasis"><em>members</em></span>) that work together to perform a task. Clusters can be used to provide highly available services or resources. The redundancy of multiple machines is used to guard against failures of many types.
		</p><p>
			High availability clusters provide highly available services by eliminating single points of failure and by failing over services from one cluster node to another in case a node becomes inoperative. Typically, services in a high availability cluster read and write data (by means of read-write mounted file systems). Therefore, a high availability cluster must maintain data integrity as one cluster node takes over control of a service from another cluster node. Node failures in a high availability cluster are not visible from clients outside the cluster. (High availability clusters are sometimes referred to as failover clusters.) The High Availability Add-On provides high availability clustering through its high availability service management component, <code class="literal command">Pacemaker</code>.
		</p><section class="section" id="con_high-availability-add-on-concepts-overview-of-high-availability"><div class="titlepage"><div><div><h2 class="title">1.1. High Availability Add-On components</h2></div></div></div><p>
				The High Availability Add-On consists of the following major components:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Cluster infrastructure — Provides fundamental functions for nodes to work together as a cluster: configuration file management, membership management, lock management, and fencing.
					</li><li class="listitem">
						High availability service management — Provides failover of services from one cluster node to another in case a node becomes inoperative.
					</li><li class="listitem">
						Cluster administration tools — Configuration and management tools for setting up, configuring, and managing the High Availability Add-On. The tools are for use with the cluster infrastructure components, the high availability and service management components, and storage.
					</li></ul></div><p>
				You can supplement the High Availability Add-On with the following components:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Red Hat GFS2 (Global File System 2) — Part of the Resilient Storage Add-On, this provides a cluster file system for use with the High Availability Add-On. GFS2 allows multiple nodes to share storage at a block level as if the storage were connected locally to each cluster node. GFS2 cluster file system requires a cluster infrastructure.
					</li><li class="listitem">
						LVM Locking Daemon (<code class="literal">lvmlockd</code>) — Part of the Resilient Storage Add-On, this provides volume management of cluster storage. <code class="literal">lvmlockd</code> support also requires cluster infrastructure.
					</li><li class="listitem">
						Load Balancer Add-On — Routing software that provides high availability load balancing and failover in layer 4 (TCP) and layer 7 (HTTP, HTTPS) services. The Load Balancer Add-On runs in a cluster of redundant virtual routers that uses load algorithms to distribute client requests to real servers, collectively acting as a virtual server. It is not necessary to use the Load Balancer Add-On in conjunction with Pacemaker.
					</li></ul></div></section><section class="section" id="s1-Pacemakeroverview-HAAO"><div class="titlepage"><div><div><h2 class="title">1.2. Pacemaker overview</h2></div></div></div><p>
				Pacemaker is a cluster resource manager. It achieves maximum availability for your cluster services and resources by making use of the cluster infrastructure’s messaging and membership capabilities to deter and recover from node and resource-level failure.
			</p><section class="section" id="s2-Pacemakerarchitecture-HAAO"><div class="titlepage"><div><div><h3 class="title">1.2.1. Pacemaker architecture components</h3></div></div></div><p>
					A cluster configured with Pacemaker comprises separate component daemons that monitor cluster membership, scripts that manage the services, and resource management subsystems that monitor the disparate resources.
				</p><p>
					The following components form the Pacemaker architecture:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Cluster Information Base (CIB)</span></dt><dd>
								The Pacemaker information daemon, which uses XML internally to distribute and synchronize current configuration and status information from the Designated Coordinator (DC) — a node assigned by Pacemaker to store and distribute cluster state and actions by means of the CIB — to all other cluster nodes.
							</dd><dt><span class="term">Cluster Resource Management Daemon (CRMd)</span></dt><dd><p class="simpara">
								Pacemaker cluster resource actions are routed through this daemon. Resources managed by CRMd can be queried by client systems, moved, instantiated, and changed when needed.
							</p><p class="simpara">
								Each cluster node also includes a local resource manager daemon (LRMd) that acts as an interface between CRMd and resources. LRMd passes commands from CRMd to agents, such as starting and stopping and relaying status information.
							</p></dd><dt><span class="term">Shoot the Other Node in the Head (STONITH)</span></dt><dd>
								STONITH is the Pacemaker fencing implementation. It acts as a cluster resource in Pacemaker that processes fence requests, forcefully powering down nodes and removing them from the cluster to ensure data integrity. STONITH is configured in the CIB and can be monitored as a normal cluster resource. For a general overview of fencing, see <a class="xref" href="index.html#s1-fencing-HAAO" title="1.3. Fencing overview">Section 1.3, “Fencing overview”</a>.
							</dd><dt><span class="term">corosync</span></dt><dd><p class="simpara">
								<code class="literal">corosync</code> is the component - and a daemon of the same name - that serves the core membership and member-communication needs for high availability clusters. It is required for the High Availability Add-On to function.
							</p><p class="simpara">
								In addition to those membership and messaging functions, <code class="literal">corosync</code> also:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Manages quorum rules and determination.
									</li><li class="listitem">
										Provides messaging capabilities for applications that coordinate or operate across multiple members of the cluster and thus must communicate stateful or other information between instances.
									</li><li class="listitem">
										Uses the <code class="literal">kronosnet</code> library as its network transport to provide multiple redundant links and automatic failover.
									</li></ul></div></dd></dl></div></section><section class="section" id="s2-Pacemakertools-HAAO"><div class="titlepage"><div><div><h3 class="title">1.2.2. Configuration and management tools</h3></div></div></div><p>
					The High Availability Add-On features two configuration tools for cluster deployment, monitoring, and management.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal command">pcs</code></span></dt><dd><p class="simpara">
								The <code class="literal command">pcs</code> command line interface controls and configures Pacemaker and the <code class="literal">corosync</code> heartbeat daemon. A command-line based program, <code class="literal command">pcs</code> can perform the following cluster management tasks:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Create and configure a Pacemaker/Corosync cluster
									</li><li class="listitem">
										Modify configuration of the cluster while it is running
									</li><li class="listitem">
										Remotely configure both Pacemaker and Corosync as well as start, stop, and display status information of the cluster
									</li></ul></div></dd><dt><span class="term"><code class="literal command">pcsd</code> Web UI</span></dt><dd>
								A graphical user interface to create and configure Pacemaker/Corosync clusters.
							</dd></dl></div></section><section class="section" id="s1-configfileoverview-HAAR"><div class="titlepage"><div><div><h3 class="title">1.2.3. The cluster and pacemaker configuration files</h3></div></div></div><p>
					The configuration files for the Red Hat High Availability Add-On are <code class="literal">corosync.conf</code> and <code class="literal">cib.xml</code>.
				</p><p>
					The <code class="literal">corosync.conf</code> file provides the cluster parameters used by <code class="literal">corosync</code>, the cluster manager that Pacemaker is built on. In general, you should not edit the <code class="literal">corosync.conf</code> directly but, instead, use the <code class="literal command">pcs</code> or <code class="literal command">pcsd</code> interface.
				</p><p>
					The <code class="literal">cib.xml</code> file is an XML file that represents both the cluster’s configuration and the current state of all resources in the cluster. This file is used by Pacemaker’s Cluster Information Base (CIB). The contents of the CIB are automatically kept in sync across the entire cluster. Do not edit the <code class="literal">cib.xml</code> file directly; use the <code class="literal command">pcs</code> or <code class="literal command">pcsd</code> interface instead.
				</p></section></section><section class="section" id="s1-fencing-HAAO"><div class="titlepage"><div><div><h2 class="title">1.3. Fencing overview</h2></div></div></div><p>
				If communication with a single node in the cluster fails, then other nodes in the cluster must be able to restrict or release access to resources that the failed cluster node may have access to. This cannot be accomplished by contacting the cluster node itself as the cluster node may not be responsive. Instead, you must provide an external method, which is called fencing with a fence agent. A fence device is an external device that can be used by the cluster to restrict access to shared resources by an errant node, or to issue a hard reboot on the cluster node.
			</p><p>
				Without a fence device configured you do not have a way to know that the resources previously used by the disconnected cluster node have been released, and this could prevent the services from running on any of the other cluster nodes. Conversely, the system may assume erroneously that the cluster node has released its resources and this can lead to data corruption and data loss. Without a fence device configured data integrity cannot be guaranteed and the cluster configuration will be unsupported.
			</p><p>
				When the fencing is in progress no other cluster operation is allowed to run. Normal operation of the cluster cannot resume until fencing has completed or the cluster node rejoins the cluster after the cluster node has been rebooted.
			</p><p>
				For more information about fencing, see <a class="link" href="https://access.redhat.com/solutions/15575"> Fencing in a Red Hat High Availability Cluster</a>.
			</p></section><section class="section" id="s1-quorumoverview-HAAO"><div class="titlepage"><div><div><h2 class="title">1.4. Quorum overview</h2></div></div></div><p>
				In order to maintain cluster integrity and availability, cluster systems use a concept known as <span class="emphasis"><em>quorum</em></span> to prevent data corruption and loss. A cluster has quorum when more than half of the cluster nodes are online. To mitigate the chance of data corruption due to failure, Pacemaker by default stops all resources if the cluster does not have quorum.
			</p><p>
				Quorum is established using a voting system. When a cluster node does not function as it should or loses communication with the rest of the cluster, the majority working nodes can vote to isolate and, if needed, fence the node for servicing.
			</p><p>
				For example, in a 6-node cluster, quorum is established when at least 4 cluster nodes are functioning. If the majority of nodes go offline or become unavailable, the cluster no longer has quorum and Pacemaker stops clustered services.
			</p><p>
				The quorum features in Pacemaker prevent what is also known as <span class="emphasis"><em>split-brain</em></span>, a phenomenon where the cluster is separated from communication but each part continues working as separate clusters, potentially writing to the same data and possibly causing corruption or loss. For more information on what it means to be in a split-brain state, and on quorum concepts in general, see <a class="link" href="https://access.redhat.com/articles/2824071">Exploring Concepts of RHEL High Availability Clusters - Quorum</a>.
			</p><p>
				A Red Hat Enterprise Linux High Availability Add-On cluster uses the <code class="literal">votequorum</code> service, in conjunction with fencing, to avoid split brain situations. A number of votes is assigned to each system in the cluster, and cluster operations are allowed to proceed only when a majority of votes is present.
			</p></section><section class="section" id="s1-resourceoverview-HAAO"><div class="titlepage"><div><div><h2 class="title">1.5. Resource overview</h2></div></div></div><p>
				A <span class="emphasis"><em>cluster resource</em></span> is an instance of program, data, or application to be managed by the cluster service. These resources are abstracted by <span class="emphasis"><em>agents</em></span> that provide a standard interface for managing the resource in a cluster environment.
			</p><p>
				To ensure that resources remain healthy, you can add a monitoring operation to a resource’s definition. If you do not specify a monitoring operation for a resource, one is added by default.
			</p><p>
				You can determine the behavior of a resource in a cluster by configuring <span class="emphasis"><em>constraints</em></span>. You can configure the following categories of constraints:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						location constraints — A location constraint determines which nodes a resource can run on.
					</li><li class="listitem">
						ordering constraints — An ordering constraint determines the order in which the resources run.
					</li><li class="listitem">
						colocation constraints — A colocation constraint determines where resources will be placed relative to other resources.
					</li></ul></div><p>
				One of the most common elements of a cluster is a set of resources that need to be located together, start sequentially, and stop in the reverse order. To simplify this configuration, Pacemaker supports the concept of <span class="emphasis"><em>groups</em></span>.
			</p></section><section class="section" id="con_HA-lvm-shared-volumes-overview-of-high-availability"><div class="titlepage"><div><div><h2 class="title">1.6. LVM logical volumes in a Red Hat high availability cluster</h2></div></div></div><p>
				The Red Hat High Availability Add-On provides support for LVM volumes in two distinct cluster configurations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						High availability LVM volumes (HA-LVM) in active/passive failover configurations in which only a single node of the cluster accesses the storage at any one time.
					</li><li class="listitem">
						LVM volumes that use the <code class="literal">lvmlockd</code> daemon to manage storage devices in active/active configurations in which more than one node of the cluster requires access to the storage at the same time. The <code class="literal">lvmlockd</code> daemon is part of the Resilient Storage Add-On.
					</li></ul></div><section class="section" id="choosing_ha_lvm_or_shared_volumes"><div class="titlepage"><div><div><h3 class="title">1.6.1. Choosing HA-LVM or shared volumes</h3></div></div></div><p>
					When to use HA-LVM or shared logical volumes managed by the <code class="literal">lvmlockd</code> daemon should be based on the needs of the applications or services being deployed.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If multiple nodes of the cluster require simultaneous read/write access to LVM volumes in an active/active system, then you must use the <code class="literal">lvmlockd</code> daemon and configure your volumes as shared volumes. The <code class="literal">lvmlockd</code> daemon provides a system for coordinating activation of and changes to LVM volumes across nodes of a cluster concurrently. The <code class="literal">lvmlockd</code> daemon’s locking service provides protection to LVM metadata as various nodes of the cluster interact with volumes and make changes to their layout. This protection is contingent upon configuring any volume group that will be activated simultaneously across multiple cluster nodes as a shared volume.
						</li><li class="listitem">
							If the high availability cluster is configured to manage shared resources in an active/passive manner with only one single member needing access to a given LVM volume at a time, then you can use HA-LVM without the <code class="literal">lvmlockd</code> locking service.
						</li></ul></div><p>
					Most applications will run better in an active/passive configuration, as they are not designed or optimized to run concurrently with other instances. Choosing to run an application that is not cluster-aware on shared logical volumes may result in degraded performance. This is because there is cluster communication overhead for the logical volumes themselves in these instances. A cluster-aware application must be able to achieve performance gains above the performance losses introduced by cluster file systems and cluster-aware logical volumes. This is achievable for some applications and workloads more easily than others. Determining what the requirements of the cluster are and whether the extra effort toward optimizing for an active/active cluster will pay dividends is the way to choose between the two LVM variants. Most users will achieve the best HA results from using HA-LVM.
				</p><p>
					HA-LVM and shared logical volumes using <code class="literal">lvmlockd</code> are similar in the fact that they prevent corruption of LVM metadata and its logical volumes, which could otherwise occur if multiple machines are allowed to make overlapping changes. HA-LVM imposes the restriction that a logical volume can only be activated exclusively; that is, active on only one machine at a time. This means that only local (non-clustered) implementations of the storage drivers are used. Avoiding the cluster coordination overhead in this way increases performance. A shared volume using <code class="literal">lvmlockd</code> does not impose these restrictions and a user is free to activate a logical volume on all machines in a cluster; this forces the use of cluster-aware storage drivers, which allow for cluster-aware file systems and applications to be put on top.
				</p></section><section class="section" id="configuring_lvm_volumes_in_a_cluster"><div class="titlepage"><div><div><h3 class="title">1.6.2. Configuring LVM volumes in a cluster</h3></div></div></div><p>
					In Red Hat Enterprise Linux 8, clusters are managed through Pacemaker. Both HA-LVM and shared logical volumes are supported only in conjunction with Pacemaker clusters, and must be configured as cluster resources.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							For examples of procedures for configuring an HA-LVM volume as part of a Pacemaker cluster, see <a class="link" href="index.html#assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters" title="Chapter 5. Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster">Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster</a>. and <a class="link" href="index.html#assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters" title="Chapter 6. Configuring an active/passive NFS server in a Red Hat High Availability cluster">Configuring an active/passive NFS server in a Red Hat High Availability cluster</a>.
						</p><p class="simpara">
							Note that these procedures include the following steps:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Ensuring that only the cluster is capable of activating the volume group
								</li><li class="listitem">
									Configuring an LVM logical volume
								</li><li class="listitem">
									Configuring the LVM volume as a cluster resource
								</li></ul></div></li><li class="listitem">
							For a procedure for configuring shared LVM volumes that use the <code class="literal">lvmlockd</code> daemon to manage storage devices in active/active configurations, see <a class="link" href="index.html#proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster" title="7.1. Configuring a GFS2 file system in a cluster">Configuring a GFS2 file system in a cluster</a>
						</li></ul></div></section></section></section><section class="chapter" id="assembly_getting-started-with-pacemaker-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Getting started with Pacemaker</h1></div></div></div><p>
			The following procedures provide an introduction to the tools and processes you use to create a Pacemaker cluster. They are intended for users who are interested in seeing what the cluster software looks like and how it is administered, without needing to configure a working cluster.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				These procedures do not create a supported Red Hat cluster, which requires at least two nodes and the configuration of a fencing device.
			</p></div></div><section class="section" id="proc_learning-to-use-pacemaker-getting-started-with-pacemaker"><div class="titlepage"><div><div><h2 class="title">2.1. Learning to use Pacemaker</h2></div></div></div><p>
				This example requires a single node running RHEL 8 and it requires a floating IP address that resides on the same network as one of the node’s statically assigned IP addresses.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The node used in this example is <code class="literal">z1.example.com</code>.
					</li><li class="listitem">
						The floating IP address used in this example is 192.168.122.120.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Ensure that the name of the node on which you are running is in your <code class="literal">/etc/hosts</code> file.
				</p></div></div><p>
				By working through this procedure, you will learn how to use Pacemaker to set up a cluster, how to display cluster status, and how to configure a cluster service. This example creates an Apache HTTP server as a cluster resource and shows how the cluster responds when the resource fails.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Install the Red Hat High Availability Add-On software packages from the High Availability channel, and start and enable the <code class="literal">pcsd</code> service.
					</p><pre class="literallayout"># <code class="literal">yum install pcs pacemaker fence-agents-all</code>
...
# <code class="literal">systemctl start pcsd.service</code>
# <code class="literal">systemctl enable pcsd.service</code></pre><p class="simpara">
						If you are running the <code class="literal">firewalld</code> daemon, enable the ports that are required by the Red Hat High Availability Add-On.
					</p><pre class="literallayout"># <code class="literal">firewall-cmd --permanent --add-service=high-availability</code>
# <code class="literal">firewall-cmd --reload</code></pre></li><li class="listitem"><p class="simpara">
						Set a password for user <code class="literal">hacluster</code> on each node in the cluster and authenticate user <code class="literal">hacluster</code> for each node in the cluster on the node from which you will be running the <code class="literal">pcs</code> commands. This example is using only a single node, the node from which you are running the commands, but this step is included here since it is a necessary step in configuring a supported Red Hat High Availability multi-node cluster.
					</p><pre class="literallayout"># <code class="literal">passwd hacluster</code>
...
# <code class="literal">pcs host auth z1.example.com</code></pre></li><li class="listitem"><p class="simpara">
						Create a cluster named <code class="literal">my_cluster</code> with one member and check the status of the cluster. This command creates and starts the cluster in one step.
					</p><pre class="literallayout"># <code class="literal">pcs cluster setup my_cluster --start z1.example.com</code>
...
# <code class="literal">pcs cluster status</code>
Cluster Status:
 Stack: corosync
 Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
 Last updated: Thu Oct 11 16:11:18 2018
 Last change: Thu Oct 11 16:11:00 2018 by hacluster via crmd on z1.example.com
 1 node configured
 0 resources configured

PCSD Status:
  z1.example.com: Online</pre></li><li class="listitem"><p class="simpara">
						A Red Hat High Availability cluster requires that you configure fencing for the cluster. The reasons for this requirement are described in <a class="link" href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</a>. For this introduction, however, which is intended to show only how to use the basic Pacemaker commands, disable fencing by setting the <code class="literal">stonith-enabled</code> cluster option to <code class="literal">false</code>.
					</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							The use of <code class="literal">stonith-enabled=false</code> is completely inappropriate for a production cluster. It tells the cluster to simply pretend that failed nodes are safely fenced.
						</p></div></div><pre class="literallayout"># <code class="literal">pcs property set stonith-enabled=false</code></pre></li><li class="listitem"><p class="simpara">
						Configure a web browser on your system and create a web page to display a simple text message. If you are running the <code class="literal">firewalld</code> daemon, enable the ports that are required by <code class="literal">httpd</code>.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Do not use <code class="literal command">systemctl enable</code> to enable any services that will be managed by the cluster to start at system boot.
						</p></div></div><pre class="literallayout"># <code class="literal">yum install -y httpd wget</code>
...
# <code class="literal">firewall-cmd --permanent --add-service=http</code>
# <code class="literal">firewall-cmd --reload</code>

# <code class="literal">cat &lt;&lt;-END &gt;/var/www/html/index.html</code>
<code class="literal">&lt;html&gt;</code>
<code class="literal">&lt;body&gt;My Test Site - $(hostname)&lt;/body&gt;</code>
<code class="literal">&lt;/html&gt;</code>
<code class="literal">END</code></pre><p class="simpara">
						In order for the Apache resource agent to get the status of Apache, create the following addition to the existing configuration to enable the status server URL.
					</p><pre class="literallayout"># <code class="literal">cat &lt;&lt;-END &gt; /etc/httpd/conf.d/status.conf</code>
<code class="literal">&lt;Location /server-status&gt;</code>
<code class="literal">SetHandler server-status</code>
<code class="literal">Order deny,allow</code>
<code class="literal">Deny from all</code>
<code class="literal">Allow from 127.0.0.1</code>
<code class="literal">Allow from ::1</code>
<code class="literal">&lt;/Location&gt;</code>
<code class="literal">END</code></pre></li><li class="listitem"><p class="simpara">
						Create <code class="literal">IPaddr2</code> and <code class="literal">apache</code> resources for the cluster to manage. The 'IPaddr2' resource is a floating IP address that must not be one already associated with a physical node. If the 'IPaddr2' resource’s NIC device is not specified, the floating IP must reside on the same network as the statically assigned IP address used by the node.
					</p><p class="simpara">
						You can display a list of all available resource types with the <code class="literal command">pcs resource list</code> command. You can use the <code class="literal command">pcs resource describe <span class="emphasis"><em>resourcetype</em></span></code> command to display the parameters you can set for the specified resource type. For example, the following command displays the parameters you can set for a resource of type <code class="literal">apache</code>:
					</p><pre class="literallayout"># <code class="literal">pcs resource describe apache</code>
...</pre><p class="simpara">
						In this example, the IP address resource and the apache resource are both configured as part of a group named <code class="literal">apachegroup</code>, which ensures that the resources are kept together to run on the same node when you are configuring a working multi-node cluster.
					</p><pre class="literallayout"># <code class="literal">pcs resource create ClusterIP ocf:heartbeat:IPaddr2 ip=192.168.122.120 --group apachegroup</code>

# <code class="literal">pcs resource create WebSite ocf:heartbeat:apache configfile=/etc/httpd/conf/httpd.conf statusurl="http://localhost/server-status" --group apachegroup</code>

# <code class="literal">pcs status</code>
Cluster name: my_cluster
Stack: corosync
Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
Last updated: Fri Oct 12 09:54:33 2018
Last change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com

1 node configured
2 resources configured

Online: [ z1.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com
    WebSite    (ocf::heartbeat:apache):        Started z1.example.com

PCSD Status:
  z1.example.com: Online
...</pre><p class="simpara">
						After you have configured a cluster resource, you can use the <code class="literal command">pcs resource config</code> command to display the options that are configured for that resource.
					</p><pre class="literallayout"># <code class="literal">pcs resource config WebSite</code>
Resource: WebSite (class=ocf provider=heartbeat type=apache)
 Attributes: configfile=/etc/httpd/conf/httpd.conf statusurl=http://localhost/server-status
 Operations: start interval=0s timeout=40s (WebSite-start-interval-0s)
             stop interval=0s timeout=60s (WebSite-stop-interval-0s)
             monitor interval=1min (WebSite-monitor-interval-1min)</pre></li><li class="listitem">
						Point your browser to the website you created using the floating IP address you configured. This should display the text message you defined.
					</li><li class="listitem"><p class="simpara">
						Stop the apache web service and check the cluster status. Using <code class="literal command">killall -9</code> simulates an application-level crash.
					</p><pre class="literallayout"># <code class="literal">killall -9 httpd</code></pre><p class="simpara">
						Check the cluster status. You should see that stopping the web service caused a failed action, but that the cluster software restarted the service and you should still be able to access the website.
					</p><pre class="literallayout"># <code class="literal">pcs status</code>
Cluster name: my_cluster
...
Current DC: z1.example.com (version 1.1.13-10.el7-44eb2dd) - partition with quorum
1 node and 2 resources configured

Online: [ z1.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com
    WebSite    (ocf::heartbeat:apache):        Started z1.example.com

Failed Resource Actions:
* WebSite_monitor_60000 on z1.example.com 'not running' (7): call=13, status=complete, exitreason='none',
    last-rc-change='Thu Oct 11 23:45:50 2016', queued=0ms, exec=0ms

PCSD Status:
    z1.example.com: Online</pre><p class="simpara">
						You can clear the failure status on the resource that failed once the service is up and running again and the failed action notice will no longer appear when you view the cluster status.
					</p><pre class="literallayout"># <code class="literal">pcs resource cleanup WebSite</code></pre></li><li class="listitem"><p class="simpara">
						When you are finished looking at the cluster and the cluster status, stop the cluster services on the node. Even though you have only started services on one node for this introduction, the <code class="literal">--all</code> parameter is included since it would stop cluster services on all nodes on an actual multi-node cluster.
					</p><pre class="literallayout"># <code class="literal">pcs cluster stop --all</code></pre></li></ol></div></section><section class="section" id="proc_learning-to-configure-failover-getting-started-with-pacemaker"><div class="titlepage"><div><div><h2 class="title">2.2. Learning to configure failover</h2></div></div></div><p>
				This procedure provides an introduction to creating a Pacemaker cluster running a service that will fail over from one node to another when the node on which the service is running becomes unavailable. By working through this procedure, you can learn how to create a service in a two-node cluster and you can then observe what happens to that service when it fails on the node on which it running.
			</p><p>
				This example procedure configures a two-node Pacemaker cluster running an Apache HTTP server. You can then stop the Apache service on one node to see how the service remains available.
			</p><p>
				This procedure requires as a prerequisite that you have two nodes running Red Hat Enterprise Linux 8 that can communicate with each other, and it requires a floating IP address that resides on the same network as one of the node’s statically assigned IP addresses.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The nodes used in this example are <code class="literal">z1.example.com</code> and <code class="literal">z2.example.com</code>.
					</li><li class="listitem">
						The floating IP address used in this example is 192.168.122.120.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Ensure that the names of the nodes you are using are in the <code class="literal">/etc/hosts</code> file on each node.
				</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On both nodes, install the Red Hat High Availability Add-On software packages from the High Availability channel, and start and enable the <code class="literal">pcsd</code> service.
					</p><pre class="literallayout"># <code class="literal">yum install pcs pacemaker fence-agents-all</code>
...
# <code class="literal">systemctl start pcsd.service</code>
# <code class="literal">systemctl enable pcsd.service</code></pre><p class="simpara">
						If you are running the <code class="literal">firewalld</code> daemon, on both nodes enable the ports that are required by the Red Hat High Availability Add-On.
					</p><pre class="literallayout"># <code class="literal">firewall-cmd --permanent --add-service=high-availability</code>
# <code class="literal">firewall-cmd --reload</code></pre></li><li class="listitem"><p class="simpara">
						On both nodes in the cluster, set a password for user <code class="literal">hacluster</code> .
					</p><pre class="literallayout"># <code class="literal">passwd hacluster</code></pre></li><li class="listitem"><p class="simpara">
						Authenticate user <code class="literal">hacluster</code> for each node in the cluster on the node from which you will be running the <code class="literal">pcs</code> commands.
					</p><pre class="literallayout"># <code class="literal">pcs host auth z1.example.com z2.example.com</code></pre></li><li class="listitem"><p class="simpara">
						Create a cluster named <code class="literal">my_cluster</code> with both nodes as cluster members. This command creates and starts the cluster in one step. You only need to run this from one node in the cluster because <code class="literal">pcs</code> configuration commands take effect for the entire cluster.
					</p><p class="simpara">
						On one node in cluster, run the following command.
					</p><pre class="literallayout"># <code class="literal">pcs cluster setup my_cluster --start z1.example.com z2.example.com</code></pre></li><li class="listitem"><p class="simpara">
						A Red Hat High Availability cluster requires that you configure fencing for the cluster. The reasons for this requirement are described in <a class="link" href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</a>. For this introduction, however, to show only how failover works in this configuration, disable fencing by setting the <code class="literal">stonith-enabled</code> cluster option to <code class="literal">false</code>
					</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							The use of <code class="literal">stonith-enabled=false</code> is completely inappropriate for a production cluster. It tells the cluster to simply pretend that failed nodes are safely fenced.
						</p></div></div><pre class="literallayout"># <code class="literal">pcs property set stonith-enabled=false</code></pre></li><li class="listitem"><p class="simpara">
						After creating a cluster and disabling fencing, check the status of the cluster.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							When you run the <code class="literal command">pcs cluster status</code> command, it may show output that temporarily differs slightly from the examples as the system components start up.
						</p></div></div><pre class="literallayout"># <code class="literal">pcs cluster status</code>
Cluster Status:
 Stack: corosync
 Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
 Last updated: Thu Oct 11 16:11:18 2018
 Last change: Thu Oct 11 16:11:00 2018 by hacluster via crmd on z1.example.com
 2 nodes configured
 0 resources configured

PCSD Status:
  z1.example.com: Online
  z2.example.com: Online</pre></li><li class="listitem"><p class="simpara">
						On both nodes, configure a web browser and create a web page to display a simple text message. If you are running the <code class="literal">firewalld</code> daemon, enable the ports that are required by <code class="literal">httpd</code>.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Do not use <code class="literal command">systemctl enable</code> to enable any services that will be managed by the cluster to start at system boot.
						</p></div></div><pre class="literallayout"># <code class="literal">yum install -y httpd wget</code>
...
# <code class="literal">firewall-cmd --permanent --add-service=http</code>
# <code class="literal">firewall-cmd --reload</code>

# <code class="literal">cat &lt;&lt;-END &gt;/var/www/html/index.html</code>
<code class="literal">&lt;html&gt;</code>
<code class="literal">&lt;body&gt;My Test Site - $(hostname)&lt;/body&gt;</code>
<code class="literal">&lt;/html&gt;</code>
<code class="literal">END</code></pre><p class="simpara">
						In order for the Apache resource agent to get the status of Apache, on each node in the cluster create the following addition to the existing configuration to enable the status server URL.
					</p><pre class="literallayout"># <code class="literal">cat &lt;&lt;-END &gt; /etc/httpd/conf.d/status.conf</code>
<code class="literal">&lt;Location /server-status&gt;</code>
<code class="literal">SetHandler server-status</code>
<code class="literal">Order deny,allow</code>
<code class="literal">Deny from all</code>
<code class="literal">Allow from 127.0.0.1</code>
<code class="literal">Allow from ::1</code>
<code class="literal">&lt;/Location&gt;</code>
<code class="literal">END</code></pre></li><li class="listitem"><p class="simpara">
						Create <code class="literal">IPaddr2</code> and <code class="literal">apache</code> resources for the cluster to manage. The 'IPaddr2' resource is a floating IP address that must not be one already associated with a physical node. If the 'IPaddr2' resource’s NIC device is not specified, the floating IP must reside on the same network as the statically assigned IP address used by the node.
					</p><p class="simpara">
						You can display a list of all available resource types with the <code class="literal command">pcs resource list</code> command. You can use the <code class="literal command">pcs resource describe <span class="emphasis"><em>resourcetype</em></span></code> command to display the parameters you can set for the specified resource type. For example, the following command displays the parameters you can set for a resource of type <code class="literal">apache</code>:
					</p><pre class="literallayout"># <code class="literal">pcs resource describe apache</code>
...</pre><p class="simpara">
						In this example, the IP address resource and the apache resource are both configured as part of a group named <code class="literal">apachegroup</code>, which ensures that the resources are kept together to run on the same node.
					</p><p class="simpara">
						Run the following commands from one node in the cluster:
					</p><pre class="literallayout"># <code class="literal">pcs resource create ClusterIP ocf:heartbeat:IPaddr2 ip=192.168.122.120 --group apachegroup</code>

# <code class="literal">pcs resource create WebSite ocf:heartbeat:apache configfile=/etc/httpd/conf/httpd.conf statusurl="http://localhost/server-status" --group apachegroup</code>

# <code class="literal">pcs status</code>
Cluster name: my_cluster
Stack: corosync
Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
Last updated: Fri Oct 12 09:54:33 2018
Last change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com

2 nodes configured
2 resources configured

Online: [ z1.example.com z2.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com
    WebSite    (ocf::heartbeat:apache):        Started z1.example.com

PCSD Status:
  z1.example.com: Online
  z2.example.com: Online
...</pre><p class="simpara">
						Note that in this instance, the <code class="literal">apachegroup</code> service is running on node z1.example.com.
					</p></li><li class="listitem"><p class="simpara">
						Access the website you created, stop the service on the node on which it is running, and note how the service fails over to the second node.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Point a browser to the website you created using the floating IP address you configured. This should display the text message you defined, displaying the name of the node on which the website is running.
							</li><li class="listitem"><p class="simpara">
								Stop the apache web service. Using <code class="literal command">killall -9</code> simulates an application-level crash.
							</p><pre class="literallayout"># <code class="literal">killall -9 httpd</code></pre><p class="simpara">
								Check the cluster status. You should see that stopping the web service caused a failed action, but that the cluster software restarted the service on the node on which it had been running and you should still be able to access the web browser.
							</p><pre class="literallayout"># <code class="literal">pcs status</code>
Cluster name: my_cluster
Stack: corosync
Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
Last updated: Fri Oct 12 09:54:33 2018
Last change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com

2 nodes configured
2 resources configured

Online: [ z1.example.com z2.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com
    WebSite    (ocf::heartbeat:apache):        Started z1.example.com

Failed Resource Actions:
* WebSite_monitor_60000 on z1.example.com 'not running' (7): call=31, status=complete, exitreason='none',
    last-rc-change='Fri Feb  5 21:01:41 2016', queued=0ms, exec=0ms</pre><p class="simpara">
								Clear the failure status once the service is up and running again.
							</p><pre class="literallayout"># <code class="literal">pcs resource cleanup WebSite</code></pre></li><li class="listitem"><p class="simpara">
								Put the node on which the service is running into standby mode. Note that since we have disabled fencing we can not effectively simulate a node-level failure (such as pulling a power cable) because fencing is required for the cluster to recover from such situations.
							</p><pre class="literallayout"># <code class="literal">pcs node standby z1.example.com</code></pre></li><li class="listitem"><p class="simpara">
								Check the status of the cluster and note where the service is now running.
							</p><pre class="literallayout"># <code class="literal">pcs status</code>
Cluster name: my_cluster
Stack: corosync
Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
Last updated: Fri Oct 12 09:54:33 2018
Last change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com

2 nodes configured
2 resources configured

Node z1.example.com: standby
Online: [ z2.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z2.example.com
    WebSite    (ocf::heartbeat:apache):        Started z2.example.com</pre></li><li class="listitem">
								Access the website. There should be no loss of service, although the display message should indicate the node on which the service is now running.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						To restore cluster services to the first node, take the node out of standby mode. This will not necessarily move the service back to that node.
					</p><pre class="literallayout"># <code class="literal">pcs cluster unstandby z1.example.com</code></pre></li><li class="listitem"><p class="simpara">
						For final cleanup, stop the cluster services on both nodes.
					</p><pre class="literallayout"># <code class="literal">pcs cluster stop --all</code></pre></li></ol></div></section></section><section class="chapter" id="assembly_pcs-operation-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 3. The pcs command line interface</h1></div></div></div><p>
			The <code class="literal command">pcs</code> command line interface controls and configures cluster services such as <code class="literal">corosync</code>, <code class="literal">pacemaker</code>,<code class="literal">booth</code>, and <code class="literal">sbd</code> by providing an easier interface to their configuration files.
		</p><p>
			Note that you should not edit the <code class="literal">cib.xml</code> configuration file directly. In most cases, Pacemaker will reject a directly modified <code class="literal">cib.xml</code> file.
		</p><section class="section" id="proc_pcs-help-pcs-operation"><div class="titlepage"><div><div><h2 class="title">3.1. pcs help display</h2></div></div></div><p>
				You can use the <code class="literal">-h</code> option of <code class="literal command">pcs</code> to display the parameters of a <code class="literal command">pcs</code> command and a description of those parameters. For example, the following command displays the parameters of the <code class="literal command">pcs resource</code> command. Only a portion of the output is shown.
			</p><pre class="literallayout"># <code class="literal">pcs resource -h</code></pre></section><section class="section" id="proc_raw-config-pcs-operation"><div class="titlepage"><div><div><h2 class="title">3.2. Viewing the raw cluster configuration</h2></div></div></div><p>
				Although you should not edit the cluster configuration file directly, you can view the raw cluster configuration with the <code class="literal command">pcs cluster cib</code> command.
			</p><p>
				You can save the raw cluster configuration to a specified file with the <code class="literal command">pcs cluster cib <span class="emphasis"><em>filename</em></span></code> command. If you have previously configured a cluster and there is already an active CIB, you use the following command to save the raw xml file.
			</p><pre class="literallayout">pcs cluster cib <span class="emphasis"><em>filename</em></span></pre><p>
				For example, the following command saves the raw xml from the CIB into a file named <code class="literal">testfile</code>.
			</p><pre class="literallayout">pcs cluster cib testfile</pre></section><section class="section" id="proc_configure-testfile-pcs-operation"><div class="titlepage"><div><div><h2 class="title">3.3. Saving a configuration change to a working file</h2></div></div></div><p>
				When configuring a cluster, you can save configuration changes to a specified file without affecting the active CIB. This allows you to specify configuration updates without immediately updating the currently running cluster configuration with each individual update.
			</p><p>
				For information on saving the CIB to a file, see <a class="link" href="index.html#proc_raw-config-pcs-operation" title="3.2. Viewing the raw cluster configuration">Viewing the raw cluster configuration</a>. Once you have created that file, you can save configuration changes to that file rather than to the active CIB by using the <code class="literal">-f</code> option of the <code class="literal">pcs</code> command. When you have completed the changes and are ready to update the active CIB file, you can push those file updates with the <code class="literal command">pcs cluster cib-push</code> command.
			</p><p>
				The following is the recommended procedure for pushing changes to the CIB file. This procedure creates a copy of the original saved CIB file and makes changes to that copy. When pushing those changes to the active CIB, this procedure specifies the <code class="literal">diff-against</code> option of the <code class="literal command">pcs cluster cib-push</code> command so that only the changes between the original file and the updated file are pushed to the CIB. This allows users to make changes in parallel that do not overwrite each other, and it reduces the load on Pacemaker which does not need to parse the entire configuration file.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Save the active CIB to a file. This example saves the CIB to a file named <code class="literal">original.xml</code>.
					</p><pre class="literallayout"># <code class="literal">pcs cluster cib original.xml</code></pre></li><li class="listitem"><p class="simpara">
						Copy the saved file to the working file you will be using for the configuration updates.
					</p><pre class="literallayout"># <code class="literal">cp original.xml updated.xml</code></pre></li><li class="listitem"><p class="simpara">
						Update your configuration as needed. The following command creates a resource in the file <code class="literal">updated.xml</code> but does not add that resource to the currently running cluster configuration.
					</p><pre class="literallayout"># <code class="literal">pcs -f updated.xml resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.120 op monitor interval=30s</code></pre></li><li class="listitem"><p class="simpara">
						Push the updated file to the active CIB, specifying that you are pushing only the changes you have made to the original file.
					</p><pre class="literallayout"># <code class="literal">pcs cluster cib-push updated.xml diff-against=original.xml</code></pre></li></ol></div><p>
				Alternately, you can push the entire current content of a CIB file with the following command.
			</p><pre class="literallayout">pcs cluster cib-push <span class="emphasis"><em>filename</em></span></pre><p>
				When pushing the entire CIB file, Pacemaker checks the version and does not allow you to push a CIB file which is older than the one already in a cluster. If you need to update the entire CIB file with a version that is older than the one currently in the cluster, you can use the <code class="literal">--config</code> option of the <code class="literal command">pcs cluster cib-push</code> command.
			</p><pre class="literallayout">pcs cluster cib-push --config <span class="emphasis"><em>filename</em></span></pre></section><section class="section" id="proc_cluster-status-pcs-operation"><div class="titlepage"><div><div><h2 class="title">3.4. Displaying cluster status</h2></div></div></div><p>
				You can display the status of the cluster and the cluster resources with the following command.
			</p><pre class="literallayout">pcs status</pre><p>
				You can display the status of a particular cluster component with the <span class="emphasis"><em>commands</em></span> parameter of the <code class="literal command">pcs status</code> command, specifying <code class="literal">resources</code>, <code class="literal">cluster</code>, <code class="literal">nodes</code>, or <code class="literal">pcsd</code>.
			</p><pre class="literallayout">pcs status <span class="emphasis"><em>commands</em></span></pre><p>
				For example, the following command displays the status of the cluster resources.
			</p><pre class="literallayout">pcs status resources</pre><p>
				The following command displays the status of the cluster, but not the cluster resources.
			</p><pre class="literallayout">pcs cluster status</pre></section><section class="section" id="proc_cluster-config-display-pcs-operation"><div class="titlepage"><div><div><h2 class="title">3.5. Displaying the full cluster configuration</h2></div></div></div><p>
				Use the following command to display the full current cluster configuration.
			</p><pre class="literallayout">pcs config</pre></section></section><section class="chapter" id="assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Creating a Red Hat High-Availability cluster with Pacemaker</h1></div></div></div><p>
			The following procedure creates a Red Hat High Availability two-node cluster using <code class="literal command">pcs</code>.
		</p><p>
			Configuring the cluster in this example requires that your system include the following components:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					2 nodes, which will be used to create the cluster. In this example, the nodes used are <code class="literal">z1.example.com</code> and <code class="literal">z2.example.com</code>.
				</li><li class="listitem">
					Network switches for the private network. We recommend but do not require a private network for communication among the cluster nodes and other cluster hardware such as network power switches and Fibre Channel switches.
				</li><li class="listitem">
					A fencing device for each node of the cluster. This example uses two ports of the APC power switch with a host name of <code class="literal">zapc.example.com</code>.
				</li></ul></div><section class="section" id="proc_installing-cluster-software-creating-high-availability-cluster"><div class="titlepage"><div><div><h2 class="title">4.1. Installing cluster software</h2></div></div></div><p>
				The following procedure installs the cluster software and configures your system for cluster creation.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On each node in the cluster, install the Red Hat High Availability Add-On software packages along with all available fence agents from the High Availability channel.
					</p><pre class="literallayout"># <code class="literal">yum install pcs pacemaker fence-agents-all</code></pre><p class="simpara">
						Alternatively, you can install the Red Hat High Availability Add-On software packages along with only the fence agent that you require with the following command.
					</p><pre class="literallayout"># <code class="literal">yum install pcs pacemaker fence-agents-<span class="emphasis"><em>model</em></span></code></pre><p class="simpara">
						The following command displays a list of the available fence agents.
					</p><pre class="literallayout"># <code class="literal">rpm -q -a | grep fence</code>
fence-agents-rhevm-4.0.2-3.el7.x86_64
fence-agents-ilo-mp-4.0.2-3.el7.x86_64
fence-agents-ipmilan-4.0.2-3.el7.x86_64
...</pre><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							After you install the Red Hat High Availability Add-On packages, you should ensure that your software update preferences are set so that nothing is installed automatically. Installation on a running cluster can cause unexpected behaviors. For more information, see <a class="link" href="https://access.redhat.com/articles/2059253/">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster</a>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						If you are running the <code class="literal command">firewalld</code> daemon, execute the following commands to enable the ports that are required by the Red Hat High Availability Add-On.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can determine whether the <code class="literal command">firewalld</code> daemon is installed on your system with the <code class="literal command">rpm -q firewalld</code> command. If it is installed, you can determine whether it is running with the <code class="literal command">firewall-cmd --state</code> command.
						</p></div></div><pre class="literallayout"># <code class="literal">firewall-cmd --permanent --add-service=high-availability</code>
# <code class="literal">firewall-cmd --add-service=high-availability</code></pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The ideal firewall configuration for cluster components depends on the local environment, where you may need to take into account such considerations as whether the nodes have multiple network interfaces or whether off-host firewalling is present. The example here, which opens the ports that are generally required by a Pacemaker cluster, should be modified to suit local conditions. <a class="link" href="index.html#proc_enabling-ports-for-high-availability-creating-high-availability-cluster" title="4.7. Enabling ports for the High Availability Add-On">Enabling ports for the High Availability Add-On</a> shows the ports to enable for the Red Hat High Availability Add-On and provides an explanation for what each port is used for.
						</p></div></div></li><li class="listitem"><p class="simpara">
						In order to use <code class="literal">pcs</code> to configure the cluster and communicate among the nodes, you must set a password on each node for the user ID <code class="literal">hacluster</code>, which is the <code class="literal">pcs</code> administration account. It is recommended that the password for user <code class="literal">hacluster</code> be the same on each node.
					</p><pre class="literallayout"># <code class="literal">passwd hacluster</code>
Changing password for user hacluster.
New password:
Retype new password:
passwd: all authentication tokens updated successfully.</pre></li><li class="listitem"><p class="simpara">
						Before the cluster can be configured, the <code class="literal command">pcsd</code> daemon must be started and enabled to start up on boot on each node. This daemon works with the <code class="literal command">pcs</code> command to manage configuration across the nodes in the cluster.
					</p><p class="simpara">
						On each node in the cluster, execute the following commands to start the <code class="literal">pcsd</code> service and to enable <code class="literal">pcsd</code> at system start.
					</p><pre class="literallayout"># <code class="literal">systemctl start pcsd.service</code>
# <code class="literal">systemctl enable pcsd.service</code></pre></li></ol></div></section><section class="section" id="proc_installing-pcp-zeroconf-creating-high-availability-cluster"><div class="titlepage"><div><div><h2 class="title">4.2. Installing the pcp-zeroconf package (recommended)</h2></div></div></div><p>
				When you set up your cluster, it is recommended that you install the <code class="literal">pcp-zeroconf</code> package for the Performance Co-Pilot (PCP) tool. PCP is Red Hat’s recommended resource-monitoring tool for RHEL systems. Installing the <code class="literal">pcp-zeroconf</code> package allows you to have PCP running and collecting performance-monitoring data for the benefit of investigations into fencing, resource failures, and other events that disrupt the cluster.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Cluster deployments where PCP is enabled will need sufficient space available for PCP’s captured data on the file system that contains <code class="literal">/var/log/pcp/</code>. Typical space usage by PCP varies across deployments, but 10Gb is usually sufficient when using the <code class="literal">pcp-zeroconf</code> default settings, and some environments may require less. Monitoring usage in this directory over a 14-day period of typical activity can provide a more accurate usage expectation.
				</p></div></div><p>
				To install the <code class="literal">pcp-zeroconf</code> package, run the following command.
			</p><pre class="literallayout"># <code class="literal">yum install pcp-zeroconf</code></pre><p>
				This package enables <code class="literal">pmcd</code> and sets up data capture at a 10-second interval.
			</p><p>
				For information on reviewing PCP data, see <a class="link" href="https://access.redhat.com/solutions/4545111">Why did a RHEL High Availability cluster node reboot - and how can I prevent it from happening again?</a> on the Red Hat Customer Portal.
			</p></section><section class="section" id="proc_creating-high-availability-cluster-creating-high-availability-cluster"><div class="titlepage"><div><div><h2 class="title">4.3. Creating a high availability cluster</h2></div></div></div><p>
				This procedure creates a Red Hat High Availability Add-On cluster that consists of the nodes <code class="literal">z1.example.com</code> and <code class="literal">z2.example.com</code>.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Authenticate the <code class="literal command">pcs</code> user <code class="literal">hacluster</code> for each node in the cluster on the node from which you will be running <code class="literal command">pcs</code>.
					</p><p class="simpara">
						The following command authenticates user <code class="literal">hacluster</code> on <code class="literal">z1.example.com</code> for both of the nodes in a two-node cluster that will consist of <code class="literal">z1.example.com</code> and <code class="literal">z2.example.com</code>.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs host auth z1.example.com z2.example.com</code>
Username: <code class="literal">hacluster</code>
Password:
z1.example.com: Authorized
z2.example.com: Authorized</pre></li><li class="listitem"><p class="simpara">
						Execute the following command from <code class="literal">z1.example.com</code> to create the two-node cluster <code class="literal">my_cluster</code> that consists of nodes <code class="literal">z1.example.com</code> and <code class="literal">z2.example.com</code>. This will propagate the cluster configuration files to both nodes in the cluster. This command includes the <code class="literal">--start</code> option, which will start the cluster services on both nodes in the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs cluster setup my_cluster --start</code> <code class="literal">z1.example.com z2.example.com</code></pre></li><li class="listitem"><p class="simpara">
						Enable the cluster services to run on each node in the cluster when the node is booted.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For your particular environment, you may choose to leave the cluster services disabled by skipping this step. This allows you to ensure that if a node goes down, any issues with your cluster or your resources are resolved before the node rejoins the cluster. If you leave the cluster services disabled, you will need to manually start the services when you reboot a node by executing the <code class="literal command">pcs cluster start</code> command on that node.
						</p></div></div><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs cluster enable --all</code></pre></li></ol></div><p>
				You can display the current status of the cluster with the <code class="literal command">pcs cluster status</code> command. Because there may be a slight delay before the cluster is up and running when you start the cluster services with the <code class="literal option">--start</code> option of the <code class="literal command">pcs cluster setup</code> command, you should ensure that the cluster is up and running before performing any subsequent actions on the cluster and its configuration.
			</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs cluster status</code>
Cluster Status:
 Stack: corosync
 Current DC: z2.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
 Last updated: Thu Oct 11 16:11:18 2018
 Last change: Thu Oct 11 16:11:00 2018 by hacluster via crmd on z2.example.com
 2 Nodes configured
 0 Resources configured

...</pre></section><section class="section" id="proc_configure-multiple-ip-creating-high-availability-cluster"><div class="titlepage"><div><div><h2 class="title">4.4. Creating a high availability cluster with multiple links</h2></div></div></div><p>
				You can use the <code class="literal command">pcs cluster setup</code> command to create a Red Hat High Availability cluster with multiple links by specifying all of the links for each node.
			</p><p>
				The format for the command to create a two-node cluster with two links is as follows.
			</p><pre class="literallayout">pcs cluster setup <span class="emphasis"><em>cluster_name</em></span> <span class="emphasis"><em>node1_name</em></span> addr=<span class="emphasis"><em>node1_link0_address</em></span> addr=<span class="emphasis"><em>node1_link1_address</em></span> <span class="emphasis"><em>node2_name</em></span> addr=<span class="emphasis"><em>node2_link0_address</em></span> addr=<span class="emphasis"><em>node2_link1_address</em></span></pre><p>
				When creating a cluster with multiple links, you should take the following into account.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The order of the <code class="literal">addr=<span class="emphasis"><em>address</em></span></code> parameters is important. The first address specified after a node name is for <code class="literal">link0</code>, the second one for <code class="literal">link1</code>, and so forth.
					</li><li class="listitem">
						It is possible to specify up to eight links using the knet transport protocol, which is the default transport protocol.
					</li><li class="listitem">
						All nodes must have the same number of <code class="literal">addr=</code> parameters.
					</li><li class="listitem">
						Currently, it is not possible to add, remove, or change links in an existing cluster using the <code class="literal command">pcs</code> command.
					</li><li class="listitem">
						As with single-link clusters, do not mix IPv4 and IPv6 addresses in one link, although you can have one link running IPv4 and the other running IPv6.
					</li><li class="listitem">
						As with single-link clusters, you can specify addresses as IP addresses or as names as long as the names resolve to IPv4 or IPv6 addresses for which IPv4 and IPv6 addresses are not mixed in one link.
					</li></ul></div><p>
				The following example creates a two-node cluster named <code class="literal">my_twolink_cluster</code> with two nodes, <code class="literal">rh80-node1</code> and <code class="literal">rh80-node2</code>. <code class="literal">rh80-node1</code> has two interfaces, IP address 192.168.122.201 as <code class="literal">link0</code> and 192.168.123.201 as <code class="literal">link1</code>. <code class="literal">rh80-node2</code> has two interfaces, IP address 192.168.122.202 as <code class="literal">link0</code> and 192.168.123.202 as <code class="literal">link1</code>.
			</p><pre class="literallayout"># <code class="literal">pcs cluster setup my_twolink_cluster rh80-node1 addr=192.168.122.201 addr=192.168.123.201 rh80-node2 addr=192.168.122.202 addr=192.168.123.202</code></pre><p>
				When adding a node to a cluster with multiple links, you must specify addresses for all links. The following example adds the node <code class="literal">rh80-node3</code> to a cluster, specifying IP address 192.168.122.203 as link0 and 192.168.123.203 as link1.
			</p><pre class="literallayout"># <code class="literal">pcs cluster node add rh80-node3 addr=192.168.122.203 addr=192.168.123.203</code></pre></section><section class="section" id="proc_configuring-fencing-creating-high-availability-cluster"><div class="titlepage"><div><div><h2 class="title">4.5. Configuring fencing</h2></div></div></div><p>
				You must configure a fencing device for each node in the cluster. For information about the fence configuration commands and options, see <a class="link" href="index.html#assembly_configuring-fencing-configuring-and-managing-high-availability-clusters" title="Chapter 9. Configuring fencing in a Red Hat High Availability cluster">Configuring fencing in a Red Hat High Availability cluster</a>.
			</p><p>
				For general information on fencing and its importance in a Red Hat High Availability cluster, see <a class="link" href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</a>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					When configuring a fencing device, attention should be given to whether that device shares power with any nodes or devices in the cluster. If a node and its fence device do share power, then the cluster may be at risk of being unable to fence that node if the power to it and its fence device should be lost. Such a cluster should either have redundant power supplies for fence devices and nodes, or redundant fence devices that do not share power. Alternative methods of fencing such as SBD or storage fencing may also bring redundancy in the event of isolated power losses.
				</p></div></div><p>
				This example uses the APC power switch with a host name of <code class="literal">zapc.example.com</code> to fence the nodes, and it uses the <code class="literal">fence_apc_snmp</code> fencing agent. Because both nodes will be fenced by the same fencing agent, you can configure both fencing devices as a single resource, using the <code class="literal">pcmk_host_map</code> option.
			</p><p>
				You create a fencing device by configuring the device as a <code class="literal">stonith</code> resource with the <code class="literal command">pcs stonith create</code> command. The following command configures a <code class="literal">stonith</code> resource named <code class="literal">myapc</code> that uses the <code class="literal">fence_apc_snmp</code> fencing agent for nodes <code class="literal">z1.example.com</code> and <code class="literal">z2.example.com</code>. The <code class="literal">pcmk_host_map</code> option maps <code class="literal">z1.example.com</code> to port 1, and <code class="literal">z2.example.com</code> to port 2. The login value and password for the APC device are both <code class="literal">apc</code>. By default, this device will use a monitor interval of sixty seconds for each node.
			</p><p>
				Note that you can use an IP address when specifying the host name for the nodes.
			</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs stonith create myapc fence_apc_snmp</code> \
<code class="literal">ipaddr="zapc.example.com" pcmk_host_map="z1.example.com:1;z2.example.com:2"</code> \
<code class="literal">login="apc" passwd="apc"</code></pre><p>
				The following command displays the parameters of an existing STONITH device.
			</p><pre class="literallayout">[root@rh7-1 ~]# <code class="literal">pcs stonith config myapc</code>
 Resource: myapc (class=stonith type=fence_apc_snmp)
  Attributes: ipaddr=zapc.example.com pcmk_host_map=z1.example.com:1;z2.example.com:2 login=apc passwd=apc
  Operations: monitor interval=60s (myapc-monitor-interval-60s)</pre><p>
				After configuring your fence device, you should test the device. For information on testing a fence device, see <a class="link" href="index.html#proc_testing-fence-devices-configuring-fencing" title="9.5. Testing a fence device">Testing a fence device</a>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Do not test your fence device by disabling the network interface, as this will not properly test fencing.
				</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Once fencing is configured and a cluster has been started, a network restart will trigger fencing for the node which restarts the network even when the timeout is not exceeded. For this reason, do not restart the network service while the cluster service is running because it will trigger unintentional fencing on the node.
				</p></div></div></section><section class="section" id="proc_cluster-backup-creating-high-availability-cluster"><div class="titlepage"><div><div><h2 class="title">4.6. Backing up and restoring a cluster configuration</h2></div></div></div><p>
				You can back up the cluster configuration in a tarball with the following command. If you do not specify a file name, the standard output will be used.
			</p><pre class="literallayout">pcs config backup <span class="emphasis"><em>filename</em></span></pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The <code class="literal command">pcs config backup</code> command backs up only the cluster configuration itself as configured in the CIB; the configuration of resource daemons is out of the scope of this command. For example if you have configured an Apache resource in the cluster, the resource settings (which are in the CIB) will be backed up, while the Apache daemon settings (as set in`/etc/httpd`) and the files it serves will not be backed up. Similarly, if there is a database resource configured in the cluster, the database itself will not be backed up, while the database resource configuration (CIB) will be.
				</p></div></div><p>
				Use the following command to restore the cluster configuration files on all nodes from the backup. If you do not specify a file name, the standard input will be used. Specifying the <code class="literal option">--local</code> option restores only the files on the current node.
			</p><pre class="literallayout">pcs config restore [--local] [<span class="emphasis"><em>filename</em></span>]</pre></section><section class="section" id="proc_enabling-ports-for-high-availability-creating-high-availability-cluster"><div class="titlepage"><div><div><h2 class="title">4.7. Enabling ports for the High Availability Add-On</h2></div></div></div><p>
				The ideal firewall configuration for cluster components depends on the local environment, where you may need to take into account such considerations as whether the nodes have multiple network interfaces or whether off-host firewalling is present.
			</p><p>
				If you are running the <code class="literal command">firewalld</code> daemon, execute the following commands to enable the ports that are required by the Red Hat High Availability Add-On.
			</p><pre class="literallayout"># <code class="literal">firewall-cmd --permanent --add-service=high-availability</code>
# <code class="literal">firewall-cmd --add-service=high-availability</code></pre><p>
				You may need to modify which ports are open to suit local conditions.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You can determine whether the <code class="literal command">firewalld</code> daemon is installed on your system with the <code class="literal command">rpm -q firewalld</code> command. If the <code class="literal command">firewalld</code> daemon is installed, you can determine whether it is running with the <code class="literal command">firewall-cmd --state</code> command.
				</p></div></div><p>
				<a class="xref" href="index.html#tb-portenable-HAAR" title="Table 4.1. Ports to Enable for High Availability Add-On">Table 4.1, “Ports to Enable for High Availability Add-On”</a> shows the ports to enable for the Red Hat High Availability Add-On and provides an explanation for what the port is used for.
			</p><div class="table" id="tb-portenable-HAAR"><p class="title"><strong>Table 4.1. Ports to Enable for High Availability Add-On</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319467668320" scope="col">Port</th><th align="left" valign="top" id="idm140319467667232" scope="col">When Required</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319467668320"> <p>
								TCP 2224
							</p>
							 </td><td align="left" valign="top" headers="idm140319467667232"> <p>
								Default <code class="literal">pcsd</code> port required on all nodes (needed by the pcsd Web UI and required for node-to-node communication). You can configure the <code class="literal">pcsd</code> port by means of the <code class="literal">PCSD_PORT</code> parameter in the <code class="literal">/etc/sysconfig/pcsd</code> file.
							</p>
							 <p>
								It is crucial to open port 2224 in such a way that <code class="literal command">pcs</code> from any node can talk to all nodes in the cluster, including itself. When using the Booth cluster ticket manager or a quorum device you must open port 2224 on all related hosts, such as Booth arbiters or the quorum device host.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319467668320"> <p>
								TCP 3121
							</p>
							 </td><td align="left" valign="top" headers="idm140319467667232"> <p>
								Required on all nodes if the cluster has any Pacemaker Remote nodes
							</p>
							 <p>
								Pacemaker’s <code class="literal">pacemaker-based</code> daemon on the full cluster nodes will contact the <code class="literal">pacemaker_remoted</code> daemon on Pacemaker Remote nodes at port 3121. If a separate interface is used for cluster communication, the port only needs to be open on that interface. At a minimum, the port should open on Pacemaker Remote nodes to full cluster nodes. Because users may convert a host between a full node and a remote node, or run a remote node inside a container using the host’s network, it can be useful to open the port to all nodes. It is not necessary to open the port to any hosts other than nodes.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319467668320"> <p>
								TCP 5403
							</p>
							 </td><td align="left" valign="top" headers="idm140319467667232"> <p>
								Required on the quorum device host when using a quorum device with <code class="literal">corosync-qnetd</code>. The default value can be changed with the <code class="literal option">-p</code> option of the <code class="literal command">corosync-qnetd</code> command.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319467668320"> <p>
								UDP 5404-5412
							</p>
							 </td><td align="left" valign="top" headers="idm140319467667232"> <p>
								Required on corosync nodes to facilitate communication between nodes. It is crucial to open ports 5404-5412 in such a way that <code class="literal">corosync</code> from any node can talk to all nodes in the cluster, including itself.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319467668320"> <p>
								TCP 21064
							</p>
							 </td><td align="left" valign="top" headers="idm140319467667232"> <p>
								Required on all nodes if the cluster contains any resources requiring DLM (such as <code class="literal">GFS2</code>).
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319467668320"> <p>
								TCP 9929, UDP 9929
							</p>
							 </td><td align="left" valign="top" headers="idm140319467667232"> <p>
								Required to be open on all cluster nodes and booth arbitrator nodes to connections from any of those same nodes when the Booth ticket manager is used to establish a multi-site cluster.
							</p>
							 </td></tr></tbody></table></div></div></section></section><section class="chapter" id="assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster</h1></div></div></div><p>
			The following procedure configures an active/passive Apache HTTP server in a two-node Red Hat Enterprise Linux High Availability Add-On cluster using <code class="literal command">pcs</code> to configure cluster resources. In this use case, clients access the Apache HTTP server through a floating IP address. The web server runs on one of two nodes in the cluster. If the node on which the web server is running becomes inoperative, the web server starts up again on the second node of the cluster with minimal service interruption.
		</p><p>
			<a class="xref" href="index.html#configuring-ha-http-291627-haserver_cluster4" title="Figure 5.1. Apache in a Red Hat High Availability Two-Node Cluster">Figure 5.1, “Apache in a Red Hat High Availability Two-Node Cluster”</a> shows a high-level overview of the cluster in which The cluster is a two-node Red Hat High Availability cluster which is configured with a network power switch and with shared storage. The cluster nodes are connected to a public network, for client access to the Apache HTTP server through a virtual IP. The Apache server runs on either Node 1 or Node 2, each of which has access to the storage on which the Apache data is kept. In this illustration, the web server is running on Node 1 while Node 2 is available to run the server if Node 1 becomes inoperative.
		</p><div class="figure" id="configuring-ha-http-291627-haserver_cluster4"><p class="title"><strong>Figure 5.1. Apache in a Red Hat High Availability Two-Node Cluster</strong></p><div class="figure-contents"><div class="mediaobject"><img src="291627-haserver_cluster4.png" alt="Apache in a Red Hat High Availability Two-Node Cluster"/></div></div></div><p>
			This use case requires that your system include the following components:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					A two-node Red Hat High Availability cluster with power fencing configured for each node. We recommend but do not require a private network. This procedure uses the cluster example provided in <a class="link" href="index.html#assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters" title="Chapter 4. Creating a Red Hat High-Availability cluster with Pacemaker">Creating a Red Hat High-Availability cluster with Pacemaker</a>.
				</li><li class="listitem">
					A public virtual IP address, required for Apache.
				</li><li class="listitem">
					Shared storage for the nodes in the cluster, using iSCSI or Fibre Channel.
				</li></ul></div><p>
			The cluster is configured with an Apache resource group, which contains the cluster components that the web server requires: an LVM resource, a file system resource, an IP address resource, and a web server resource. This resource group can fail over from one node of the cluster to the other, allowing either node to run the web server. Before creating the resource group for this cluster, you will be performing the following procedures:
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					Configure an <code class="literal">ext4</code> file system on the logical volume <code class="literal">my_lv</code>.
				</li><li class="listitem">
					Configure a web server.
				</li></ol></div><p>
			After performing these steps, you create the resource group and the resources it contains.
		</p><section class="section" id="proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http"><div class="titlepage"><div><div><h2 class="title">5.1. Configuring an LVM volume with an ext4 file system in a Pacemaker cluster</h2></div></div></div><p>
				This use case requires that you create an LVM logical volume on storage that is shared between the nodes of the cluster.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					LVM volumes and the corresponding partitions and devices used by cluster nodes must be connected to the cluster nodes only.
				</p></div></div><p>
				The following procedure creates an LVM logical volume and then creates an ext4 file system on that volume for use in a Pacemaker cluster. In this example, the shared partition <code class="literal">/dev/sdb1</code> is used to store the LVM physical volume from which the LVM logical volume will be created.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On both nodes of the cluster, perform the following steps to set the value for the LVM system ID to the value of the <code class="literal">uname</code> identifier for the system. The LVM system ID will be used to ensure that only the cluster is capable of activating the volume group.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Set the <code class="literal">system_id_source</code> configuration option in the <code class="literal">/etc/lvm/lvm.conf</code> configuration file to <code class="literal">uname</code>.
							</p><pre class="literallayout"># Configuration option global/system_id_source.
system_id_source = "uname"</pre></li><li class="listitem"><p class="simpara">
								Verify that the LVM system ID on the node matches the <code class="literal">uname</code> for the node.
							</p><pre class="literallayout"># <code class="literal">lvm systemid</code>
  system ID: z1.example.com
# <code class="literal">uname -n</code>
  z1.example.com</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Create the LVM volume and create an ext4 file system on that volume. Since the <code class="literal">/dev/sdb1</code> partition is storage that is shared, you perform this part of the procedure on one node only.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create an LVM physical volume on partition <code class="literal">/dev/sdb1</code>.
							</p><pre class="literallayout"># <code class="literal">pvcreate /dev/sdb1</code>
  Physical volume "/dev/sdb1" successfully created</pre></li><li class="listitem"><p class="simpara">
								Create the volume group <code class="literal">my_vg</code> that consists of the physical volume <code class="literal">/dev/sdb1</code>.
							</p><pre class="literallayout"># <code class="literal">vgcreate my_vg /dev/sdb1</code>
  Volume group "my_vg" successfully created</pre></li><li class="listitem"><p class="simpara">
								Verify that the new volume group has the system ID of the node on which you are running and from which you created the volume group.
							</p><pre class="literallayout"># <code class="literal">vgs -o+systemid</code>
  VG    #PV #LV #SN Attr   VSize  VFree  System ID
  my_vg   1   0   0 wz--n- &lt;1.82t &lt;1.82t z1.example.com</pre></li><li class="listitem"><p class="simpara">
								Create a logical volume using the volume group <code class="literal">my_vg</code>.
							</p><pre class="literallayout"># <code class="literal">lvcreate -L450 -n my_lv my_vg</code>
  Rounding up size to full physical extent 452.00 MiB
  Logical volume "my_lv" created</pre><p class="simpara">
								You can use the <code class="literal command">lvs</code> command to display the logical volume.
							</p><pre class="literallayout"># <code class="literal">lvs</code>
  LV      VG      Attr      LSize   Pool Origin Data%  Move Log Copy%  Convert
  my_lv   my_vg   -wi-a---- 452.00m
  ...</pre></li><li class="listitem"><p class="simpara">
								Create an ext4 file system on the logical volume <code class="literal">my_lv</code>.
							</p><pre class="literallayout"># <code class="literal">mkfs.ext4 /dev/my_vg/my_lv</code>
mke2fs 1.44.3 (10-July-2018)
Creating filesystem with 462848 1k blocks and 115824 inodes
...</pre></li></ol></div></li></ol></div></section><section class="section" id="proc_configuring-apache-http-web-server-configuring-ha-http"><div class="titlepage"><div><div><h2 class="title">5.2. Configuring an Apache HTTP Server</h2></div></div></div><p>
				The following procedure configures an Apache HTTP Server.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Ensure that the Apache HTTP Server is installed on each node in the cluster. You also need the <code class="literal">wget</code> tool installed on the cluster to be able to check the status of the Apache HTTP Server.
					</p><p class="simpara">
						On each node, execute the following command.
					</p><pre class="literallayout"># <code class="literal">yum install -y httpd wget</code></pre><p class="simpara">
						If you are running the <code class="literal">firewalld</code> daemon, on each node in the cluster enable the ports that are required by the Red Hat High Availability Add-On.
					</p><pre class="literallayout"># <code class="literal">firewall-cmd --permanent --add-service=high-availability</code>
# <code class="literal">firewall-cmd --reload</code></pre></li><li class="listitem"><p class="simpara">
						In order for the Apache resource agent to get the status of the Apache HTTP Server, ensure that the following text is present in the <code class="literal">/etc/httpd/conf/httpd.conf</code> file on each node in the cluster, and ensure that it has not been commented out. If this text is not already present, add the text to the end of the file.
					</p><pre class="literallayout">&lt;Location /server-status&gt;
    SetHandler server-status
    Require local
&lt;/Location&gt;</pre></li><li class="listitem"><p class="simpara">
						When you use the <code class="literal">apache</code> resource agent to manage Apache, it does not use <code class="literal">systemd</code>. Because of this, you must edit the <code class="literal">logrotate</code> script supplied with Apache so that it does not use <code class="literal">systemctl</code> to reload Apache.
					</p><p class="simpara">
						Remove the following line in the <code class="literal">/etc/logrotate.d/httpd</code> file on each node in the cluster.
					</p><pre class="literallayout">/bin/systemctl reload httpd.service &gt; /dev/null 2&gt;/dev/null || true</pre><p class="simpara">
						Replace the line you removed with the following line.
					</p><pre class="literallayout">/usr/sbin/httpd -f /etc/httpd/conf/httpd.conf -c "PidFile /var/run/httpd.pid" -k graceful &gt; /dev/null 2&gt;/dev/null || true</pre></li><li class="listitem"><p class="simpara">
						Create a web page for Apache to serve up. On one node in the cluster, mount the file system you created in <a class="link" href="index.html#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http" title="5.1. Configuring an LVM volume with an ext4 file system in a Pacemaker cluster">Configuring an LVM volume with an ext4 file system</a>, create the file <code class="literal">index.html</code> on that file system, and then unmount the file system.
					</p><pre class="literallayout"># <code class="literal">mount /dev/my_vg/my_lv /var/www/</code>
# <code class="literal">mkdir /var/www/html</code>
# <code class="literal">mkdir /var/www/cgi-bin</code>
# <code class="literal">mkdir /var/www/error</code>
# <code class="literal">restorecon -R /var/www</code>
# <code class="literal">cat &lt;&lt;-END &gt;/var/www/html/index.html</code>
<code class="literal">&lt;html&gt;</code>
<code class="literal">&lt;body&gt;Hello&lt;/body&gt;</code>
<code class="literal">&lt;/html&gt;</code>
<code class="literal">END</code>
# <code class="literal">umount /var/www</code></pre></li></ol></div></section><section class="section" id="proc_configuring-resources-for-http-server-in-a-cluster-configuring-ha-http"><div class="titlepage"><div><div><h2 class="title">5.3. Creating the resources and resource groups</h2></div></div></div><p>
				This use case requires that you create four cluster resources. To ensure these resources all run on the same node, they are configured as part of the resource group <code class="literal">apachegroup</code>. The resources to create are as follows, listed in the order in which they will start.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						An <code class="literal">LVM</code> resource named <code class="literal">my_lvm</code> that uses the LVM volume group you created in <a class="link" href="index.html#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http" title="5.1. Configuring an LVM volume with an ext4 file system in a Pacemaker cluster">Configuring an LVM volume with an ext4 file system</a>.
					</li><li class="listitem">
						A <code class="literal">Filesystem</code> resource named <code class="literal">my_fs</code>, that uses the file system device <code class="literal">/dev/my_vg/my_lv</code> you created in <a class="link" href="index.html#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http" title="5.1. Configuring an LVM volume with an ext4 file system in a Pacemaker cluster">Configuring an LVM volume with an ext4 file system</a>.
					</li><li class="listitem">
						An <code class="literal">IPaddr2</code> resource, which is a floating IP address for the <code class="literal">apachegroup</code> resource group. The IP address must not be one already associated with a physical node. If the <code class="literal">IPaddr2</code> resource’s NIC device is not specified, the floating IP must reside on the same network as one of the node’s statically assigned IP addresses, otherwise the NIC device to assign the floating IP address cannot be properly detected.
					</li><li class="listitem">
						An <code class="literal">apache</code> resource named <code class="literal">Website</code> that uses the <code class="literal">index.html</code> file and the Apache configuration you defined in <a class="link" href="index.html#proc_configuring-apache-http-web-server-configuring-ha-http" title="5.2. Configuring an Apache HTTP Server">Configuring an Apache HTTP server</a>.
					</li></ol></div><p>
				The following procedure creates the resource group <code class="literal">apachegroup</code> and the resources that the group contains. The resources will start in the order in which you add them to the group, and they will stop in the reverse order in which they are added to the group. Run this procedure from one node of the cluster only.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						The following command creates the <code class="literal">LVM-activate</code> resource <code class="literal">my_lvm</code>. Because the resource group <code class="literal">apachegroup</code> does not yet exist, this command creates the resource group.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Do not configure more than one <code class="literal">LVM-activate</code> resource that uses the same LVM volume group in an active/passive HA configuration, as this could cause data corruption. Additionally, do not configure an <code class="literal">LVM-activate</code> resource as a clone resource in an active/passive HA configuration.
						</p></div></div><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create my_lvm ocf:heartbeat:LVM-activate</code> <code class="literal">vgname=my_vg</code> <code class="literal">vg_access_mode=system_id --group apachegroup</code></pre><p class="simpara">
						When you create a resource, the resource is started automatically. You can use the following command to confirm that the resource was created and has started.
					</p><pre class="literallayout"># <code class="literal">pcs resource status</code>
 Resource Group: apachegroup
     my_lvm	(ocf::heartbeat:LVM-activate):	Started</pre><p class="simpara">
						You can manually stop and start an individual resource with the <code class="literal command">pcs resource disable</code> and <code class="literal command">pcs resource enable</code> commands.
					</p></li><li class="listitem"><p class="simpara">
						The following commands create the remaining resources for the configuration, adding them to the existing resource group <code class="literal">apachegroup</code>.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create my_fs Filesystem</code> \
<code class="literal">device="/dev/my_vg/my_lv" directory="/var/www" fstype="ext4"</code> \
<code class="literal">--group apachegroup</code>

[root@z1 ~]# <code class="literal">pcs resource create VirtualIP IPaddr2 ip=198.51.100.3</code> \
<code class="literal">cidr_netmask=24 --group apachegroup</code>

[root@z1 ~]# <code class="literal">pcs resource create Website apache</code> \
<code class="literal">configfile="/etc/httpd/conf/httpd.conf"</code> \
<code class="literal">statusurl="http://127.0.0.1/server-status" --group apachegroup</code></pre></li><li class="listitem"><p class="simpara">
						After creating the resources and the resource group that contains them, you can check the status of the cluster. Note that all four resources are running on the same node.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs status</code>
Cluster name: my_cluster
Last updated: Wed Jul 31 16:38:51 2013
Last change: Wed Jul 31 16:42:14 2013 via crm_attribute on z1.example.com
Stack: corosync
Current DC: z2.example.com (2) - partition with quorum
Version: 1.1.10-5.el7-9abe687
2 Nodes configured
6 Resources configured

Online: [ z1.example.com z2.example.com ]

Full list of resources:
 myapc	(stonith:fence_apc_snmp):	Started z1.example.com
 Resource Group: apachegroup
     my_lvm	(ocf::heartbeat:LVM):	Started z1.example.com
     my_fs	(ocf::heartbeat:Filesystem):	Started z1.example.com
     VirtualIP	(ocf::heartbeat:IPaddr2):	Started z1.example.com
     Website	(ocf::heartbeat:apache):	Started z1.example.com</pre><p class="simpara">
						Note that if you have not configured a fencing device for your cluster, by default the resources do not start.
					</p></li><li class="listitem"><p class="simpara">
						Once the cluster is up and running, you can point a browser to the IP address you defined as the <code class="literal">IPaddr2</code> resource to view the sample display, consisting of the simple word "Hello".
					</p><pre class="literallayout">Hello</pre><p class="simpara">
						If you find that the resources you configured are not running, you can run the <code class="literal command">pcs resource debug-start <span class="emphasis"><em>resource</em></span></code> command to test the resource configuration.
					</p></li></ol></div></section><section class="section" id="proc_testing-resource-configuration-in-a-cluster-configuring-ha-http"><div class="titlepage"><div><div><h2 class="title">5.4. Testing the resource configuration</h2></div></div></div><p>
				In the cluster status display shown in <a class="link" href="index.html#proc_configuring-resources-for-http-server-in-a-cluster-configuring-ha-http" title="5.3. Creating the resources and resource groups">Creating the resources and resource groups</a>, all of the resources are running on node <code class="literal">z1.example.com</code>. You can test whether the resource group fails over to node <code class="literal">z2.example.com</code> by using the following procedure to put the first node in <code class="literal">standby</code> mode, after which the node will no longer be able to host resources.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						The following command puts node <code class="literal">z1.example.com</code> in <code class="literal">standby</code> mode.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs node standby z1.example.com</code></pre></li><li class="listitem"><p class="simpara">
						After putting node <code class="literal">z1</code> in <code class="literal">standby</code> mode, check the cluster status. Note that the resources should now all be running on <code class="literal">z2</code>.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs status</code>
Cluster name: my_cluster
Last updated: Wed Jul 31 17:16:17 2013
Last change: Wed Jul 31 17:18:34 2013 via crm_attribute on z1.example.com
Stack: corosync
Current DC: z2.example.com (2) - partition with quorum
Version: 1.1.10-5.el7-9abe687
2 Nodes configured
6 Resources configured

Node z1.example.com (1): standby
Online: [ z2.example.com ]

Full list of resources:

 myapc	(stonith:fence_apc_snmp):	Started z1.example.com
 Resource Group: apachegroup
     my_lvm	(ocf::heartbeat:LVM):	Started z2.example.com
     my_fs	(ocf::heartbeat:Filesystem):	Started z2.example.com
     VirtualIP	(ocf::heartbeat:IPaddr2):	Started z2.example.com
     Website	(ocf::heartbeat:apache):	Started z2.example.com</pre><p class="simpara">
						The web site at the defined IP address should still display, without interruption.
					</p></li><li class="listitem"><p class="simpara">
						To remove <code class="literal">z1</code> from <code class="literal">standby</code> mode, enter the following command.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs cluster unstandby z1.example.com</code></pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Removing a node from <code class="literal">standby</code> mode does not in itself cause the resources to fail back over to that node.
						</p></div></div></li></ol></div></section></section><section class="chapter" id="assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Configuring an active/passive NFS server in a Red Hat High Availability cluster</h1></div></div></div><p>
			The following procedure configures a highly available active/passive NFS server on a two-node Red Hat Enterprise Linux High Availability Add-On cluster using shared storage. The procedure uses <code class="literal command">pcs</code> to configure Pacemaker cluster resources. In this use case, clients access the NFS file system through a floating IP address. The NFS server runs on one of two nodes in the cluster. If the node on which the NFS server is running becomes inoperative, the NFS server starts up again on the second node of the cluster with minimal service interruption.
		</p><section class="section" id="prerequisites"><div class="titlepage"><div><div><h2 class="title">6.1. Prerequisites</h2></div></div></div><p>
				This use case requires that your system include the following components:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						A two-node Red Hat High Availability cluster with power fencing configured for each node. We recommend but do not require a private network. This procedure uses the cluster example provided in <a class="link" href="index.html#assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters" title="Chapter 4. Creating a Red Hat High-Availability cluster with Pacemaker">Creating a Red Hat High-Availability cluster with Pacemaker</a>.
					</li><li class="listitem">
						A public virtual IP address, required for the NFS server.
					</li><li class="listitem">
						Shared storage for the nodes in the cluster, using iSCSI or Fibre Channel.
					</li></ul></div></section><section class="section" id="procedural_overview"><div class="titlepage"><div><div><h2 class="title">6.2. Procedural overview</h2></div></div></div><p>
				Configuring a highly available active/passive NFS server on an existing two-node Red Hat Enterprise Linux High Availability cluster requires that you perform the following steps:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Configure an <code class="literal">ext4</code> file system on the LVM logical volume <code class="literal">my_lv</code> on the shared storage for the nodes in the cluster.
					</li><li class="listitem">
						Configure an NFS share on the shared storage on the LVM logical volume.
					</li><li class="listitem">
						Create the cluster resources.
					</li><li class="listitem">
						Test the NFS server you have configured.
					</li></ol></div></section><section class="section" id="proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs"><div class="titlepage"><div><div><h2 class="title">6.3. Configuring an LVM volume with an ext4 file system in a Pacemaker cluster</h2></div></div></div><p>
				This use case requires that you create an LVM logical volume on storage that is shared between the nodes of the cluster.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					LVM volumes and the corresponding partitions and devices used by cluster nodes must be connected to the cluster nodes only.
				</p></div></div><p>
				The following procedure creates an LVM logical volume and then creates an ext4 file system on that volume for use in a Pacemaker cluster. In this example, the shared partition <code class="literal">/dev/sdb1</code> is used to store the LVM physical volume from which the LVM logical volume will be created.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On both nodes of the cluster, perform the following steps to set the value for the LVM system ID to the value of the <code class="literal">uname</code> identifier for the system. The LVM system ID will be used to ensure that only the cluster is capable of activating the volume group.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Set the <code class="literal">system_id_source</code> configuration option in the <code class="literal">/etc/lvm/lvm.conf</code> configuration file to <code class="literal">uname</code>.
							</p><pre class="literallayout"># Configuration option global/system_id_source.
system_id_source = "uname"</pre></li><li class="listitem"><p class="simpara">
								Verify that the LVM system ID on the node matches the <code class="literal">uname</code> for the node.
							</p><pre class="literallayout"># <code class="literal">lvm systemid</code>
  system ID: z1.example.com
# <code class="literal">uname -n</code>
  z1.example.com</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Create the LVM volume and create an ext4 file system on that volume. Since the <code class="literal">/dev/sdb1</code> partition is storage that is shared, you perform this part of the procedure on one node only.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create an LVM physical volume on partition <code class="literal">/dev/sdb1</code>.
							</p><pre class="literallayout"># <code class="literal">pvcreate /dev/sdb1</code>
  Physical volume "/dev/sdb1" successfully created</pre></li><li class="listitem"><p class="simpara">
								Create the volume group <code class="literal">my_vg</code> that consists of the physical volume <code class="literal">/dev/sdb1</code>.
							</p><pre class="literallayout"># <code class="literal">vgcreate my_vg /dev/sdb1</code>
  Volume group "my_vg" successfully created</pre></li><li class="listitem"><p class="simpara">
								Verify that the new volume group has the system ID of the node on which you are running and from which you created the volume group.
							</p><pre class="literallayout"># <code class="literal">vgs -o+systemid</code>
  VG    #PV #LV #SN Attr   VSize  VFree  System ID
  my_vg   1   0   0 wz--n- &lt;1.82t &lt;1.82t z1.example.com</pre></li><li class="listitem"><p class="simpara">
								Create a logical volume using the volume group <code class="literal">my_vg</code>.
							</p><pre class="literallayout"># <code class="literal">lvcreate -L450 -n my_lv my_vg</code>
  Rounding up size to full physical extent 452.00 MiB
  Logical volume "my_lv" created</pre><p class="simpara">
								You can use the <code class="literal command">lvs</code> command to display the logical volume.
							</p><pre class="literallayout"># <code class="literal">lvs</code>
  LV      VG      Attr      LSize   Pool Origin Data%  Move Log Copy%  Convert
  my_lv   my_vg   -wi-a---- 452.00m
  ...</pre></li><li class="listitem"><p class="simpara">
								Create an ext4 file system on the logical volume <code class="literal">my_lv</code>.
							</p><pre class="literallayout"># <code class="literal">mkfs.ext4 /dev/my_vg/my_lv</code>
mke2fs 1.44.3 (10-July-2018)
Creating filesystem with 462848 1k blocks and 115824 inodes
...</pre></li></ol></div></li></ol></div></section><section class="section" id="proc_configuring-nfs-share-configuring-ha-nfs"><div class="titlepage"><div><div><h2 class="title">6.4. Configuring an NFS share</h2></div></div></div><p>
				The following procedure configures the NFS share for the NFS service failover.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On both nodes in the cluster, create the <code class="literal">/nfsshare</code> directory.
					</p><pre class="literallayout"># <code class="literal">mkdir /nfsshare</code></pre></li><li class="listitem"><p class="simpara">
						On one node in the cluster, perform the following procedure.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Mount the ext4 file system that you created in <a class="link" href="index.html#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs" title="6.3. Configuring an LVM volume with an ext4 file system in a Pacemaker cluster">Configuring an LVM volume with an ext4 file system</a> on the <code class="literal">/nfsshare</code> directory.
							</p><pre class="literallayout">[root@z1 ~]# <code class="literal">mount /dev/my_vg/my_lv /nfsshare</code></pre></li><li class="listitem"><p class="simpara">
								Create an <code class="literal">exports</code> directory tree on the <code class="literal">/nfsshare</code> directory.
							</p><pre class="literallayout">[root@z1 ~]# <code class="literal">mkdir -p /nfsshare/exports</code>
[root@z1 ~]# <code class="literal">mkdir -p /nfsshare/exports/export1</code>
[root@z1 ~]# <code class="literal">mkdir -p /nfsshare/exports/export2</code></pre></li><li class="listitem"><p class="simpara">
								Place files in the <code class="literal">exports</code> directory for the NFS clients to access. For this example, we are creating test files named <code class="literal">clientdatafile1</code> and <code class="literal">clientdatafile2</code>.
							</p><pre class="literallayout">[root@z1 ~]# <code class="literal">touch /nfsshare/exports/export1/clientdatafile1</code>
[root@z1 ~]# <code class="literal">touch /nfsshare/exports/export2/clientdatafile2</code></pre></li><li class="listitem"><p class="simpara">
								Unmount the ext4 file system and deactivate the LVM volume group.
							</p><pre class="literallayout">[root@z1 ~]# <code class="literal">umount /dev/my_vg/my_lv</code>
[root@z1 ~]# <code class="literal">vgchange -an my_vg</code></pre></li></ol></div></li></ol></div></section><section class="section" id="proc_configuring_resources_for_nfs_server_in_a_cluster-configuring-ha-nfs"><div class="titlepage"><div><div><h2 class="title">6.5. Configuring the resources and resource group for an NFS server in a cluster</h2></div></div></div><p>
				This section provides the procedure for configuring the cluster resources for this use case.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If you have not configured a fencing device for your cluster, by default the resources do not start.
				</p><p>
					If you find that the resources you configured are not running, you can run the <code class="literal command">pcs resource debug-start <span class="emphasis"><em>resource</em></span></code> command to test the resource configuration. This starts the service outside of the cluster’s control and knowledge. At the point the configured resources are running again, run <code class="literal command">pcs resource cleanup <span class="emphasis"><em>resource</em></span></code> to make the cluster aware of the updates.
				</p></div></div><p>
				The following procedure configures the system resources. To ensure these resources all run on the same node, they are configured as part of the resource group <code class="literal">nfsgroup</code>. The resources will start in the order in which you add them to the group, and they will stop in the reverse order in which they are added to the group. Run this procedure from one node of the cluster only.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create the LVM-activate resource named <code class="literal">my_lvm</code>. Because the resource group <code class="literal">nfsgroup</code> does not yet exist, this command creates the resource group.
					</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							Do not configure more than one <code class="literal">LVM-activate</code> resource that uses the same LVM volume group in an active/passive HA configuration, as this risks data corruption. Additionally, do not configure an <code class="literal">LVM-activate</code> resource as a clone resource in an active/passive HA configuration.
						</p></div></div><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create my_lvm ocf:heartbeat:LVM-activate</code> <code class="literal">vgname=my_vg</code> <code class="literal">vg_access_mode=system_id --group nfsgroup</code></pre></li><li class="listitem"><p class="simpara">
						Check the status of the cluster to verify that the resource is running.
					</p><pre class="literallayout">root@z1 ~]#  <code class="literal">pcs status</code>
Cluster name: my_cluster
Last updated: Thu Jan  8 11:13:17 2015
Last change: Thu Jan  8 11:13:08 2015
Stack: corosync
Current DC: z2.example.com (2) - partition with quorum
Version: 1.1.12-a14efad
2 Nodes configured
3 Resources configured

Online: [ z1.example.com z2.example.com ]

Full list of resources:
 myapc  (stonith:fence_apc_snmp):       Started z1.example.com
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM):   Started z1.example.com

PCSD Status:
  z1.example.com: Online
  z2.example.com: Online

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled</pre></li><li class="listitem"><p class="simpara">
						Configure a <code class="literal">Filesystem</code> resource for the cluster.
					</p><p class="simpara">
						The following command configures an ext4 <code class="literal">Filesystem</code> resource named <code class="literal">nfsshare</code> as part of the <code class="literal">nfsgroup</code> resource group. This file system uses the LVM volume group and ext4 file system you created in <a class="link" href="index.html#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs" title="6.3. Configuring an LVM volume with an ext4 file system in a Pacemaker cluster">Configuring an LVM volume with an ext4 file system</a> and will be mounted on the <code class="literal">/nfsshare</code> directory you created in <a class="link" href="index.html#proc_configuring-nfs-share-configuring-ha-nfs" title="6.4. Configuring an NFS share">Configuring an NFS share</a>.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create nfsshare Filesystem</code> \
<code class="literal">device=/dev/my_vg/my_lv directory=/nfsshare</code> \
<code class="literal">fstype=ext4 --group nfsgroup</code></pre><p class="simpara">
						You can specify mount options as part of the resource configuration for a <code class="literal">Filesystem</code> resource with the <code class="literal">options=<span class="emphasis"><em>options</em></span></code> parameter. Run the <code class="literal command">pcs resource describe Filesystem</code> command for full configuration options.
					</p></li><li class="listitem"><p class="simpara">
						Verify that the <code class="literal">my_lvm</code> and <code class="literal">nfsshare</code> resources are running.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs status</code>
...
Full list of resources:
 myapc  (stonith:fence_apc_snmp):       Started z1.example.com
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM):   Started z1.example.com
     nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com
...</pre></li><li class="listitem"><p class="simpara">
						Create the <code class="literal">nfsserver</code> resource named <code class="literal">nfs-daemon</code> as part of the resource group <code class="literal">nfsgroup</code>.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The <code class="literal">nfsserver</code> resource allows you to specify an <code class="literal">nfs_shared_infodir</code> parameter, which is a directory that NFS servers use to store NFS-related stateful information.
						</p><p>
							It is recommended that this attribute be set to a subdirectory of one of the <code class="literal">Filesystem</code> resources you created in this collection of exports. This ensures that the NFS servers are storing their stateful information on a device that will become available to another node if this resource group needs to relocate. In this example;
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">/nfsshare</code> is the shared-storage directory managed by the <code class="literal">Filesystem</code> resource
								</li><li class="listitem">
									<code class="literal">/nfsshare/exports/export1</code> and <code class="literal">/nfsshare/exports/export2</code> are the export directories
								</li><li class="listitem">
									<code class="literal">/nfsshare/nfsinfo</code> is the shared-information directory for the <code class="literal">nfsserver</code> resource
								</li></ul></div></div></div><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create nfs-daemon nfsserver</code> \
<code class="literal">nfs_shared_infodir=/nfsshare/nfsinfo nfs_no_notify=true</code> \
<code class="literal">--group nfsgroup</code>

[root@z1 ~]# <code class="literal">pcs status</code>
...</pre></li><li class="listitem"><p class="simpara">
						Add the <code class="literal">exportfs</code> resources to export the <code class="literal">/nfsshare/exports</code> directory. These resources are part of the resource group <code class="literal">nfsgroup</code>. This builds a virtual directory for NFSv4 clients. NFSv3 clients can access these exports as well.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The <code class="literal">fsid=0</code> option is required only if you want to create a virtual directory for NFSv4 clients. For more information, see <a class="link" href="https://access.redhat.com/solutions/548083/">How do I configure the fsid option in an NFS server’s /etc/exports file?</a>.
						</p></div></div><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create nfs-root exportfs</code> \
<code class="literal">clientspec=192.168.122.0/255.255.255.0</code> \
<code class="literal">options=rw,sync,no_root_squash</code> \
<code class="literal">directory=/nfsshare/exports</code> \
<code class="literal">fsid=0 --group nfsgroup</code>

[root@z1 ~]# # <code class="literal">pcs resource create nfs-export1 exportfs</code> \
<code class="literal">clientspec=192.168.122.0/255.255.255.0</code> \
<code class="literal">options=rw,sync,no_root_squash directory=/nfsshare/exports/export1</code> \
<code class="literal">fsid=1 --group nfsgroup</code>

[root@z1 ~]# # <code class="literal">pcs resource create nfs-export2 exportfs</code> \
<code class="literal">clientspec=192.168.122.0/255.255.255.0</code> \
<code class="literal">options=rw,sync,no_root_squash directory=/nfsshare/exports/export2</code> \
<code class="literal">fsid=2 --group nfsgroup</code></pre></li><li class="listitem"><p class="simpara">
						Add the floating IP address resource that NFS clients will use to access the NFS share. This resource is part of the resource group <code class="literal">nfsgroup</code>. For this example deployment, we are using 192.168.122.200 as the floating IP address.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create nfs_ip IPaddr2</code> \
<code class="literal">ip=192.168.122.200 cidr_netmask=24 --group nfsgroup</code></pre></li><li class="listitem"><p class="simpara">
						Add an <code class="literal">nfsnotify</code> resource for sending NFSv3 reboot notifications once the entire NFS deployment has initialized. This resource is part of the resource group <code class="literal">nfsgroup</code>.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For the NFS notification to be processed correctly, the floating IP address must have a host name associated with it that is consistent on both the NFS servers and the NFS client.
						</p></div></div><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create nfs-notify nfsnotify</code> \
<code class="literal">source_host=192.168.122.200 --group nfsgroup</code></pre></li><li class="listitem"><p class="simpara">
						After creating the resources and the resource constraints, you can check the status of the cluster. Note that all resources are running on the same node.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs status</code>
...
Full list of resources:
 myapc  (stonith:fence_apc_snmp):       Started z1.example.com
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM):   Started z1.example.com
     nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com
     nfs-daemon (ocf::heartbeat:nfsserver):     Started z1.example.com
     nfs-root   (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs-export1        (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs-export2        (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z1.example.com
     nfs-notify (ocf::heartbeat:nfsnotify):     Started z1.example.com
...</pre></li></ol></div></section><section class="section" id="proc_testing-nfs-resource-configuration-configuring-ha-nfs"><div class="titlepage"><div><div><h2 class="title">6.6. Testing the NFS resource configuration</h2></div></div></div><p>
				You can validate your system configuration with the following procedures. You should be able to mount the exported file system with either NFSv3 or NFSv4.
			</p><section class="section" id="testing_the_nfs_export"><div class="titlepage"><div><div><h3 class="title">6.6.1. Testing the NFS export</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On a node outside of the cluster, residing in the same network as the deployment, verify that the NFS share can be seen by mounting the NFS share. For this example, we are using the 192.168.122.0/24 network.
						</p><pre class="literallayout"># <code class="literal">showmount -e 192.168.122.200</code>
Export list for 192.168.122.200:
/nfsshare/exports/export1 192.168.122.0/255.255.255.0
/nfsshare/exports         192.168.122.0/255.255.255.0
/nfsshare/exports/export2 192.168.122.0/255.255.255.0</pre></li><li class="listitem"><p class="simpara">
							To verify that you can mount the NFS share with NFSv4, mount the NFS share to a directory on the client node. After mounting, verify that the contents of the export directories are visible. Unmount the share after testing.
						</p><pre class="literallayout"># <code class="literal">mkdir nfsshare</code>
# <code class="literal">mount -o "vers=4" 192.168.122.200:export1 nfsshare</code>
# <code class="literal">ls nfsshare</code>
clientdatafile1
# <code class="literal">umount nfsshare</code></pre></li><li class="listitem"><p class="simpara">
							Verify that you can mount the NFS share with NFSv3. After mounting, verify that the test file <code class="literal">clientdatafile1</code> is visible. Unlike NFSv4, since NFSv3 does not use the virtual file system, you must mount a specific export. Unmount the share after testing.
						</p><pre class="literallayout"># <code class="literal">mkdir nfsshare</code>
# <code class="literal">mount -o "vers=3" 192.168.122.200:/nfsshare/exports/export2 nfsshare</code>
# <code class="literal">ls nfsshare</code>
clientdatafile2
# <code class="literal">umount nfsshare</code></pre></li></ol></div></section><section class="section" id="testing_for_failover"><div class="titlepage"><div><div><h3 class="title">6.6.2. Testing for failover</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On a node outside of the cluster, mount the NFS share and verify access to the <code class="literal">clientdatafile1</code> we created in <a class="link" href="index.html#proc_configuring-nfs-share-configuring-ha-nfs" title="6.4. Configuring an NFS share">Configuring an NFS share</a>
						</p><pre class="literallayout"># <code class="literal">mkdir nfsshare</code>
# <code class="literal">mount -o "vers=4" 192.168.122.200:export1 nfsshare</code>
# <code class="literal">ls nfsshare</code>
clientdatafile1</pre></li><li class="listitem"><p class="simpara">
							From a node within the cluster, determine which node in the cluster is running <code class="literal">nfsgroup</code>. In this example, <code class="literal">nfsgroup</code> is running on <code class="literal">z1.example.com</code>.
						</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs status</code>
...
Full list of resources:
 myapc  (stonith:fence_apc_snmp):       Started z1.example.com
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM):   Started z1.example.com
     nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com
     nfs-daemon (ocf::heartbeat:nfsserver):     Started z1.example.com
     nfs-root   (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs-export1        (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs-export2        (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z1.example.com
     nfs-notify (ocf::heartbeat:nfsnotify):     Started z1.example.com
...</pre></li><li class="listitem"><p class="simpara">
							From a node within the cluster, put the node that is running <code class="literal">nfsgroup</code> in standby mode.
						</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs node standby z1.example.com</code></pre></li><li class="listitem"><p class="simpara">
							Verify that <code class="literal">nfsgroup</code> successfully starts on the other cluster node.
						</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs status</code>
...
Full list of resources:
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM):   Started z2.example.com
     nfsshare   (ocf::heartbeat:Filesystem):    Started z2.example.com
     nfs-daemon (ocf::heartbeat:nfsserver):     Started z2.example.com
     nfs-root   (ocf::heartbeat:exportfs):      Started z2.example.com
     nfs-export1        (ocf::heartbeat:exportfs):      Started z2.example.com
     nfs-export2        (ocf::heartbeat:exportfs):      Started z2.example.com
     nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z2.example.com
     nfs-notify (ocf::heartbeat:nfsnotify):     Started z2.example.com
...</pre></li><li class="listitem"><p class="simpara">
							From the node outside the cluster on which you have mounted the NFS share, verify that this outside node still continues to have access to the test file within the NFS mount.
						</p><pre class="literallayout"># <code class="literal">ls nfsshare</code>
clientdatafile1</pre><p class="simpara">
							Service will be lost briefly for the client during the failover but the client should recover it with no user intervention. By default, clients using NFSv4 may take up to 90 seconds to recover the mount; this 90 seconds represents the NFSv4 file lease grace period observed by the server on startup. NFSv3 clients should recover access to the mount in a matter of a few seconds.
						</p></li><li class="listitem"><p class="simpara">
							From a node within the cluster, remove the node that was initially running running <code class="literal">nfsgroup</code> from standby mode. This will not in itself move the cluster resources back to this node.
						</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs cluster unstandby z1.example.com</code></pre></li></ol></div></section></section></section><section class="chapter" id="assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 7. GFS2 file systems in a cluster</h1></div></div></div><p>
			This section provides:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					A procedure to set up a Pacemaker cluster that includes GFS2 file systems
				</li><li class="listitem">
					A procedure to migrate RHEL 7 logical volumes that contain GFS2 file systems to a RHEL 8 cluster
				</li></ul></div><section class="section" id="proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster"><div class="titlepage"><div><div><h2 class="title">7.1. Configuring a GFS2 file system in a cluster</h2></div></div></div><p>
				This procedure is an outline of the steps required to set up a Pacemaker cluster that includes GFS2 file systems. This example creates three GFS2 file systems on three logical volumes.
			</p><p>
				As a prerequisite for this procedure, you must install and start the cluster software on all nodes and create a basic two-node cluster. You must also configure fencing for the cluster. For information on creating a Pacemaker cluster and configuring fencing for the cluster, see <a class="link" href="index.html#assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters" title="Chapter 4. Creating a Red Hat High-Availability cluster with Pacemaker">Creating a Red Hat High-Availability cluster with Pacemaker</a>.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On both nodes of the cluster, install the <code class="literal">lvm2-lockd</code>, <code class="literal">gfs2-utils</code>, and <code class="literal">dlm</code> packages. The <code class="literal">lvm2-lockd</code> package is part of the AppStream channel and the <code class="literal">gfs2-utils</code> and <code class="literal">dlm</code> packages are part of the Resilient Storage channel.
					</p><pre class="literallayout"># <code class="literal">yum install lvm2-lockd gfs2-utils dlm</code></pre></li><li class="listitem"><p class="simpara">
						Set up a <code class="literal">dlm</code> resource. This is a required dependency for configuring a GFS2 file system in a cluster. This example creates the <code class="literal">dlm</code> resource as part of a resource group named <code class="literal">locking</code>.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence</code></pre></li><li class="listitem"><p class="simpara">
						Clone the <code class="literal">locking</code> resource group so that the resource group can be active on both nodes of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource clone locking interleave=true</code></pre></li><li class="listitem"><p class="simpara">
						Set up an <code class="literal">lvmlockd</code> resource as part of the group <code class="literal">locking</code>.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence</code></pre></li><li class="listitem"><p class="simpara">
						Check the status of the cluster to ensure that the <code class="literal">locking</code> resource group has started on both nodes of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs status --full</code>
Cluster name: my_cluster
[...]

Online: [ z1.example.com (1) z2.example.com (2) ]

Full list of resources:

 smoke-apc      (stonith:fence_apc):    Started z1.example.com
 Clone Set: locking-clone [locking]
     Resource Group: locking:0
         dlm    (ocf::pacemaker:controld):      Started z1.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Resource Group: locking:1
         dlm    (ocf::pacemaker:controld):      Started z2.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
     Started: [ z1.example.com z2.example.com ]</pre></li><li class="listitem"><p class="simpara">
						Verify that the <code class="literal">lvmlockd</code> daemon is running on both nodes of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">ps -ef | grep lvmlockd</code>
root     12257     1  0 17:45 ?        00:00:00 lvmlockd -p /run/lvmlockd.pid -A 1 -g dlm
[root@z2 ~]# <code class="literal">ps -ef | grep lvmlockd</code>
root     12270     1  0 17:45 ?        00:00:00 lvmlockd -p /run/lvmlockd.pid -A 1 -g dlm</pre></li><li class="listitem"><p class="simpara">
						On one node of the cluster, create two shared volume groups. One volume group will contain two GFS2 file systems, and the other volume group will contain one GFS2 file system.
					</p><p class="simpara">
						The following command creates the shared volume group <code class="literal">shared_vg1</code> on <code class="literal">/dev/vdb</code>.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">vgcreate --shared shared_vg1 /dev/vdb</code>
  Physical volume "/dev/vdb" successfully created.
  Volume group "shared_vg1" successfully created
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre><p class="simpara">
						The following command creates the shared volume group <code class="literal">shared_vg2</code> on <code class="literal">/dev/vdc</code>.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">vgcreate --shared shared_vg2 /dev/vdc</code>
  Physical volume "/dev/vdc" successfully created.
  Volume group "shared_vg2" successfully created
  VG shared_vg2 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre></li><li class="listitem"><p class="simpara">
						On the second node in the cluster, start the lock manager for each of the shared volume groups.
					</p><pre class="literallayout">[root@z2 ~]# <code class="literal">vgchange --lock-start shared_vg1</code>
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...
[root@z2 ~]# <code class="literal">vgchange --lock-start shared_vg2</code>
  VG shared_vg2 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre></li><li class="listitem"><p class="simpara">
						On one node in the cluster, create the shared logical volumes and format the volumes with a GFS2 file system. One journal is required for each node that mounts the file system. Ensure that you create enough journals for each of the nodes in your cluster.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">lvcreate --activate sy -L5G -n shared_lv1 shared_vg1</code>
  Logical volume "shared_lv1" created.
[root@z1 ~]# <code class="literal">lvcreate --activate sy -L5G -n shared_lv2 shared_vg1</code>
  Logical volume "shared_lv2" created.
[root@z1 ~]# <code class="literal">lvcreate --activate sy -L5G -n shared_lv1 shared_vg2</code>
  Logical volume "shared_lv1" created.

[root@z1 ~]# <code class="literal">mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/shared_vg1/shared_lv1</code>
[root@z1 ~]# <code class="literal">mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo2 /dev/shared_vg1/shared_lv2</code>
[root@z1 ~]# <code class="literal">mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo3 /dev/shared_vg2/shared_lv1</code></pre></li><li class="listitem"><p class="simpara">
						Create an <code class="literal">LVM-activate</code> resource for each logical volume to automatically activate that logical volume on all nodes.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create an <code class="literal">LVM-activate</code> resource named <code class="literal">sharedlv1</code> for the logical volume <code class="literal">shared_lv1</code> in volume group <code class="literal">shared_vg1</code>. This command also creates the resource group <code class="literal">shared_vg1</code> that includes the resource. In this example, the resource group has the same name as the shared volume group that includes the logical volume.
							</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd</code></pre></li><li class="listitem"><p class="simpara">
								Create an <code class="literal">LVM-activate</code> resource named <code class="literal">sharedlv2</code> for the logical volume <code class="literal">shared_lv2</code> in volume group <code class="literal">shared_vg1</code>. This resource will also be part of the resource group <code class="literal">shared_vg1</code>.
							</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create sharedlv2 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv2 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd</code></pre></li><li class="listitem"><p class="simpara">
								Create an <code class="literal">LVM-activate</code> resource named <code class="literal">sharedlv3</code> for the logical volume <code class="literal">shared_lv1</code> in volume group <code class="literal">shared_vg2</code>. This command also creates the resource group <code class="literal">shared_vg2</code> that includes the resource.
							</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create sharedlv3 --group shared_vg2 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg2 activation_mode=shared vg_access_mode=lvmlockd</code></pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Clone the two new resource groups.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource clone shared_vg1 interleave=true</code>
[root@z1 ~]# <code class="literal">pcs resource clone shared_vg2 interleave=true</code></pre></li><li class="listitem"><p class="simpara">
						Configure ordering constraints to ensure that the <code class="literal">locking</code> resource group that includes the <code class="literal">dlm</code> and <code class="literal">lvmlockd</code> resources starts first.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs constraint order start locking-clone then shared_vg1-clone</code>
Adding locking-clone shared_vg1-clone (kind: Mandatory) (Options: first-action=start then-action=start)
[root@z1 ~]# <code class="literal">pcs constraint order start locking-clone then shared_vg2-clone</code>
Adding locking-clone shared_vg2-clone (kind: Mandatory) (Options: first-action=start then-action=start)</pre></li><li class="listitem"><p class="simpara">
						Configure colocation constraints to ensure that the <code class="literal">vg1</code> and <code class="literal">vg2</code> resource groups start on the same node as the <code class="literal">locking</code> resource group.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs constraint colocation add shared_vg1-clone with locking-clone</code>
[root@z1 ~]# <code class="literal">pcs constraint colocation add shared_vg2-clone with locking-clone</code></pre></li><li class="listitem"><p class="simpara">
						On both nodes in the cluster, verify that the logical volumes are active. There may be a delay of a few seconds.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">lvs</code>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g
  shared_lv2 shared_vg1  -wi-a----- 5.00g
  shared_lv1 shared_vg2  -wi-a----- 5.00g

[root@z2 ~]# <code class="literal">lvs</code>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g
  shared_lv2 shared_vg1  -wi-a----- 5.00g
  shared_lv1 shared_vg2  -wi-a----- 5.00g</pre></li><li class="listitem"><p class="simpara">
						Create a file system resource to automatically mount each GFS2 file system on all nodes.
					</p><p class="simpara">
						You should not add the file system to the <code class="literal">/etc/fstab</code> file because it will be managed as a Pacemaker cluster resource. Mount options can be specified as part of the resource configuration with <code class="literal">options=<span class="emphasis"><em>options</em></span></code>. Run the <code class="literal command">pcs resource describe Filesystem</code> command for full configuration options.
					</p><p class="simpara">
						The following commands create the file system resources. These commands add each resource to the resource group that includes the logical volume resource for that file system.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/shared_vg1/shared_lv1" directory="/mnt/gfs1" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</code>
[root@z1 ~]# <code class="literal">pcs resource create sharedfs2 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/shared_vg1/shared_lv2" directory="/mnt/gfs2" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</code>
[root@z1 ~]# <code class="literal">pcs resource create sharedfs3 --group shared_vg2 ocf:heartbeat:Filesystem device="/dev/shared_vg2/shared_lv1" directory="/mnt/gfs3" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</code></pre></li><li class="listitem"><p class="simpara">
						Verify that the GFS2 file systems are mounted on both nodes of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">mount | grep gfs2</code>
/dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)

[root@z2 ~]# <code class="literal">mount | grep gfs2</code>
/dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)</pre></li><li class="listitem"><p class="simpara">
						Check the status of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs status --full</code>
Cluster name: my_cluster
[...1

Full list of resources:

 smoke-apc      (stonith:fence_apc):    Started z1.example.com
 Clone Set: locking-clone [locking]
     Resource Group: locking:0
         dlm    (ocf::pacemaker:controld):      Started z2.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
     Resource Group: locking:1
         dlm    (ocf::pacemaker:controld):      Started z1.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Started: [ z1.example.com z2.example.com ]
 Clone Set: shared_vg1-clone [shared_vg1]
     Resource Group: shared_vg1:0
         sharedlv1      (ocf::heartbeat:LVM-activate):  Started z2.example.com
         sharedlv2      (ocf::heartbeat:LVM-activate):  Started z2.example.com
         sharedfs1      (ocf::heartbeat:Filesystem):    Started z2.example.com
         sharedfs2      (ocf::heartbeat:Filesystem):    Started z2.example.com
     Resource Group: shared_vg1:1
         sharedlv1      (ocf::heartbeat:LVM-activate):  Started z1.example.com
         sharedlv2      (ocf::heartbeat:LVM-activate):  Started z1.example.com
         sharedfs1      (ocf::heartbeat:Filesystem):    Started z1.example.com
         sharedfs2      (ocf::heartbeat:Filesystem):    Started example.co
     Started: [ z1.example.com z2.example.com ]
 Clone Set: shared_vg2-clone [shared_vg2]
     Resource Group: shared_vg2:0
         sharedlv3      (ocf::heartbeat:LVM-activate):  Started z2.example.com
         sharedfs3      (ocf::heartbeat:Filesystem):    Started z2.example.com
     Resource Group: shared_vg2:1
         sharedlv3      (ocf::heartbeat:LVM-activate):  Started z1.example.com
         sharedfs3      (ocf::heartbeat:Filesystem):    Started z1.example.com
     Started: [ z1.example.com z2.example.com ]

...</pre></li></ol></div></section><section class="section" id="proc_migrate-gfs2-rhel7-rhel8-configuring-gfs2-cluster"><div class="titlepage"><div><div><h2 class="title">7.2. Migrating a GFS2 file system from RHEL7 to RHEL8</h2></div></div></div><p>
				In Red Hat Enterprise Linux 8, LVM uses the LVM lock daemon <code class="literal">lvmlockd</code> instead of <code class="literal">clvmd</code> for managing shared storage devices in an active/active cluster. This requires that you configure the logical volumes that your active/active cluster will require as shared logical volumes. Additionally, this requires that you use the <code class="literal">LVM-activate</code> resource to manage an LVM volume and that you use the <code class="literal">lvmlockd</code> resource agent to manage the <code class="literal">lvmlockd</code> daemon. See <a class="link" href="index.html#proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster" title="7.1. Configuring a GFS2 file system in a cluster">Configuring a GFS2 file system in a cluster</a> for a full procedure for configuring a Pacemaker cluster that includes GFS2 file systems using shared logical volumes.
			</p><p>
				To use your existing Red Hat Enterprise Linux 7 logical volumes when configuring a RHEL8 cluster that includes GFS2 file systems, perform the following procedure from the RHEL8 cluster. In this example, the clustered RHEL 7 logical volume is part of the volume group <code class="literal">upgrade_gfs_vg</code>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The RHEL8 cluster must have the same name as the RHEL7 cluster that includes the GFS2 file system in order for the existing file system to be valid.
				</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Ensure that the logical volumes containing the GFS2 file systems are currently inactive. This procedure is safe only if all nodes have stopped using the volume group.
					</li><li class="listitem"><p class="simpara">
						From one node in the cluster, forcibly change the volume group to be local.
					</p><pre class="literallayout">[root@rhel8-01 ~]# <code class="literal">vgchange --lock-type none --lock-opt force upgrade_gfs_vg</code>
Forcibly change VG lock type to none? [y/n]: y
  Volume group "upgrade_gfs_vg" successfully changed</pre></li><li class="listitem"><p class="simpara">
						From one node in the cluster, change the local volume group to a shared volume group
					</p><pre class="literallayout">[root@rhel8-01 ~]# <code class="literal">vgchange --lock-type dlm upgrade_gfs_vg</code>
   Volume group "upgrade_gfs_vg" successfully changed</pre></li><li class="listitem"><p class="simpara">
						On each node in the cluster, start locking for the volume group.
					</p><pre class="literallayout">[root@rhel8-01 ~]# <code class="literal">vgchange --lock-start upgrade_gfs_vg</code>
  VG upgrade_gfs_vg starting dlm lockspace
  Starting locking.  Waiting until locks are ready...
[root@rhel8-02 ~]# <code class="literal">vgchange --lock-start upgrade_gfs_vg</code>
  VG upgrade_gfs_vg starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre></li></ol></div><p>
				After performing this procedure, you can create an <code class="literal">LVM-activate</code> resource for each logical volume.
			</p></section></section><section class="chapter" id="assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Getting started with the pcsd Web UI</h1></div></div></div><p>
			The <code class="literal">pcsd</code> Web UI is a graphical user interface to create and configure Pacemaker/Corosync clusters.
		</p><section class="section" id="proc_installing-cluster-software-getting-started-with-the-pcsd-web-ui"><div class="titlepage"><div><div><h2 class="title">8.1. Installing cluster software</h2></div></div></div><p>
				The following procedure installs the cluster software and configures your system for cluster creation.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On each node in the cluster, install the Red Hat High Availability Add-On software packages along with all available fence agents from the High Availability channel.
					</p><pre class="literallayout"># <code class="literal">yum install pcs pacemaker fence-agents-all</code></pre><p class="simpara">
						Alternatively, you can install the Red Hat High Availability Add-On software packages along with only the fence agent that you require with the following command.
					</p><pre class="literallayout"># <code class="literal">yum install pcs pacemaker fence-agents-<span class="emphasis"><em>model</em></span></code></pre><p class="simpara">
						The following command displays a list of the available fence agents.
					</p><pre class="literallayout"># <code class="literal">rpm -q -a | grep fence</code>
fence-agents-rhevm-4.0.2-3.el7.x86_64
fence-agents-ilo-mp-4.0.2-3.el7.x86_64
fence-agents-ipmilan-4.0.2-3.el7.x86_64
...</pre><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							After you install the Red Hat High Availability Add-On packages, you should ensure that your software update preferences are set so that nothing is installed automatically. Installation on a running cluster can cause unexpected behaviors. For more information, see <a class="link" href="https://access.redhat.com/articles/2059253/">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster</a>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						If you are running the <code class="literal command">firewalld</code> daemon, execute the following commands to enable the ports that are required by the Red Hat High Availability Add-On.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can determine whether the <code class="literal command">firewalld</code> daemon is installed on your system with the <code class="literal command">rpm -q firewalld</code> command. If it is installed, you can determine whether it is running with the <code class="literal command">firewall-cmd --state</code> command.
						</p></div></div><pre class="literallayout"># <code class="literal">firewall-cmd --permanent --add-service=high-availability</code>
# <code class="literal">firewall-cmd --add-service=high-availability</code></pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The ideal firewall configuration for cluster components depends on the local environment, where you may need to take into account such considerations as whether the nodes have multiple network interfaces or whether off-host firewalling is present. The example here, which opens the ports that are generally required by a Pacemaker cluster, should be modified to suit local conditions. <a class="link" href="index.html#proc_enabling-ports-for-high-availability-creating-high-availability-cluster" title="4.7. Enabling ports for the High Availability Add-On">Enabling ports for the High Availability Add-On</a> shows the ports to enable for the Red Hat High Availability Add-On and provides an explanation for what each port is used for.
						</p></div></div></li><li class="listitem"><p class="simpara">
						In order to use <code class="literal">pcs</code> to configure the cluster and communicate among the nodes, you must set a password on each node for the user ID <code class="literal">hacluster</code>, which is the <code class="literal">pcs</code> administration account. It is recommended that the password for user <code class="literal">hacluster</code> be the same on each node.
					</p><pre class="literallayout"># <code class="literal">passwd hacluster</code>
Changing password for user hacluster.
New password:
Retype new password:
passwd: all authentication tokens updated successfully.</pre></li><li class="listitem"><p class="simpara">
						Before the cluster can be configured, the <code class="literal command">pcsd</code> daemon must be started and enabled to start up on boot on each node. This daemon works with the <code class="literal command">pcs</code> command to manage configuration across the nodes in the cluster.
					</p><p class="simpara">
						On each node in the cluster, execute the following commands to start the <code class="literal">pcsd</code> service and to enable <code class="literal">pcsd</code> at system start.
					</p><pre class="literallayout"># <code class="literal">systemctl start pcsd.service</code>
# <code class="literal">systemctl enable pcsd.service</code></pre></li></ol></div></section><section class="section" id="proc_setting-up-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui"><div class="titlepage"><div><div><h2 class="title">8.2. Setting up the pcsd Web UI</h2></div></div></div><p>
				After you have installed the Pacemaker configuration tools and configured your system for cluster configuration, use the following procedure to set up your system to use the <code class="literal command">pcsd</code> Web UI to configure a cluster.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On any system, open a browser to the following URL, specifying one of the nodes of the cluster (note that this uses the <code class="literal">https</code> protocol). This brings up the <code class="literal command">pcsd</code> Web UI login screen.
					</p><pre class="literallayout"><a class="link" href="https://">https://</a><span class="emphasis"><em>nodename</em></span>:2224</pre></li><li class="listitem"><p class="simpara">
						Log in as user <code class="literal">hacluster</code>. This brings up the <code class="literal">Manage Clusters</code> page as shown in <a class="xref" href="index.html#getting-started-with-the-pcsd-web-ui-fig-manage-cluster" title="Figure 8.1. Manage Clusters page">Figure 8.1, “Manage Clusters page”</a>.
					</p><div class="figure" id="getting-started-with-the-pcsd-web-ui-fig-manage-cluster"><p class="title"><strong>Figure 8.1. Manage Clusters page</strong></p><div class="figure-contents"><div class="mediaobject"><img src="manageclusters.png" alt="The Manage Clusters page"/></div></div></div></li></ol></div></section><section class="section" id="assembly_creating-a-cluster-with-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui"><div class="titlepage"><div><div><h2 class="title">8.3. Creating a cluster with the pcsd Web UI</h2></div></div></div><p>
				From the Manage Clusters page, you can create a new cluster, add an existing cluster to the Web UI, or remove a cluster from the Web UI.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						To create a cluster, click on <code class="literal">Create New</code>. Enter the name of the cluster to create and the nodes that constitute the cluster. If you have not previously authenticated the user <code class="literal">hacluster</code> for each node in the cluster, you will be asked to authenticate the cluster nodes.
					</li><li class="listitem">
						When creating the cluster, you can configure advanced cluster options by clicking <code class="literal">Go to advanced settings</code> on this screen. The advanced cluster configurations you can configure are described in <a class="link" href="index.html#proc_configuring-advanced-cluster-options-with-the-pcsd-web-ui-creating-a-cluster-with-the-pcsd-web-ui" title="8.3.1. Configuring advanced cluster configuration options with the pcsd Web UI">Configuring advanced cluster configuration options with the pcsd Web UI</a>.
					</li><li class="listitem">
						To add an existing cluster to the Web UI, click on <code class="literal">Add Existing</code> and enter the host name or IP address of a node in the cluster that you would like to manage with the Web UI.
					</li></ul></div><p>
				Once you have created or added a cluster, the cluster name is displayed on the Manage Cluster page. Selecting the cluster displays information about the cluster.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					When using the <code class="literal command">pcsd</code> Web UI to configure a cluster, you can move your mouse over the text describing many of the options to see longer descriptions of those options as a <code class="literal">tooltip</code> display.
				</p></div></div><section class="section" id="proc_configuring-advanced-cluster-options-with-the-pcsd-web-ui-creating-a-cluster-with-the-pcsd-web-ui"><div class="titlepage"><div><div><h3 class="title">8.3.1. Configuring advanced cluster configuration options with the pcsd Web UI</h3></div></div></div><p>
					When creating a cluster, you can configure additional cluster options by clicking <span class="guibutton">Go to advanced settings</span> on the Create cluster screen. This allows you to modify the configurable settings of the following cluster components:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Transport settings: Values for the transport mechanism used for cluster communication
						</li><li class="listitem">
							Quorum settings: Values for the quorum options of the <code class="literal">votequorum</code> service
						</li><li class="listitem">
							Totem settings: Values for the Totem protocol used by Corosync
						</li></ul></div><p>
					Selecting those options displays the settings you can configure. For information on each of the settings, place the mouse pointer over the particular option.
				</p></section><section class="section" id="proc_setting-permissions-with-the-pcsd-web-ui-creating-a-cluster-with-the-pcsd-web-ui"><div class="titlepage"><div><div><h3 class="title">8.3.2. Setting cluster management permissions</h3></div></div></div><p>
					There are two sets of cluster permissions that you can grant to users:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Permissions for managing the cluster with the Web UI, which also grants permissions to run <code class="literal command">pcs</code> commands that connect to nodes over a network. This section describes how to configure those permissions with the Web UI.
						</li><li class="listitem">
							Permissions for local users to allow read-only or read-write access to the cluster configuration, using ACLs. Configuring ACLs with the Web UI is described in <a class="link" href="index.html#proc_configuring-cluster-components-with-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui" title="8.4. Configuring cluster components with the pcsd Web UI">Configuring cluster components with the pcsd Web UI</a>.
						</li></ul></div><p>
					You can grant permission for specific users other than user <code class="literal">hacluster</code> to manage the cluster through the Web UI and to run <code class="literal command">pcs</code> commands that connect to nodes over a network by adding them to the group <code class="literal">haclient</code>. You can then configure the permissions set for an individual member of the group <code class="literal">haclient</code> by clicking the Permissions tab on the Manage Clusters page and setting the permissions on the resulting screen. From this screen, you can also set permissions for groups.
				</p><p>
					You can grant the following permissions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Read permissions, to view the cluster settings
						</li><li class="listitem">
							Write permissions, to modify the cluster settings (except for permissions and ACLs)
						</li><li class="listitem">
							Grant permissions, to modify the cluster permissions and ACLs
						</li><li class="listitem">
							Full permissions, for unrestricted access to a cluster, including adding and removing nodes, with access to keys and certificates
						</li></ul></div></section></section><section class="section" id="proc_configuring-cluster-components-with-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui"><div class="titlepage"><div><div><h2 class="title">8.4. Configuring cluster components with the pcsd Web UI</h2></div></div></div><p>
				To configure the components and attributes of a cluster, click on the name of the cluster displayed on the <code class="literal">Manage Clusters</code> screen. This brings up the <code class="literal">Nodes</code> page, as described in <a class="xref" href="index.html#s2-guiclustnodes-HAAR" title="8.4.1. Configuring cluster nodes with the pcsd Web UI">Section 8.4.1, “Configuring cluster nodes with the pcsd Web UI”</a>. This page displays a menu along the top of the page with the following entries:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Nodes, as described in <a class="xref" href="index.html#s2-guiclustnodes-HAAR" title="8.4.1. Configuring cluster nodes with the pcsd Web UI">Section 8.4.1, “Configuring cluster nodes with the pcsd Web UI”</a>
					</li><li class="listitem">
						Resources, as described in <a class="xref" href="index.html#s2-guiclustresources-HAAR" title="8.4.2. Configuring cluster resources with the pcsd Web UI">Section 8.4.2, “Configuring cluster resources with the pcsd Web UI”</a>
					</li><li class="listitem">
						Fence Devices, as described in <a class="xref" href="index.html#s2-guifencedevices-HAAR" title="8.4.3. Configuring fence devices with the pcsd Web UI">Section 8.4.3, “Configuring fence devices with the pcsd Web UI”</a>
					</li><li class="listitem">
						ACLs, as described in <a class="xref" href="index.html#s2-guiaclset-HAAR" title="8.4.4. Configuring ACLs with the pcsd Web UI">Section 8.4.4, “Configuring ACLs with the pcsd Web UI”</a>
					</li><li class="listitem">
						Cluster Properties, as described in <a class="xref" href="index.html#s2-guiclustprops-HAAR" title="8.4.5. Configuring cluster properties with the pcsd Web UI">Section 8.4.5, “Configuring cluster properties with the pcsd Web UI”</a>
					</li></ul></div><section class="section" id="s2-guiclustnodes-HAAR"><div class="titlepage"><div><div><h3 class="title">8.4.1. Configuring cluster nodes with the pcsd Web UI</h3></div></div></div><p>
					Selecting the <code class="literal">Nodes</code> option from the menu along the top of the cluster management page displays the currently configured nodes and the status of the currently selected node, including which resources are running on the node and the resource location preferences. This is the default page that is displayed when you select a cluster from the <code class="literal">Manage Clusters</code> screen.
				</p><p>
					Form this page, You can add or remove nodes. You can also start, stop, restart, or put a node in standby or maintenance mode. For information on standby mode, see <a class="link" href="index.html#proc_stopping-individual-node-cluster-maintenance" title="29.1. Putting a node into standby mode">Putting a node into standby mode</a>. For information on maintenance mode, see <a class="link" href="index.html#proc_setting-maintenance-mode-cluster-maintenance" title="29.5. Putting a cluster in maintenance mode">Putting a cluster in maintenance mode</a>.
				</p><p>
					You can also configure fence devices directly from this page, as described in by selecting <code class="literal">Configure Fencing</code>. Configuring fence devices is described in <a class="xref" href="index.html#s2-guifencedevices-HAAR" title="8.4.3. Configuring fence devices with the pcsd Web UI">Section 8.4.3, “Configuring fence devices with the pcsd Web UI”</a>.
				</p></section><section class="section" id="s2-guiclustresources-HAAR"><div class="titlepage"><div><div><h3 class="title">8.4.2. Configuring cluster resources with the pcsd Web UI</h3></div></div></div><p>
					Selecting the <code class="literal">Resources</code> option from the menu along the top of the cluster management page displays the currently configured resources for the cluster, organized according to resource groups. Selecting a group or a resource displays the attributes of that group or resource.
				</p><p>
					From this screen, you can add or remove resources, you can edit the configuration of existing resources, and you can create a resource group.
				</p><p>
					To add a new resource to the cluster:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Click <code class="literal">Add</code>. This brings up the <code class="literal">Add Resource</code> screen.
						</li><li class="listitem">
							When you select a resource type from the dropdown <code class="literal">Type</code> menu, the arguments you must specify for that resource appear in the menu.
						</li><li class="listitem">
							You can click <code class="literal">Optional Arguments</code> to display additional arguments you can specify for the resource you are defining.
						</li><li class="listitem">
							After entering the parameters for the resource you are creating, click <code class="literal">Create Resource</code>.
						</li></ul></div><p>
					When configuring the arguments for a resource, a brief description of the argument appears in the menu. If you move the cursor to the field, a longer help description of that argument is displayed.
				</p><p>
					You can define a resource as a cloned resource, or as a promotable clone resource. For information on these resource types, see <a class="link" href="index.html#assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters" title="Chapter 17. Creating cluster resources that are active on multiple nodes (cloned resources)">Creating cluster resources that are active on multiple nodes (cloned resources)</a>.
				</p><p>
					Once you have created at least one resource, you can create a resource group. For general information on resource groups, see <a class="link" href="index.html#assembly_resource-groups-configuring-cluster-resources" title="10.4. Configuring resource groups">Configuring resource groups</a>.
				</p><p>
					To create a resource group:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Select the resources that will be part of the group from the <code class="literal">Resources</code> screen, then click <code class="literal">Create Group</code>. This displays the <code class="literal">Create Group</code> screen.
						</li><li class="listitem">
							From the <code class="literal">Create Group</code> screen, you can rearrange the order of the resources in a resource group by using drag-and-drop to move the list of the resources around.
						</li><li class="listitem">
							Enter a group name and click <code class="literal">Create Group</code>. This returns you to the <code class="literal">Resources</code> screen, which now displays the group name and the resources within that group.
						</li></ul></div><p>
					After you have created a resource group, you can indicate that group’s name as a resource parameter when you create or modify additional resources.
				</p></section><section class="section" id="s2-guifencedevices-HAAR"><div class="titlepage"><div><div><h3 class="title">8.4.3. Configuring fence devices with the pcsd Web UI</h3></div></div></div><p>
					Selecting the <code class="literal">Fence Devices</code> option from the menu along the top of the cluster management page displays <code class="literal">Fence Devices</code> screen, showing the currently configured fence devices.
				</p><p>
					To add a new fence device to the cluster:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Click <code class="literal">Add</code>. This brings up the <code class="literal">Add Fence Device</code> screen.
						</li><li class="listitem">
							When you select a fence device type from the drop-down <code class="literal">Type</code> menu, the arguments you must specify for that fence device appear in the menu.
						</li><li class="listitem">
							You can click on <code class="literal">Optional Arguments</code> to display additional arguments you can specify for the fence device you are defining.
						</li><li class="listitem">
							After entering the parameters for the new fence device, click <code class="literal">Create Fence Instance</code>.
						</li></ul></div><p>
					To configure an SBD fencing device, click on <code class="literal">SBD</code> on the <code class="literal">Fence Devices</code> screen. This calls up a screen that allows you to enable or disable SBD in the cluster.
				</p><p>
					For more information on fence devices, see <a class="link" href="index.html#assembly_configuring-fencing-configuring-and-managing-high-availability-clusters" title="Chapter 9. Configuring fencing in a Red Hat High Availability cluster">Configuring fencing in a Red Hat High Availability cluster</a>.
				</p></section><section class="section" id="s2-guiaclset-HAAR"><div class="titlepage"><div><div><h3 class="title">8.4.4. Configuring ACLs with the pcsd Web UI</h3></div></div></div><p>
					Selecting the <code class="literal">ACLS</code> option from the menu along the top of the cluster management page displays a screen from which you can set permissions for local users, allowing read-only or read-write access to the cluster configuration by using access control lists (ACLs).
				</p><p>
					To assign ACL permissions, you create a role and define the access permissions for that role. Each role can have an unlimited number of permissions (read/write/deny) applied to either an XPath query or the ID of a specific element. After defining the role, you can assign it to an existing user or group.
				</p><p>
					For more information on assigning permission using ACLs, see <a class="link" href="index.html#proc_setting-local-cluster-permissions-cluster-permissions" title="19.2. Setting local permissions using ACLs">Setting local permissions using ACLs</a>.
				</p></section><section class="section" id="s2-guiclustprops-HAAR"><div class="titlepage"><div><div><h3 class="title">8.4.5. Configuring cluster properties with the pcsd Web UI</h3></div></div></div><p>
					Selecting the <code class="literal">Cluster Properties</code> option from the menu along the top of the cluster management page displays the cluster properties and allows you to modify these properties from their default values. For information on the Pacemaker cluster properties, see <a class="link" href="index.html#ref_cluster-properties-options-controlling-cluster-behavior" title="21.1. Summary of cluster properties and options">Pacemaker cluster properties</a>.
				</p></section></section><section class="section" id="proc_configuring-ha-pcsd-web-ui-getting-started-with-the-pcsd-web-ui"><div class="titlepage"><div><div><h2 class="title">8.5. Configuring a high availability pcsd Web UI</h2></div></div></div><p>
				When you use the <code class="literal">pcsd</code> Web UI, you connect to one of the nodes of the cluster to display the cluster management pages. If the node to which you are connecting goes down or becomes unavailable, you can reconnect to the cluster by opening your browser to a URL that specifies a different node of the cluster.
			</p><p>
				It is possible, however, to configure the <code class="literal">pcsd</code> Web UI itself for high availability, in which case you can continue to manage the cluster without entering a new URL.
			</p><p>
				To configure the <code class="literal">pcsd</code> Web UI for high availability, perform the following steps.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Ensure that the <code class="literal">pcsd</code> certificates are synced across the nodes of the cluster by setting <code class="literal">PCSD_SSL_CERT_SYNC_ENABLED</code> to <code class="literal">true</code> in the <code class="literal">/etc/sysconfig/pcsd</code> configuration file. Enabling certificate syncing causes <code class="literal">pcsd</code> to sync the certificates for the cluster setup and node add commands. In RHEL 8, <code class="literal">PCSD_SSL_CERT_SYNC_ENABLED</code> is set to <code class="literal">false</code> by default.
					</li><li class="listitem">
						Create an <code class="literal">IPaddr2</code> cluster resource, which is a floating IP address that you will use to connect to the <code class="literal">pcsd</code> Web UI. The IP address must not be one already associated with a physical node. If the <code class="literal">IPaddr2</code> resource’s NIC device is not specified, the floating IP must reside on the same network as one of the node’s statically assigned IP addresses, otherwise the NIC device to assign the floating IP address cannot be properly detected.
					</li><li class="listitem"><p class="simpara">
						Create custom SSL certificates for use with <code class="literal">pcsd</code> and ensure that they are valid for the addresses of the nodes used to connect to the <code class="literal">pcsd</code> Web UI.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								To create custom SSL certificates, you can use either wildcard certificates or you can use the Subject Alternative Name certificate extension. For information on the Red Hat Certificate System, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_certificate_system/9/html/administration_guide/index">Red Hat Certificate System Administration Guide</a>.
							</li><li class="listitem">
								Install the custom certificates for <code class="literal">pcsd</code> with the <code class="literal">pcs pcsd certkey</code> command.
							</li><li class="listitem">
								Sync the <code class="literal">pcsd</code> certificates to all nodes in the cluster with the <code class="literal">pcs pcsd sync-certificates</code> command.
							</li></ol></div></li><li class="listitem">
						Connect to the <code class="literal">pcsd</code> Web UI using the floating IP address you configured as a cluster resource.
					</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Even when you configure the <code class="literal">pcsd</code> Web UI for high availability, you will be asked to log in again when the node to which you are connecting goes down.
				</p></div></div></section></section><section class="chapter" id="assembly_configuring-fencing-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 9. Configuring fencing in a Red Hat High Availability cluster</h1></div></div></div><p>
			A node that is unresponsive may still be accessing data. The only way to be certain that your data is safe is to fence the node using STONITH. STONITH is an acronym for "Shoot The Other Node In The Head" and it protects your data from being corrupted by rogue nodes or concurrent access. Using STONITH, you can be certain that a node is truly offline before allowing the data to be accessed from another node.
		</p><p>
			STONITH also has a role to play in the event that a clustered service cannot be stopped. In this case, the cluster uses STONITH to force the whole node offline, thereby making it safe to start the service elsewhere.
		</p><p>
			For more complete general information on fencing and its importance in a Red Hat High Availability cluster, see <a class="link" href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</a>.
		</p><p>
			You implement STONITH in a Pacemaker cluster by configuring fence devices for the nodes of the cluster.
		</p><section class="section" id="proc_displaying-fence-agents-configuring-fencing"><div class="titlepage"><div><div><h2 class="title">9.1. Displaying available fence agents and their options</h2></div></div></div><p>
				Use the following command to view of list of all available STONITH agents. When you specify a filter, this command displays only the STONITH agents that match the filter.
			</p><pre class="literallayout">pcs stonith list [<span class="emphasis"><em>filter</em></span>]</pre><p>
				Use the following command to view the options for the specified STONITH agent.
			</p><pre class="literallayout">pcs stonith describe <span class="emphasis"><em>stonith_agent</em></span></pre><p>
				For example, the following command displays the options for the fence agent for APC over telnet/SSH.
			</p><pre class="literallayout"># <code class="literal">pcs stonith describe fence_apc</code>
Stonith options for: fence_apc
  ipaddr (required): IP Address or Hostname
  login (required): Login Name
  passwd: Login password or passphrase
  passwd_script: Script to retrieve password
  cmd_prompt: Force command prompt
  secure: SSH connection
  port (required): Physical plug number or name of virtual machine
  identity_file: Identity file for ssh
  switch: Physical switch number on device
  inet4_only: Forces agent to use IPv4 addresses only
  inet6_only: Forces agent to use IPv6 addresses only
  ipport: TCP port to use for connection with device
  action (required): Fencing Action
  verbose: Verbose mode
  debug: Write debug information to given file
  version: Display version information and exit
  help: Display help and exit
  separator: Separator for CSV created by operation list
  power_timeout: Test X seconds for status change after ON/OFF
  shell_timeout: Wait X seconds for cmd prompt after issuing command
  login_timeout: Wait X seconds for cmd prompt after login
  power_wait: Wait X seconds after issuing ON/OFF
  delay: Wait X seconds before fencing is started
  retry_on: Count of attempts to retry power on</pre><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					For fence agents that provide a <code class="literal">method</code> option, a value of <code class="literal">cycle</code> is unsupported and should not be specified, as it may cause data corruption.
				</p></div></div></section><section class="section" id="proc_creating-fence-devices-configuring-fencing"><div class="titlepage"><div><div><h2 class="title">9.2. Creating a fence device</h2></div></div></div><p>
				The format for the command to create a stonith device is as follows. For a listing of the available stonith creation options, see the <code class="literal command">pcs stonith -h</code> display.
			</p><pre class="literallayout">pcs stonith create <span class="emphasis"><em>stonith_id</em></span> <span class="emphasis"><em>stonith_device_type</em></span> [<span class="emphasis"><em>stonith_device_options</em></span>] [op  <span class="emphasis"><em>operation_action</em></span> <span class="emphasis"><em>operation_options</em></span>]</pre><p>
				The following command creates a single fencing device for a single node.
			</p><pre class="literallayout"># <code class="literal">pcs stonith create MyStonith fence_virt pcmk_host_list=f1 op monitor interval=30s</code></pre><p>
				Some fence devices can fence only a single node, while other devices can fence multiple nodes. The parameters you specify when you create a fencing device depend on what your fencing device supports and requires.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Some fence devices can automatically determine what nodes they can fence.
					</li><li class="listitem">
						You can use the <code class="literal">pcmk_host_list</code> parameter when creating a fencing device to specify all of the machines that are controlled by that fencing device.
					</li><li class="listitem">
						Some fence devices require a mapping of host names to the specifications that the fence device understands. You can map host names with the <code class="literal">pcmk_host_map</code> parameter when creating a fencing device.
					</li></ul></div><p>
				For information on the <code class="literal">pcmk_host_list</code> and <code class="literal">pcmk_host_map</code> parameters, see <a class="link" href="index.html#tb-fencedevice-props-HAAR" title="Table 9.1. General Properties of Fencing Devices">General Properties of Fencing Devices</a>.
			</p><p>
				After configuring a fence device, it is imperative that you test the device to ensure that it is working correctly. For information on testing a fence device, see <a class="link" href="index.html#proc_testing-fence-devices-configuring-fencing" title="9.5. Testing a fence device">Testing a fence device</a>.
			</p></section><section class="section" id="ref_general-fence-device-properties-configuring-fencing"><div class="titlepage"><div><div><h2 class="title">9.3. General properties of fencing devices</h2></div></div></div><p>
				Any cluster node can fence any other cluster node with any fence device, regardless of whether the fence resource is started or stopped. Whether the resource is started controls only the recurring monitor for the device, not whether it can be used, with the following exceptions:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You can disable a fencing device by running the <code class="literal command">pcs stonith disable <span class="emphasis"><em>stonith_id</em></span></code> command. This will prevent any node from using that device.
					</li><li class="listitem">
						To prevent a specific node from using a fencing device, you can configure location constraints for the fencing resource with the <code class="literal command">pcs constraint location …​ avoids</code> command.
					</li><li class="listitem">
						Configuring <code class="literal">stonith-enabled=false</code> will disable fencing altogether. Note, however, that Red Hat does not support clusters when fencing is disabled, as it is not suitable for a production environment.
					</li></ul></div><p>
				<a class="xref" href="index.html#tb-fencedevice-props-HAAR" title="Table 9.1. General Properties of Fencing Devices">Table 9.1, “General Properties of Fencing Devices”</a> describes the general properties you can set for fencing devices.
			</p><div class="table" id="tb-fencedevice-props-HAAR"><p class="title"><strong>Table 9.1. General Properties of Fencing Devices</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 27%; " class="col_1"><!--Empty--></col><col style="width: 18%; " class="col_2"><!--Empty--></col><col style="width: 27%; " class="col_3"><!--Empty--></col><col style="width: 27%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319469948720" scope="col">Field</th><th align="left" valign="top" id="idm140319469947632" scope="col">Type</th><th align="left" valign="top" id="idm140319469946544" scope="col">Default</th><th align="left" valign="top" id="idm140319469344832" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319469948720"> <p>
								<code class="literal">pcmk_host_map</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469947632"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140319469946544"> </td><td align="left" valign="top" headers="idm140319469344832"> <p>
								A mapping of host names to port numbers for devices that do not support host names. For example: <code class="literal">node1:1;node2:2,3</code> tells the cluster to use port 1 for node1 and ports 2 and 3 for node2
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469948720"> <p>
								<code class="literal">pcmk_host_list</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469947632"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140319469946544"> </td><td align="left" valign="top" headers="idm140319469344832"> <p>
								A list of machines controlled by this device (Optional unless <code class="literal">pcmk_host_check=static-list</code>).
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469948720"> <p>
								<code class="literal">pcmk_host_check</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469947632"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140319469946544"> <p>
								* <code class="literal">static-list</code> if either <code class="literal">pcmk_host_list</code> or <code class="literal">pcmk_host_map</code> is set
							</p>
							 <p>
								* Otherwise, <code class="literal">dynamic-list</code> if the fence device supports the <code class="literal">list</code> action
							</p>
							 <p>
								* Otherwise, <code class="literal">status</code> if the fence device supports the <code class="literal">status</code> action
							</p>
							 <p>
								*Otherwise, <code class="literal">none</code>.
							</p>
							 </td><td align="left" valign="top" headers="idm140319469344832"> <p>
								How to determine which machines are controlled by the device. Allowed values: <code class="literal">dynamic-list</code> (query the device), <code class="literal">static-list</code> (check the <code class="literal">pcmk_host_list</code> attribute), none (assume every device can fence every machine)
							</p>
							 </td></tr></tbody></table></div></div></section><section class="section" id="ref_advanced-fence-device-properties-configuring-fencing"><div class="titlepage"><div><div><h2 class="title">9.4. Advanced fencing configuration options</h2></div></div></div><p>
				<a class="xref" href="index.html#tb-fencepropsadvanced-HAAR" title="Table 9.2. Advanced Properties of Fencing Devices">Table 9.2, “Advanced Properties of Fencing Devices”</a> summarizes additional properties you can set for fencing devices. Note that these properties are for advanced use only.
			</p><div class="table" id="tb-fencepropsadvanced-HAAR"><p class="title"><strong>Table 9.2. Advanced Properties of Fencing Devices</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 29%; " class="col_1"><!--Empty--></col><col style="width: 14%; " class="col_2"><!--Empty--></col><col style="width: 14%; " class="col_3"><!--Empty--></col><col style="width: 43%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319469378160" scope="col">Field</th><th align="left" valign="top" id="idm140319469377072" scope="col">Type</th><th align="left" valign="top" id="idm140319469375984" scope="col">Default</th><th align="left" valign="top" id="idm140319469374896" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_host_argument</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								port
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								An alternate parameter to supply instead of port. Some devices do not support the standard port parameter or may provide additional ones. Use this to specify an alternate, device-specific parameter that should indicate the machine to be fenced. A value of <code class="literal">none</code> can be used to tell the cluster not to supply any additional parameters.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_reboot_action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								reboot
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								An alternate command to run instead of <code class="literal">reboot</code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the reboot action.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_reboot_timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								Specify an alternate timeout to use for reboot actions instead of <code class="literal">stonith-timeout</code>. Some devices need much more/less time to complete than normal. Use this to specify an alternate, device-specific, timeout for reboot actions.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_reboot_retries</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								The maximum number of times to retry the <code class="literal">reboot</code> command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries reboot actions before giving up.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_off_action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								off
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								An alternate command to run instead of <code class="literal">off</code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the off action.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_off_timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								Specify an alternate timeout to use for off actions instead of <code class="literal">stonith-timeout</code>. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for off actions.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_off_retries</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								The maximum number of times to retry the off command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries off actions before giving up.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_list_action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								list
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								An alternate command to run instead of <code class="literal">list</code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the list action.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_list_timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								Specify an alternate timeout to use for list actions. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for list actions.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_list_retries</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								The maximum number of times to retry the <code class="literal">list</code> command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries list actions before giving up.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_monitor_action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								monitor
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								An alternate command to run instead of <code class="literal">monitor</code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the monitor action.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_monitor_timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								Specify an alternate timeout to use for monitor actions instead of <code class="literal">stonith-timeout</code>. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for monitor actions.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_monitor_retries</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								The maximum number of times to retry the <code class="literal">monitor</code> command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries monitor actions before giving up.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_status_action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								status
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								An alternate command to run instead of <code class="literal">status</code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the status action.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_status_timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								Specify an alternate timeout to use for status actions instead of <code class="literal">stonith-timeout</code>. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for status actions.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_status_retries</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								The maximum number of times to retry the status command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries status actions before giving up.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_delay_base</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								0s
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								Enable a base delay for stonith actions and specify a base delay value. In a cluster with an even number of nodes, configuring a delay can help avoid nodes fencing each other at the same time in an even split. A random delay can be useful when the same fence device is used for all nodes, and differing static delays can be useful on each fencing device when a separate device is used for each node. The overall delay is derived from a random delay value adding this static delay so that the sum is kept below the maximum delay. If you set <code class="literal">pcmk_delay_base</code> but do not set <code class="literal">pcmk_delay_max</code>, there is no random component to the delay and it will be the value of <code class="literal">pcmk_delay_base</code>.
							</p>
							 <p>
								Some individual fence agents implement a "delay" parameter, which is independent of delays configured with a <code class="literal">pcmk_delay_*</code> property. If both of these delays are configured, they are added together and thus would generally not be used in conjunction.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_delay_max</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								0s
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								Enable a random delay for stonith actions and specify the maximum of random delay. In a cluster with an even number of nodes, configuring a delay can help avoid nodes fencing each other at the same time in an even split. A random delay can be useful when the same fence device is used for all nodes, and differing static delays can be useful on each fencing device when a separate device is used for each node. The overall delay is derived from this random delay value adding a static delay so that the sum is kept below the maximum delay. If you set <code class="literal">pcmk_delay_max</code> but do not set <code class="literal">pcmk_delay_base</code> there is no static component to the delay.
							</p>
							 <p>
								Some individual fence agents implement a "delay" parameter, which is independent of delays configured with a <code class="literal">pcmk_delay_*</code> property. If both of these delays are configured, they are added together and thus would generally not be used in conjunction.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_action_limit</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								1
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								The maximum number of actions that can be performed in parallel on this device. The cluster property <code class="literal">concurrent-fencing=true</code> needs to be configured first (this is the default value for RHEL 8.1 and later). A value of -1 is unlimited.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_on_action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								on
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								For advanced use only: An alternate command to run instead of <code class="literal">on</code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the <code class="literal">on</code> action.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_on_timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								For advanced use only: Specify an alternate timeout to use for <code class="literal">on</code> actions instead of <code class="literal">stonith-timeout</code>. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for <code class="literal">on</code> actions.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319469378160"> <p>
								<code class="literal">pcmk_on_retries</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319469377072"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140319469375984"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm140319469374896"> <p>
								For advanced use only: The maximum number of times to retry the <code class="literal">on</code> command within the timeout period. Some devices do not support multiple connections. Operations may <code class="literal">fail</code> if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries <code class="literal">on</code> actions before giving up.
							</p>
							 </td></tr></tbody></table></div></div><p>
				In addition to the properties you can set for individual fence devices, there are also cluster properties you can set that determine fencing behavior, as described in <a class="xref" href="index.html#tb-clusterfenceprops-HAAR" title="Table 9.3. Cluster properties that determine fencing behavior">Table 9.3, “Cluster properties that determine fencing behavior”</a>.
			</p><div class="table" id="tb-clusterfenceprops-HAAR"><p class="title"><strong>Table 9.3. Cluster properties that determine fencing behavior</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 29%; " class="col_1"><!--Empty--></col><col style="width: 29%; " class="col_2"><!--Empty--></col><col style="width: 43%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319470688272" scope="col">Option</th><th align="left" valign="top" id="idm140319470687184" scope="col">Default</th><th align="left" valign="top" id="idm140319470686096" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319470688272"> <p>
								<code class="literal">stonith-enabled</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319470687184"> <p>
								true
							</p>
							 </td><td align="left" valign="top" headers="idm140319470686096"> <p>
								Indicates that failed nodes and nodes with resources that cannot be stopped should be fenced. Protecting your data requires that you set this <code class="literal">true</code>.
							</p>
							 <p>
								If <code class="literal">true</code>, or unset, the cluster will refuse to start resources unless one or more STONITH resources have been configured also.
							</p>
							 <p>
								Red Hat only supports clusters with this value set to <code class="literal">true</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319470688272"> <p>
								<code class="literal">stonith-action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319470687184"> <p>
								reboot
							</p>
							 </td><td align="left" valign="top" headers="idm140319470686096"> <p>
								Action to send to STONITH device. Allowed values: <code class="literal">reboot</code>, <code class="literal">off</code>. The value <code class="literal">poweroff</code> is also allowed, but is only used for legacy devices.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319470688272"> <p>
								<code class="literal">stonith-timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319470687184"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140319470686096"> <p>
								How long to wait for a STONITH action to complete.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319470688272"> <p>
								<code class="literal">stonith-max-attempts</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319470687184"> <p>
								10
							</p>
							 </td><td align="left" valign="top" headers="idm140319470686096"> <p>
								How many times fencing can fail for a target before the cluster will no longer immediately re-attempt it.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319470688272"> <p>
								<code class="literal">stonith-watchdog-timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319470687184"> </td><td align="left" valign="top" headers="idm140319470686096"> <p>
								The maximum time to wait until a node can be assumed to have been killed by the hardware watchdog. It is recommended that this value be set to twice the value of the hardware watchdog timeout. This option is needed only if watchdog-based SBD is used for fencing.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319470688272"> <p>
								<code class="literal">concurrent-fencing</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319470687184"> <p>
								true (RHEL 8.1 and later)
							</p>
							 </td><td align="left" valign="top" headers="idm140319470686096"> <p>
								Allow fencing operations to be performed in parallel.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319470688272"> <p>
								<code class="literal">fence-reaction</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319470687184"> <p>
								stop
							</p>
							 </td><td align="left" valign="top" headers="idm140319470686096"> <p>
								(Red Hat Enterprise Linux 8.2 and later) Determines how a cluster node should react if notified of its own fencing. A cluster node may receive notification of its own fencing if fencing is misconfigured, or if fabric fencing is in use that does not cut cluster communication. Allowed values are <code class="literal">stop</code> to attempt to immediately stop Pacemaker and stay stopped, or <code class="literal">panic</code> to attempt to immediately reboot the local node, falling back to stop on failure.
							</p>
							 <p>
								Although the default value for this property is <code class="literal">stop</code>, the safest choice for this value is <code class="literal">panic</code>, which attempts to immediately reboot the local node. If you prefer the stop behavior, as is most likely to be the case in conjunction with fabric fencing, it is recommended that you set this explicitly.
							</p>
							 </td></tr></tbody></table></div></div><p>
				For information on setting cluster properties, see <a class="link" href="index.html#setting-cluster-properties-controlling-cluster-behavior" title="21.2. Setting and removing cluster properties">Setting and removing cluster properties</a>.
			</p></section><section class="section" id="proc_testing-fence-devices-configuring-fencing"><div class="titlepage"><div><div><h2 class="title">9.5. Testing a fence device</h2></div></div></div><p>
				Fencing is a fundamental part of the Red Hat Cluster infrastructure and it is therefore important to validate or test that fencing is working properly.
			</p><p>
				Use the following procedure to test a fence device.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Use ssh, telnet, HTTP, or whatever remote protocol is used to connect to the device to manually log in and test the fence device or see what output is given. For example, if you will be configuring fencing for an IPMI-enabled device, then try to log in remotely with <code class="literal command">ipmitool</code>. Take note of the options used when logging in manually because those options might be needed when using the fencing agent.
					</p><p class="simpara">
						If you are unable to log in to the fence device, verify that the device is pingable, there is nothing such as a firewall configuration that is preventing access to the fence device, remote access is enabled on the fencing device, and the credentials are correct.
					</p></li><li class="listitem"><p class="simpara">
						Run the fence agent manually, using the fence agent script. This does not require that the cluster services are running, so you can perform this step before the device is configured in the cluster. This can ensure that the fence device is responding properly before proceeding.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The examples in this section use the <code class="literal command">fence_ipmilan</code> fence agent script for an iLO device. The actual fence agent you will use and the command that calls that agent will depend on your server hardware. You should consult the man page for the fence agent you are using to determine which options to specify. You will usually need to know the login and password for the fence device and other information related to the fence device.
						</p></div></div><p class="simpara">
						The following example shows the format you would use to run the <code class="literal command">fence_ipmilan</code> fence agent script with <code class="literal">-o status</code> parameter to check the status of the fence device interface on another node without actually fencing it. This allows you to test the device and get it working before attempting to reboot the node. When running this command, you specify the name and password of an iLO user that has power on and off permissions for the iLO device.
					</p><pre class="literallayout"># <code class="literal">fence_ipmilan -a ipaddress -l username -p password -o status</code></pre><p class="simpara">
						The following example shows the format you would use to run the <code class="literal command">fence_ipmilan</code> fence agent script with the <code class="literal">-o reboot</code> parameter. Running this command on one node reboots the node managed by this iLO device.
					</p><pre class="literallayout"># <code class="literal">fence_ipmilan -a ipaddress -l username -p password -o reboot</code></pre><p class="simpara">
						If the fence agent failed to properly do a status, off, on, or reboot action, you should check the hardware, the configuration of the fence device, and the syntax of your commands. In addition, you can run the fence agent script with the debug output enabled. The debug output is useful for some fencing agents to see where in the sequence of events the fencing agent script is failing when logging into the fence device.
					</p><pre class="literallayout"># <code class="literal">fence_ipmilan -a ipaddress -l username -p password -o status -D /tmp/$(hostname)-fence_agent.debug</code></pre><p class="simpara">
						When diagnosing a failure that has occurred, you should ensure that the options you specified when manually logging in to the fence device are identical to what you passed on to the fence agent with the fence agent script.
					</p><p class="simpara">
						For fence agents that support an encrypted connection, you may see an error due to certificate validation failing, requiring that you trust the host or that you use the fence agent’s <code class="literal">ssl-insecure</code> parameter. Similarly, if SSL/TLS is disabled on the target device, you may need to account for this when setting the SSL parameters for the fence agent.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If the fence agent that is being tested is a <code class="literal command">fence_drac</code>, <code class="literal command">fence_ilo</code>, or some other fencing agent for a systems management device that continues to fail, then fall back to trying <code class="literal command">fence_ipmilan</code>. Most systems management cards support IPMI remote login and the only supported fencing agent is <code class="literal command">fence_ipmilan</code>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Once the fence device has been configured in the cluster with the same options that worked manually and the cluster has been started, test fencing with the <code class="literal command">pcs stonith fence</code> command from any node (or even multiple times from different nodes), as in the following example. The <code class="literal command">pcs stonith fence</code> command reads the cluster configuration from the CIB and calls the fence agent as configured to execute the fence action. This verifies that the cluster configuration is correct.
					</p><pre class="literallayout"># <code class="literal">pcs stonith fence node_name</code></pre><p class="simpara">
						If the <code class="literal command">pcs stonith fence</code> command works properly, that means the fencing configuration for the cluster should work when a fence event occurs. If the command fails, it means that cluster management cannot invoke the fence device through the configuration it has retrieved. Check for the following issues and update your cluster configuration as needed.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Check your fence configuration. For example, if you have used a host map you should ensure that the system can find the node using the host name you have provided.
							</li><li class="listitem">
								Check whether the password and user name for the device include any special characters that could be misinterpreted by the bash shell. Making sure that you enter passwords and user names surrounded by quotation marks could address this issue.
							</li><li class="listitem">
								Check whether you can connect to the device using the exact IP address or host name you specified in the <code class="literal command">pcs stonith</code> command. For example, if you give the host name in the stonith command but test by using the IP address, that is not a valid test.
							</li><li class="listitem"><p class="simpara">
								If the protocol that your fence device uses is accessible to you, use that protocol to try to connect to the device. For example many agents use ssh or telnet. You should try to connect to the device with the credentials you provided when configuring the device, to see if you get a valid prompt and can log in to the device.
							</p><p class="simpara">
								If you determine that all your parameters are appropriate but you still have trouble connecting to your fence device, you can check the logging on the fence device itself, if the device provides that, which will show if the user has connected and what command the user issued. You can also search through the <code class="literal">/var/log/messages</code> file for instances of stonith and error, which could give some idea of what is transpiring, but some agents can provide additional information.
							</p></li></ul></div></li><li class="listitem"><p class="simpara">
						Once the fence device tests are working and the cluster is up and running, test an actual failure. To do this, take an action in the cluster that should initiate a token loss.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Take down a network. How you take a network depends on your specific configuration. In many cases, you can physically pull the network or power cables out of the host. For information on simulating a network failure, see <a class="link" href="https://access.redhat.com/solutions/79523/">What is the proper way to simulate a network failure on a RHEL Cluster?</a>.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Disabling the network interface on the local host rather than physically disconnecting the network or power cables is not recommended as a test of fencing because it does not accurately simulate a typical real-world failure.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Block corosync traffic both inbound and outbound using the local firewall.
							</p><p class="simpara">
								The following example blocks corosync, assuming the default corosync port is used, <code class="literal">firewalld</code> is used as the local firewall, and the network interface used by corosync is in the default firewall zone:
							</p><pre class="literallayout"># <code class="literal">firewall-cmd --direct --add-rule ipv4 filter OUTPUT 2 -p udp --dport=5405 -j DROP</code>
# <code class="literal">firewall-cmd --add-rich-rule='rule family="ipv4" port port="5405" protocol="udp" drop'</code></pre></li><li class="listitem"><p class="simpara">
								Simulate a crash and panic your machine with <code class="literal">sysrq-trigger</code>. Note, however, that triggering a kernel panic can cause data loss; it is recommended that you disable your cluster resources first.
							</p><pre class="literallayout"># <code class="literal">echo c &gt; /proc/sysrq-trigger</code></pre></li></ul></div></li></ol></div></section><section class="section" id="proc_configuring-fencing-levels-configuring-fencing"><div class="titlepage"><div><div><h2 class="title">9.6. Configuring fencing levels</h2></div></div></div><p>
				Pacemaker supports fencing nodes with multiple devices through a feature called fencing topologies. To implement topologies, create the individual devices as you normally would and then define one or more fencing levels in the fencing topology section in the configuration.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Each level is attempted in ascending numeric order, starting at 1.
					</li><li class="listitem">
						If a device fails, processing terminates for the current level. No further devices in that level are exercised and the next level is attempted instead.
					</li><li class="listitem">
						If all devices are successfully fenced, then that level has succeeded and no other levels are tried.
					</li><li class="listitem">
						The operation is finished when a level has passed (success), or all levels have been attempted (failed).
					</li></ul></div><p>
				Use the following command to add a fencing level to a node. The devices are given as a comma-separated list of stonith ids, which are attempted for the node at that level.
			</p><pre class="literallayout">pcs stonith level add <span class="emphasis"><em>level</em></span> <span class="emphasis"><em>node</em></span> <span class="emphasis"><em>devices</em></span></pre><p>
				The following command lists all of the fencing levels that are currently configured.
			</p><pre class="literallayout">pcs stonith level</pre><p>
				In the following example, there are two fence devices configured for node <code class="literal">rh7-2</code>: an ilo fence device called <code class="literal">my_ilo</code> and an apc fence device called <code class="literal">my_apc</code>. These commands set up fence levels so that if the device <code class="literal">my_ilo</code> fails and is unable to fence the node, then Pacemaker will attempt to use the device <code class="literal">my_apc</code>. This example also shows the output of the <code class="literal">pcs stonith level</code> command after the levels are configured.
			</p><pre class="literallayout"># <code class="literal">pcs stonith level add 1 rh7-2 my_ilo</code>
# <code class="literal">pcs stonith level add 2 rh7-2 my_apc</code>
# <code class="literal">pcs stonith level</code>
 Node: rh7-2
  Level 1 - my_ilo
  Level 2 - my_apc</pre><p>
				The following command removes the fence level for the specified node and devices. If no nodes or devices are specified then the fence level you specify is removed from all nodes.
			</p><pre class="literallayout">pcs stonith level remove <span class="emphasis"><em>level</em></span> [<span class="emphasis"><em>node_id</em></span>] [<span class="emphasis"><em>stonith_id</em></span>] ... [<span class="emphasis"><em>stonith_id</em></span>]</pre><p>
				The following command clears the fence levels on the specified node or stonith id. If you do not specify a node or stonith id, all fence levels are cleared.
			</p><pre class="literallayout">pcs stonith level clear [<span class="emphasis"><em>node</em></span>|<span class="emphasis"><em>stonith_id</em></span>(s)]</pre><p>
				If you specify more than one stonith id, they must be separated by a comma and no spaces, as in the following example.
			</p><pre class="literallayout"># <code class="literal">pcs stonith level clear dev_a,dev_b</code></pre><p>
				The following command verifies that all fence devices and nodes specified in fence levels exist.
			</p><pre class="literallayout">pcs stonith level verify</pre><p>
				You can specify nodes in fencing topology by a regular expression applied on a node name and by a node attribute and its value. For example, the following commands configure nodes <code class="literal">node1</code>, <code class="literal">node2</code>, and <code class="literal">`node3</code> to use fence devices <code class="literal">apc1</code> and <code class="literal">`apc2</code>, and nodes <code class="literal">`node4</code>, <code class="literal">node5</code>, and <code class="literal">`node6</code> to use fence devices <code class="literal">apc3</code> and <code class="literal">`apc4</code>.
			</p><pre class="literallayout">pcs stonith level add 1 "regexp%node[1-3]" apc1,apc2
pcs stonith level add 1 "regexp%node[4-6]" apc3,apc4</pre><p>
				The following commands yield the same results by using node attribute matching.
			</p><pre class="literallayout">pcs node attribute node1 rack=1
pcs node attribute node2 rack=1
pcs node attribute node3 rack=1
pcs node attribute node4 rack=2
pcs node attribute node5 rack=2
pcs node attribute node6 rack=2
pcs stonith level add 1 attrib%rack=1 apc1,apc2
pcs stonith level add 1 attrib%rack=2 apc3,apc4</pre></section><section class="section" id="proc_configuring-fencing-for-redundant-power-configuring-fencing"><div class="titlepage"><div><div><h2 class="title">9.7. Configuring fencing for redundant power supplies</h2></div></div></div><p>
				When configuring fencing for redundant power supplies, the cluster must ensure that when attempting to reboot a host, both power supplies are turned off before either power supply is turned back on.
			</p><p>
				If the node never completely loses power, the node may not release its resources. This opens up the possibility of nodes accessing these resources simultaneously and corrupting them.
			</p><p>
				You need to define each device only once and to specify that both are required to fence the node, as in the following example.
			</p><pre class="literallayout"># <code class="literal">pcs stonith create apc1 fence_apc_snmp ipaddr=apc1.example.com login=user passwd='7a4D#1j!pz864' pcmk_host_map="node1.example.com:1;node2.example.com:2"</code>

# <code class="literal">pcs stonith create apc2 fence_apc_snmp ipaddr=apc2.example.com login=user passwd='7a4D#1j!pz864' pcmk_host_map="node1.example.com:1;node2.example.com:2"</code>

# <code class="literal">pcs stonith level add 1 node1.example.com apc1,apc2</code>
# <code class="literal">pcs stonith level add 1 node2.example.com apc1,apc2</code></pre></section><section class="section" id="proc_displaying-configuring-fence-devices-configuring-fencing"><div class="titlepage"><div><div><h2 class="title">9.8. Displaying configured fence devices</h2></div></div></div><p>
				The following command shows all currently configured fence devices. If a <span class="emphasis"><em>stonith_id</em></span> is specified, the command shows the options for that configured stonith device only. If the <code class="literal">--full</code> option is specified, all configured stonith options are displayed.
			</p><pre class="literallayout">pcs stonith config [<span class="emphasis"><em>stonith_id</em></span>] [--full]</pre></section><section class="section" id="proc_modifying-fence-devices-configuring-fencing"><div class="titlepage"><div><div><h2 class="title">9.9. Modifying and deleting fence devices</h2></div></div></div><p>
				Use the following command to modify or add options to a currently configured fencing device.
			</p><pre class="literallayout">pcs stonith update <span class="emphasis"><em>stonith_id</em></span> [<span class="emphasis"><em>stonith_device_options</em></span>]</pre><p>
				Use the following command to remove a fencing device from the current configuration.
			</p><pre class="literallayout">pcs stonith delete <span class="emphasis"><em>stonith_id</em></span></pre></section><section class="section" id="proc_manually-fencing-a-node-configuring-fencing"><div class="titlepage"><div><div><h2 class="title">9.10. Manually fencing a cluster node</h2></div></div></div><p>
				You can fence a node manually with the following command. If you specify <code class="literal option">--off</code> this will use the <code class="literal">off</code> API call to stonith which will turn the node off instead of rebooting it.
			</p><pre class="literallayout">pcs stonith fence <span class="emphasis"><em>node</em></span> [--off]</pre><p>
				In a situation where no stonith device is able to fence a node even if it is no longer active, the cluster may not be able to recover the resources on the node. If this occurs, after manually ensuring that the node is powered down you can enter the following command to confirm to the cluster that the node is powered down and free its resources for recovery.
			</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					If the node you specify is not actually off, but running the cluster software or services normally controlled by the cluster, data corruption/cluster failure will occur.
				</p></div></div><pre class="literallayout">pcs stonith confirm <span class="emphasis"><em>node</em></span></pre></section><section class="section" id="proc_disabling-a-fence-device-configuring-fencing"><div class="titlepage"><div><div><h2 class="title">9.11. Disabling a fence device</h2></div></div></div><p>
				To disable a fencing device/resource, you run the <code class="literal">pcs stonith disable</code> command.
			</p><p>
				The following command disables the fence device <code class="literal">myapc</code>.
			</p><pre class="literallayout"># <code class="literal">pcs stonith disable myapc</code></pre></section><section class="section" id="proc_preventing-a-node-from-using-a-fence-device-configuring-fencing"><div class="titlepage"><div><div><h2 class="title">9.12. Preventing a node from using a fence device</h2></div></div></div><p>
				To prevent a specific node from using a fencing device, you can configure location constraints for the fencing resource.
			</p><p>
				The following example prevents fence device <code class="literal">node1-ipmi</code> from running on <code class="literal">node1</code>.
			</p><pre class="literallayout"># <code class="literal">pcs constraint location node1-ipmi avoids node1</code></pre></section><section class="section" id="proc_configuring-acpi-for-fence-devices-configuring-fencing"><div class="titlepage"><div><div><h2 class="title">9.13. Configuring ACPI for use with integrated fence devices</h2></div></div></div><p>
				If your cluster uses integrated fence devices, you must configure ACPI (Advanced Configuration and Power Interface) to ensure immediate and complete fencing.
			</p><p>
				If a cluster node is configured to be fenced by an integrated fence device, disable ACPI Soft-Off for that node. Disabling ACPI Soft-Off allows an integrated fence device to turn off a node immediately and completely rather than attempting a clean shutdown (for example, <code class="literal command">shutdown -h now</code>). Otherwise, if ACPI Soft-Off is enabled, an integrated fence device can take four or more seconds to turn off a node (see the note that follows). In addition, if ACPI Soft-Off is enabled and a node panics or freezes during shutdown, an integrated fence device may not be able to turn off the node. Under those circumstances, fencing is delayed or unsuccessful. Consequently, when a node is fenced with an integrated fence device and ACPI Soft-Off is enabled, a cluster recovers slowly or requires administrative intervention to recover.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The amount of time required to fence a node depends on the integrated fence device used. Some integrated fence devices perform the equivalent of pressing and holding the power button; therefore, the fence device turns off the node in four to five seconds. Other integrated fence devices perform the equivalent of pressing the power button momentarily, relying on the operating system to turn off the node; therefore, the fence device turns off the node in a time span much longer than four to five seconds.
				</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The preferred way to disable ACPI Soft-Off is to change the BIOS setting to "instant-off" or an equivalent setting that turns off the node without delay, as described in <a class="xref" href="index.html#s2-bios-setting-CA" title="9.13.1. Disabling ACPI Soft-Off with the BIOS">Section 9.13.1, “Disabling ACPI Soft-Off with the BIOS”</a>.
					</li></ul></div><p>
				Disabling ACPI Soft-Off with the BIOS may not be possible with some systems. If disabling ACPI Soft-Off with the BIOS is not satisfactory for your cluster, you can disable ACPI Soft-Off with one of the following alternate methods:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Setting <code class="literal">HandlePowerKey=ignore</code> in the <code class="literal">/etc/systemd/logind.conf</code> file and verifying that the node node turns off immediately when fenced, as described in <a class="xref" href="index.html#s2-acpi-disable-logind-CA" title="9.13.2. Disabling ACPI Soft-Off in the logind.conf file">Section 9.13.2, “Disabling ACPI Soft-Off in the logind.conf file”</a>. This is the first alternate method of disabling ACPI Soft-Off.
					</li><li class="listitem"><p class="simpara">
						Appending <code class="literal">acpi=off</code> to the kernel boot command line, as described in <a class="xref" href="index.html#s2-acpi-disable-boot-CA" title="9.13.3. Disabling ACPI completely in the GRUB 2 File">Section 9.13.3, “Disabling ACPI completely in the GRUB 2 File”</a>. This is the second alternate method of disabling ACPI Soft-Off, if the preferred or the first alternate method is not available.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							This method completely disables ACPI; some computers do not boot correctly if ACPI is completely disabled. Use this method <span class="emphasis"><em>only</em></span> if the other methods are not effective for your cluster.
						</p></div></div></li></ul></div><section class="section" id="s2-bios-setting-CA"><div class="titlepage"><div><div><h3 class="title">9.13.1. Disabling ACPI Soft-Off with the BIOS</h3></div></div></div><p>
					You can disable ACPI Soft-Off by configuring the BIOS of each cluster node with the following procedure.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The procedure for disabling ACPI Soft-Off with the BIOS may differ among server systems. You should verify this procedure with your hardware documentation.
					</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Reboot the node and start the <code class="literal command">BIOS CMOS Setup Utility</code> program.
						</li><li class="listitem">
							Navigate to the Power menu (or equivalent power management menu).
						</li><li class="listitem"><p class="simpara">
							At the Power menu, set the <code class="literal">Soft-Off by PWR-BTTN</code> function (or equivalent) to <code class="literal">Instant-Off</code> (or the equivalent setting that turns off the node by means of the power button without delay). <a class="xref" href="index.html#ex-bios-acpi-off-CA" title="BIOS CMOS Setup Utility:"><code class="literal command">BIOS CMOS Setup Utility</code>:</a> shows a Power menu with <code class="literal">ACPI Function</code> set to <code class="literal">Enabled</code> and <code class="literal">Soft-Off by PWR-BTTN</code> set to <code class="literal">Instant-Off</code>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The equivalents to <code class="literal">ACPI Function</code>, <code class="literal">Soft-Off by PWR-BTTN</code>, and <code class="literal">Instant-Off</code> may vary among computers. However, the objective of this procedure is to configure the BIOS so that the computer is turned off by means of the power button without delay.
							</p></div></div></li><li class="listitem">
							Exit the <code class="literal command">BIOS CMOS Setup Utility</code> program, saving the BIOS configuration.
						</li><li class="listitem">
							Verify that the node turns off immediately when fenced. For information on testing a fence device, see <a class="link" href="index.html#proc_testing-fence-devices-configuring-fencing" title="9.5. Testing a fence device">Testing a fence device</a>.
						</li></ol></div><div id="ex-bios-acpi-off-CA" class="formalpara"><p class="title"><strong><code class="literal command">BIOS CMOS Setup Utility</code>:</strong></p><p>
						
<pre class="literallayout">`Soft-Off by PWR-BTTN` set to
`Instant-Off`</pre>

					</p></div><div class="informalexample"><pre class="literallayout">+---------------------------------------------|-------------------+
|    ACPI Function             [Enabled]      |    Item Help      |
|    ACPI Suspend Type         [S1(POS)]      |-------------------|
|  x Run VGABIOS if S3 Resume   Auto          |   Menu Level   *  |
|    Suspend Mode              [Disabled]     |                   |
|    HDD Power Down            [Disabled]     |                   |
|    Soft-Off by PWR-BTTN      [Instant-Off   |                   |
|    CPU THRM-Throttling       [50.0%]        |                   |
|    Wake-Up by PCI card       [Enabled]      |                   |
|    Power On by Ring          [Enabled]      |                   |
|    Wake Up On LAN            [Enabled]      |                   |
|  x USB KB Wake-Up From S3     Disabled      |                   |
|    Resume by Alarm           [Disabled]     |                   |
|  x  Date(of Month) Alarm       0            |                   |
|  x  Time(hh:mm:ss) Alarm       0 :  0 :     |                   |
|    POWER ON Function         [BUTTON ONLY   |                   |
|  x KB Power ON Password       Enter         |                   |
|  x Hot Key Power ON           Ctrl-F1       |                   |
|                                             |                   |
|                                             |                   |
+---------------------------------------------|-------------------+</pre><p>
					This example shows <code class="literal">ACPI Function</code> set to <code class="literal">Enabled</code>, and <code class="literal">Soft-Off by PWR-BTTN</code> set to <code class="literal">Instant-Off</code>.
				</p></div></section><section class="section" id="s2-acpi-disable-logind-CA"><div class="titlepage"><div><div><h3 class="title">9.13.2. Disabling ACPI Soft-Off in the logind.conf file</h3></div></div></div><p>
					To disable power-key handing in the <code class="literal">/etc/systemd/logind.conf</code> file, use the following procedure.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Define the following configuration in the <code class="literal">/etc/systemd/logind.conf</code> file:
						</p><pre class="literallayout">HandlePowerKey=ignore</pre></li><li class="listitem"><p class="simpara">
							Reload the <code class="literal">systemd</code> configuration:
						</p><pre class="literallayout"># <code class="literal">systemctl daemon-reload</code></pre></li><li class="listitem">
							Verify that the node turns off immediately when fenced. For information on testing a fence device, see <a class="link" href="index.html#proc_testing-fence-devices-configuring-fencing" title="9.5. Testing a fence device">Testing a fence device</a>.
						</li></ol></div></section><section class="section" id="s2-acpi-disable-boot-CA"><div class="titlepage"><div><div><h3 class="title">9.13.3. Disabling ACPI completely in the GRUB 2 File</h3></div></div></div><p>
					You can disable ACPI Soft-Off by appending <code class="literal">acpi=off</code> to the GRUB menu entry for a kernel.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						This method completely disables ACPI; some computers do not boot correctly if ACPI is completely disabled. Use this method <span class="emphasis"><em>only</em></span> if the other methods are not effective for your cluster.
					</p></div></div><p>
					Use the following procedure to disable ACPI in the GRUB 2 file:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Use the <code class="literal option">--args</code> option in combination with the <code class="literal option">--update-kernel</code> option of the <code class="literal command">grubby</code> tool to change the <code class="literal">grub.cfg</code> file of each cluster node as follows:
						</p><pre class="literallayout"># <code class="literal">grubby --args=acpi=off --update-kernel=ALL</code></pre></li><li class="listitem">
							Reboot the node.
						</li><li class="listitem">
							Verify that the node turns off immediately when fenced. For information on testing a fence device, see <a class="link" href="index.html#proc_testing-fence-devices-configuring-fencing" title="9.5. Testing a fence device">Testing a fence device</a>.
						</li></ol></div></section></section></section><section class="chapter" id="assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 10. Configuring cluster resources</h1></div></div></div><p>
			The format for the command to create a cluster resource is as follows:
		</p><pre class="literallayout">pcs resource create <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>standard</em></span>:[<span class="emphasis"><em>provider</em></span>:]]<span class="emphasis"><em>type</em></span> [<span class="emphasis"><em>resource_options</em></span>] [op <span class="emphasis"><em>operation_action operation_options</em></span> [<span class="emphasis"><em>operation_action</em></span> <span class="emphasis"><em>operation options</em></span>]...] [meta <span class="emphasis"><em>meta_options</em></span>...] [clone [<span class="emphasis"><em>clone_options</em></span>] | master [<span class="emphasis"><em>master_options</em></span>] | --group <span class="emphasis"><em>group_name</em></span> [--before <span class="emphasis"><em>resource_id</em></span> | --after <span class="emphasis"><em>resource_id</em></span>] | [bundle <span class="emphasis"><em>bundle_id</em></span>] [--disabled] [--wait[=<span class="emphasis"><em>n</em></span>]]</pre><p>
			Key cluster resource creation options include the following:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					When you specify the <code class="literal option">--group</code> option, the resource is added to the resource group named. If the group does not exist, this creates the group and adds this resource to the group.
				</li></ul></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					The <code class="literal option">--before</code> and <code class="literal option">--after</code> options specify the position of the added resource relative to a resource that already exists in a resource group.
				</li><li class="listitem">
					Specifying the <code class="literal option">--disabled</code> option indicates that the resource is not started automatically.
				</li></ul></div><p>
			You can determine the behavior of a resource in a cluster by configuring constraints for that resource.
		</p><h3 id="resource_creation_examples">Resource creation examples</h3><p>
			The following command creates a resource with the name <code class="literal">VirtualIP</code> of standard <code class="literal">ocf</code>, provider <code class="literal">heartbeat</code>, and type <code class="literal">IPaddr2</code>. The floating address of this resource is 192.168.0.120, and the system will check whether the resource is running every 30 seconds.
		</p><pre class="literallayout"># <code class="literal">pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.120 cidr_netmask=24 op monitor interval=30s</code></pre><p>
			Alternately, you can omit the <span class="emphasis"><em>standard</em></span> and <span class="emphasis"><em>provider</em></span> fields and use the following command. This will default to a standard of <code class="literal">ocf</code> and a provider of <code class="literal">heartbeat</code>.
		</p><pre class="literallayout"># <code class="literal">pcs resource create VirtualIP IPaddr2 ip=192.168.0.120 cidr_netmask=24 op monitor interval=30s</code></pre><h3 id="deleting_a_configured_resource">Deleting a configured resource</h3><p>
			Use the following command to delete a configured resource.
		</p><pre class="literallayout">pcs resource delete <span class="emphasis"><em>resource_id</em></span></pre><p>
			For example, the following command deletes an existing resource with a resource ID of <code class="literal">VirtualIP</code>.
		</p><pre class="literallayout"># <code class="literal">pcs resource delete VirtualIP</code></pre><section class="section" id="ref_resource-properties.adoc-configuring-cluster-resources"><div class="titlepage"><div><div><h2 class="title">10.1. Resource agent identifiers</h2></div></div></div><p>
				The identifiers that you define for a resource tell the cluster which agent to use for the resource, where to find that agent and what standards it conforms to. <a class="xref" href="index.html#tb-resource-props-summary-HAAR" title="Table 10.1. Resource Agent Identifiers">Table 10.1, “Resource Agent Identifiers”</a>, describes these properties.
			</p><div class="table" id="tb-resource-props-summary-HAAR"><p class="title"><strong>Table 10.1. Resource Agent Identifiers</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 67%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319460611120" scope="col">Field</th><th align="left" valign="top" id="idm140319460610032" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319460611120"> <p>
								standard
							</p>
							 </td><td align="left" valign="top" headers="idm140319460610032"> <p>
								The standard the agent conforms to. Allowed values and their meaning:
							</p>
							 <p>
								* <code class="literal">ocf</code> - The specified <span class="emphasis"><em>type</em></span> is the name of an executable file conforming to the Open Cluster Framework Resource Agent API and located beneath <code class="literal">/usr/lib/ocf/resource.d/<span class="emphasis"><em>provider</em></span></code>
							</p>
							 <p>
								* <code class="literal">lsb</code> - The specified <span class="emphasis"><em>type</em></span> is the name of an executable file conforming to Linux Standard Base Init Script Actions. If the type does not specify a full path, the system will look for it in the <code class="literal">/etc/init.d</code> directory.
							</p>
							 <p>
								* <code class="literal">systemd</code> - The specified <span class="emphasis"><em>type</em></span> is the name of an installed <code class="literal">systemd</code> unit
							</p>
							 <p>
								* <code class="literal">service</code> - Pacemaker will search for the specified <span class="emphasis"><em>type</em></span>, first as an <code class="literal">lsb</code> agent, then as a <code class="literal">systemd</code> agent
							</p>
							 <p>
								* <code class="literal">nagios</code> - The specified <span class="emphasis"><em>type</em></span> is the name of an executable file conforming to the Nagios Plugin API and located in the <code class="literal">/usr/libexec/nagios/plugins</code> directory, with OCF-style metadata stored separately in the <code class="literal">/usr/share/nagios/plugins-metadata</code> directory (available in the <code class="literal">nagios-agents-metadata</code> package for certain common plugins).
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460611120"> <p>
								type
							</p>
							 </td><td align="left" valign="top" headers="idm140319460610032"> <p>
								The name of the resource agent you wish to use, for example <code class="literal">IPaddr</code> or <code class="literal">Filesystem</code>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460611120"> <p>
								provider
							</p>
							 </td><td align="left" valign="top" headers="idm140319460610032"> <p>
								The OCF spec allows multiple vendors to supply the same resource agent. Most of the agents shipped by Red Hat use <code class="literal">heartbeat</code> as the provider.
							</p>
							 </td></tr></tbody></table></div></div><p>
				<a class="xref" href="index.html#tb-resource-displayopts-HAAR" title="Table 10.2. Commands to Display Resource Properties">Table 10.2, “Commands to Display Resource Properties”</a> summarizes the commands that display the available resource properties.
			</p><div class="table" id="tb-resource-displayopts-HAAR"><p class="title"><strong>Table 10.2. Commands to Display Resource Properties</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319457930096" scope="col">pcs Display Command</th><th align="left" valign="top" id="idm140319457929008" scope="col">Output</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319457930096"> <p>
								<code class="literal command">pcs resource list</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319457929008"> <p>
								Displays a list of all available resources.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319457930096"> <p>
								<code class="literal command">pcs resource standards</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319457929008"> <p>
								Displays a list of available resource agent standards.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319457930096"> <p>
								<code class="literal command">pcs resource providers</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319457929008"> <p>
								Displays a list of available resource agent providers.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319457930096"> <p>
								<code class="literal command">pcs resource list <span class="emphasis"><em>string</em></span></code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319457929008"> <p>
								Displays a list of available resources filtered by the specified string. You can use this command to display resources filtered by the name of a standard, a provider, or a type.
							</p>
							 </td></tr></tbody></table></div></div></section><section class="section" id="proc_displaying-resource-specific-parameters-configuring-cluster-resources"><div class="titlepage"><div><div><h2 class="title">10.2. Displaying resource-specific parameters</h2></div></div></div><p>
				For any individual resource, you can use the following command to display a description of the resource, the parameters you can set for that resource, and the default values that are set for the resource.
			</p><pre class="literallayout">pcs resource describe [<span class="emphasis"><em>standard</em></span>:[<span class="emphasis"><em>provider</em></span>:]]<span class="emphasis"><em>type</em></span></pre><p>
				For example, the following command displays information for a resource of type <code class="literal">apache</code>.
			</p><pre class="literallayout"># <code class="literal">pcs resource describe ocf:heartbeat:apache</code>
This is the resource agent for the Apache Web server.
This resource agent operates both version 1.x and version 2.x Apache
servers.

...</pre></section><section class="section" id="proc_configuring-resource-meta-options-configuring-cluster-resources"><div class="titlepage"><div><div><h2 class="title">10.3. Configuring resource meta options</h2></div></div></div><p>
				In addition to the resource-specific parameters, you can configure additional resource options for any resource. These options are used by the cluster to decide how your resource should behave.
			</p><p>
				<a class="xref" href="index.html#tb-resource-options-HAAR" title="Table 10.3. Resource Meta Options">Table 10.3, “Resource Meta Options”</a> describes the resource meta options.
			</p><div class="table" id="tb-resource-options-HAAR"><p class="title"><strong>Table 10.3. Resource Meta Options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 38%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 38%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319459223312" scope="col">Field</th><th align="left" valign="top" id="idm140319459222224" scope="col">Default</th><th align="left" valign="top" id="idm140319459221136" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319459223312"> <p>
								<code class="literal">priority</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459222224"> <p>
								<code class="literal">0</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459221136"> <p>
								If not all resources can be active, the cluster will stop lower priority resources in order to keep higher priority ones active.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319459223312"> <p>
								<code class="literal">target-role</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459222224"> <p>
								<code class="literal">Started</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459221136"> <p>
								What state should the cluster attempt to keep this resource in? Allowed values:
							</p>
							 <p>
								* <span class="emphasis"><em>Stopped</em></span> - Force the resource to be stopped
							</p>
							 <p>
								* <span class="emphasis"><em>Started</em></span> - Allow the resource to be started (and in the case of promotable clones, promoted to master role if appropriate)
							</p>
							 <p>
								* <span class="emphasis"><em>Master</em></span> - Allow the resource to be started and, if appropriate, promoted
							</p>
							 <p>
								* <span class="emphasis"><em>Slave</em></span> - Allow the resource to be started, but only in Slave mode if the resource is promotable
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319459223312"> <p>
								<code class="literal">is-managed</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459222224"> <p>
								<code class="literal">true</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459221136"> <p>
								Is the cluster allowed to start and stop the resource? Allowed values: <code class="literal">true</code>, <code class="literal">false</code>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319459223312"> <p>
								<code class="literal">resource-stickiness</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459222224"> <p>
								0
							</p>
							 </td><td align="left" valign="top" headers="idm140319459221136"> <p>
								Value to indicate how much the resource prefers to stay where it is.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319459223312"> <p>
								<code class="literal">requires</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459222224"> <p>
								Calculated
							</p>
							 </td><td align="left" valign="top" headers="idm140319459221136"> <p>
								Indicates under what conditions the resource can be started.
							</p>
							 <p>
								Defaults to <code class="literal">fencing</code> except under the conditions noted below. Possible values:
							</p>
							 <p>
								* <code class="literal">nothing</code> - The cluster can always start the resource.
							</p>
							 <p>
								* <code class="literal">quorum</code> - The cluster can only start this resource if a majority of the configured nodes are active. This is the default value if <code class="literal">stonith-enabled</code> is <code class="literal">false</code> or the resource’s <code class="literal">standard</code> is <code class="literal">stonith</code>.
							</p>
							 <p>
								* <code class="literal">fencing</code> - The cluster can only start this resource if a majority of the configured nodes are active <span class="emphasis"><em>and</em></span> any failed or unknown nodes have been fenced.
							</p>
							 <p>
								* <code class="literal">unfencing</code> - The cluster can only start this resource if a majority of the configured nodes are active <span class="emphasis"><em>and</em></span> any failed or unknown nodes have been fenced <span class="emphasis"><em>and</em></span> only on nodes that have been <span class="emphasis"><em>unfenced</em></span>. This is the default value if the <code class="literal">provides=unfencing</code> <code class="literal">stonith</code> meta option has been set for a fencing device.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319459223312"> <p>
								<code class="literal">migration-threshold</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459222224"> <p>
								<code class="literal">INFINITY</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459221136"> <p>
								How many failures may occur for this resource on a node, before this node is marked ineligible to host this resource. A value of 0 indicates that this feature is disabled (the node will never be marked ineligible); by contrast, the cluster treats <code class="literal">INFINITY</code> (the default) as a very large but finite number. This option has an effect only if the failed operation has <code class="literal">on-fail=restart</code> (the default), and additionally for failed start operations if the cluster property <code class="literal">start-failure-is-fatal</code> is <code class="literal">false</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319459223312"> <p>
								<code class="literal">failure-timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459222224"> <p>
								<code class="literal">0</code> (disabled)
							</p>
							 </td><td align="left" valign="top" headers="idm140319459221136"> <p>
								Used in conjunction with the <code class="literal">migration-threshold</code> option, indicates how many seconds to wait before acting as if the failure had not occurred, and potentially allowing the resource back to the node on which it failed. As with any time-based actions, this is not guaranteed to be checked more frequently than the value of the <code class="literal">cluster-recheck-interval</code> cluster parameter.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319459223312"> <p>
								<code class="literal">multiple-active</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459222224"> <p>
								<code class="literal">stop_start</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459221136"> <p>
								What should the cluster do if it ever finds the resource active on more than one node. Allowed values:
							</p>
							 <p>
								* <code class="literal">block</code> - mark the resource as unmanaged
							</p>
							 <p>
								* <code class="literal">stop_only</code> - stop all active instances and leave them that way
							</p>
							 <p>
								* <code class="literal">stop_start</code> - stop all active instances and start the resource in one location only
							</p>
							 </td></tr></tbody></table></div></div><section class="section" id="changing_the_default_value_of_a_resource_option"><div class="titlepage"><div><div><h3 class="title">10.3.1. Changing the default value of a resource option</h3></div></div></div><p>
					To change the default value of a resource option, use the following command.
				</p><pre class="literallayout">pcs resource defaults <span class="emphasis"><em>options</em></span></pre><p>
					For example, the following command resets the default value of <code class="literal">resource-stickiness</code> to 100.
				</p><pre class="literallayout"># <code class="literal">pcs resource defaults resource-stickiness=100</code></pre></section><section class="section" id="displaying_currently_configured_resource_defaults"><div class="titlepage"><div><div><h3 class="title">10.3.2. Displaying currently configured resource defaults</h3></div></div></div><p>
					Omitting the <span class="emphasis"><em>options</em></span> parameter from the <code class="literal command">pcs resource defaults</code> displays a list of currently configured default values for resource options. The following example shows the output of this command after you have reset the default value of <code class="literal">resource-stickiness</code> to 100.
				</p><pre class="literallayout"># <code class="literal">pcs resource defaults</code>
resource-stickiness: 100</pre></section><section class="section" id="setting_meta_options_on_resource_creation"><div class="titlepage"><div><div><h3 class="title">10.3.3. Setting meta options on resource creation</h3></div></div></div><p>
					Whether you have reset the default value of a resource meta option or not, you can set a resource option for a particular resource to a value other than the default when you create the resource. The following shows the format of the <code class="literal">pcs resource create</code> command you use when specifying a value for a resource meta option.
				</p><pre class="literallayout">pcs resource create <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>standard</em></span>:[<span class="emphasis"><em>provider</em></span>:]]<span class="emphasis"><em>type</em></span> [<span class="emphasis"><em>resource options</em></span>] [meta <span class="emphasis"><em>meta_options</em></span>...]</pre><p>
					For example, the following command creates a resource with a <code class="literal">resource-stickiness</code> value of 50.
				</p><pre class="literallayout"># <code class="literal">pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.120 meta resource-stickiness=50</code></pre><p>
					You can also set the value of a resource meta option for an existing resource, group, cloned resource, or master resource with the following command.
				</p><pre class="literallayout">pcs resource meta <span class="emphasis"><em>resource_id</em></span> | <span class="emphasis"><em>group_id</em></span> | <span class="emphasis"><em>clone_id</em></span> <span class="emphasis"><em>meta_options</em></span></pre><p>
					In the following example, there is an existing resource named <code class="literal">dummy_resource</code>. This command sets the <code class="literal">failure-timeout</code> meta option to 20 seconds, so that the resource can attempt to restart on the same node in 20 seconds.
				</p><pre class="literallayout"># <code class="literal">pcs resource meta dummy_resource failure-timeout=20s</code></pre><p>
					After executing this command, you can display the values for the resource to verify that <code class="literal">failure-timeout=20s</code> is set.
				</p><pre class="literallayout"># <code class="literal">pcs resource config dummy_resource</code>
 Resource: dummy_resource (class=ocf provider=heartbeat type=Dummy)
  Meta Attrs: failure-timeout=20s
  ...</pre></section></section><section class="section" id="assembly_resource-groups-configuring-cluster-resources"><div class="titlepage"><div><div><h2 class="title">10.4. Configuring resource groups</h2></div></div></div><p>
				One of the most common elements of a cluster is a set of resources that need to be located together, start sequentially, and stop in the reverse order. To simplify this configuration, Pacemaker supports the concept of resource groups.
			</p><section class="section" id="proc_creating-resource-groups-resourceegroups"><div class="titlepage"><div><div><h3 class="title">10.4.1. Creating a resource group</h3></div></div></div><p>
					You create a resource group with the following command, specifying the resources to include in the group. If the group does not exist, this command creates the group. If the group exists, this command adds additional resources to the group. The resources will start in the order you specify them with this command, and will stop in the reverse order of their starting order.
				</p><pre class="literallayout">pcs resource group add <span class="emphasis"><em>group_name</em></span> <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>resource_id</em></span>] ... [<span class="emphasis"><em>resource_id</em></span>] [--before <span class="emphasis"><em>resource_id</em></span> | --after <span class="emphasis"><em>resource_id</em></span>]</pre><p>
					You can use the <code class="literal option">--before</code> and <code class="literal option">--after</code> options of this command to specify the position of the added resources relative to a resource that already exists in the group.
				</p><p>
					You can also add a new resource to an existing group when you create the resource, using the following command. The resource you create is added to the group named <span class="emphasis"><em>group_name</em></span>. If the group <span class="emphasis"><em>group_name</em></span> does not exist, it will be created.
				</p><pre class="literallayout">pcs resource create <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>standard</em></span>:[<span class="emphasis"><em>provider</em></span>:]]<span class="emphasis"><em>type</em></span> [resource_options] [op <span class="emphasis"><em>operation_action</em></span> <span class="emphasis"><em>operation_options</em></span>] --group <span class="emphasis"><em>group_name</em></span></pre><p>
					There is no limit to the number of resources a group can contain. The fundamental properties of a group are as follows.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Resources are colocated within a group.
						</li><li class="listitem">
							Resources are started in the order in which you specify them. If a resource in the group cannot run anywhere, then no resource specified after that resource is allowed to run.
						</li><li class="listitem">
							Resources are stopped in the reverse order in which you specify them.
						</li></ul></div><p>
					The following example creates a resource group named <code class="literal">shortcut</code> that contains the existing resources <code class="literal">IPaddr</code> and <code class="literal">Email</code>.
				</p><pre class="literallayout"># <code class="literal">pcs resource group add shortcut IPaddr Email</code></pre><p>
					In this example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The <code class="literal">IPaddr</code> is started first, then <code class="literal">Email</code>.
						</li><li class="listitem">
							The <code class="literal">Email</code> resource is stopped first, then <code class="literal">IPAddr</code>.
						</li><li class="listitem">
							If <code class="literal">IPaddr</code> cannot run anywhere, neither can <code class="literal">Email</code>.
						</li><li class="listitem">
							If <code class="literal">Email</code> cannot run anywhere, however, this does not affect <code class="literal">IPaddr</code> in any way.
						</li></ul></div></section><section class="section" id="removing_a_resource_group"><div class="titlepage"><div><div><h3 class="title">10.4.2. Removing a resource group</h3></div></div></div><p>
					You remove a resource from a group with the following command. If there are no remaining resources in the group, this command removes the group itself.
				</p><pre class="literallayout">pcs resource group remove <span class="emphasis"><em>group_name</em></span> <span class="emphasis"><em>resource_id</em></span>...</pre></section><section class="section" id="displaying_resource_groups"><div class="titlepage"><div><div><h3 class="title">10.4.3. Displaying resource groups</h3></div></div></div><p>
					The following command lists all currently configured resource groups.
				</p><pre class="literallayout">pcs resource group list</pre></section><section class="section" id="s2-group_options-HAAR"><div class="titlepage"><div><div><h3 class="title">10.4.4. Group options</h3></div></div></div><p>
					You can set the following options for a resource group, and they maintain the same meaning as when they are set for a single resource: <code class="literal">priority</code>, <code class="literal">target-role</code>, <code class="literal">is-managed</code>. For information on resource meta options, see <a class="link" href="index.html#proc_configuring-resource-meta-options-configuring-cluster-resources" title="10.3. Configuring resource meta options">Configuring resource meta options</a>.
				</p></section><section class="section" id="s2-group_stickiness-HAAR"><div class="titlepage"><div><div><h3 class="title">10.4.5. Group stickiness</h3></div></div></div><p>
					Stickiness, the measure of how much a resource wants to stay where it is, is additive in groups. Every active resource of the group will contribute its stickiness value to the group’s total. So if the default <code class="literal">resource-stickiness</code> is 100, and a group has seven members, five of which are active, then the group as a whole will prefer its current location with a score of 500.
				</p></section></section><section class="section" id="con_determining-resource-behavior-configuring-cluster-resources"><div class="titlepage"><div><div><h2 class="title">10.5. Determining resource behavior</h2></div></div></div><p>
				You can determine the behavior of a resource in a cluster by configuring constraints for that resource. You can configure the following categories of constraints:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">location</code> constraints — A location constraint determines which nodes a resource can run on. For information on configuring location constraints, see <a class="link" href="index.html#assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters" title="Chapter 11. Determining which nodes a resource can run on">Determining which nodes a resource can run on</a>.
					</li><li class="listitem">
						<code class="literal">order</code> constraints — An ordering constraint determines the order in which the resources run. For information on configuring ordering constraints, see <a class="link" href="index.html#assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters" title="Chapter 12. Determining the order in which cluster resources are run">Determining the order in which cluster resources are run</a>.
					</li><li class="listitem">
						<code class="literal">colocation</code> constraints — A colocation constraint determines where resources will be placed relative to other resources. For information on colocation constraints, see <a class="link" href="index.html#assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters" title="Chapter 13. Colocating cluster resources">Colocating cluster resources</a>.
					</li></ul></div><p>
				As a shorthand for configuring a set of constraints that will locate a set of resources together and ensure that the resources start sequentially and stop in reverse order, Pacemaker supports the concept of resource groups. After you have created a resource group, you can configure constraints on the group itself just as you configure constraints for individual resources. For information on resource groups, see <a class="link" href="index.html#assembly_resource-groups-configuring-cluster-resources" title="10.4. Configuring resource groups">Configuring resource groups</a>.
			</p></section></section><section class="chapter" id="assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 11. Determining which nodes a resource can run on</h1></div></div></div><p>
			Location constraints determine which nodes a resource can run on. You can configure location constraints to determine whether a resource will prefer or avoid a specified node.
		</p><section class="section" id="proc_configuring-location-constraints-determining-which-node-a-resource-runs-on"><div class="titlepage"><div><div><h2 class="title">11.1. Configuring location constraints</h2></div></div></div><p>
				You can configure a basic location constraint to specify whether a resource prefers or avoids a node, with an optional <code class="literal">score</code> value to indicate the relative degree of preference for the constraint.
			</p><p>
				The following command creates a location constraint for a resource to prefer the specified node or nodes. Note that it is possible to create constraints on a particular resource for more than one node with a single command.
			</p><pre class="literallayout">pcs constraint location <span class="emphasis"><em>rsc</em></span> prefers <span class="emphasis"><em>node</em></span>[=<span class="emphasis"><em>score</em></span>] [<span class="emphasis"><em>node</em></span>[=<span class="emphasis"><em>score</em></span>]] ...</pre><p>
				The following command creates a location constraint for a resource to avoid the specified node or nodes.
			</p><pre class="literallayout">pcs constraint location <span class="emphasis"><em>rsc</em></span> avoids <span class="emphasis"><em>node</em></span>[=<span class="emphasis"><em>score</em></span>] [<span class="emphasis"><em>node</em></span>[=<span class="emphasis"><em>score</em></span>]] ...</pre><p>
				<a class="xref" href="index.html#tb-locationconstraint-options-HAAR-determining-which-node-a-resource-runs-on" title="Table 11.1. Location Constraint Options">Table 11.1, “Location Constraint Options”</a> summarizes the meanings of the basic options for configuring location constraints.
			</p><div class="table" id="tb-locationconstraint-options-HAAR-determining-which-node-a-resource-runs-on"><p class="title"><strong>Table 11.1. Location Constraint Options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 67%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319458949200" scope="col">Field</th><th align="left" valign="top" id="idm140319458948112" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319458949200"> <p>
								<code class="literal">rsc</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458948112"> <p>
								A resource name
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458949200"> <p>
								<code class="literal">node</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458948112"> <p>
								A node’s name
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458949200"> <p>
								<code class="literal">score</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458948112"> <p>
								Positive integer value to indicate the degree of preference for whether the given resource should prefer or avoid the given node. <code class="literal">INFINITY</code> is the default <code class="literal">score</code> value for a resource location constraint.
							</p>
							 <p>
								A value of <code class="literal">INFINITY</code> for <span class="emphasis"><em>score</em></span> in a command that configures a resource to prefer a node indicates that the resource will prefer that node if the node is available, but does not prevent the resource from running on another node if the specified node is unavailable. A value of <code class="literal">INFINITY</code> in a command that configures a resource to avoid a node indicates that the resource will never run on that node, even if no other node is available.
							</p>
							 <p>
								A numeric score (that is, not <code class="literal">INFINITY</code>) means the constraint is optional, and will be honored unless some other factor outweighs it. For example, if the resource is already placed on a different node, and its <code class="literal">resource-stickiness</code> score is higher than a <code class="literal">prefers</code> location constraint’s score, then the resource will be left where it is.
							</p>
							 </td></tr></tbody></table></div></div><p>
				The following command creates a location constraint to specify that the resource <code class="literal">Webserver</code> prefers node <code class="literal">node1</code>.
			</p><pre class="literallayout">pcs constraint location Webserver prefers node1</pre><p>
				<code class="literal command">pcs</code> supports regular expressions in location constraints on the command line. These constraints apply to multiple resources based on the regular expression matching resource name. This allows you to configure multiple location contraints with a single command line.
			</p><p>
				The following command creates a location constraint to specify that resources <code class="literal">dummy0</code> to <code class="literal">dummy9</code> prefer <code class="literal">node1</code>.
			</p><pre class="literallayout">pcs constraint location 'regexp%dummy[0-9]' prefers node1</pre><p>
				Since Pacemaker uses POSIX extended regular expressions as documented at <a class="link" href="http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_04">http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_04</a>, you can specify the same constraint with the following command.
			</p><pre class="literallayout">pcs constraint location 'regexp%dummy[[:digit:]]' prefers node1</pre></section><section class="section" id="proc_limiting-resource-discovery-to-a-subset-of-nodes-determining-which-node-a-resource-runs-on"><div class="titlepage"><div><div><h2 class="title">11.2. Limiting resource discovery to a subset of nodes</h2></div></div></div><p>
				Before Pacemaker starts a resource anywhere, it first runs a one-time monitor operation (often referred to as a "probe") on every node, to learn whether the resource is already running. This process of resource discovery can result in errors on nodes that are unable to execute the monitor.
			</p><p>
				When configuring a location constraint on a node, you can use the <code class="literal option">resource-discovery</code> option of the <code class="literal command">pcs constraint location</code> command to indicate a preference for whether Pacemaker should perform resource discovery on this node for the specified resource. Limiting resource discovery to a subset of nodes the resource is physically capable of running on can significantly boost performance when a large set of nodes is present. When <code class="literal">pacemaker_remote</code> is in use to expand the node count into the hundreds of nodes range, this option should be considered.
			</p><p>
				The following command shows the format for specifying the <code class="literal option">resource-discovery</code> option of the <code class="literal command">pcs constraint location</code> command. In this command, a positive value for <span class="emphasis"><em>score</em></span> corresponds to a basic location constraint that configures a resource to prefer a node, while a negative value for <span class="emphasis"><em>score</em></span> corresponds to a basic location`constraint that configures a resource to avoid a node. As with basic location constraints, you can use regular expressions for resources with these constraints as well.
			</p><pre class="literallayout">pcs constraint location add <span class="emphasis"><em>id</em></span> <span class="emphasis"><em>rsc</em></span> <span class="emphasis"><em>node</em></span> <span class="emphasis"><em>score</em></span> [resource-discovery=<span class="emphasis"><em>option</em></span>]</pre><p>
				<a class="xref" href="index.html#tb-resourcediscoveryconstraint-options-HAAR-determining-which-node-a-resource-runs-on" title="Table 11.2. Resource Discovery Constraint Parameters">Table 11.2, “Resource Discovery Constraint Parameters”</a> summarizes the meanings of the basic parameters for configuring constraints for resource discovery.
			</p><div class="table" id="tb-resourcediscoveryconstraint-options-HAAR-determining-which-node-a-resource-runs-on"><p class="title"><strong>Table 11.2. Resource Discovery Constraint Parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 40%; " class="col_1"><!--Empty--></col><col style="width: 60%; " class="col_2"><!--Empty--></col></colgroup><tbody><tr><td align="left" valign="top"> <p>
								Field
							</p>
							 </td><td align="left" valign="top"> <p>
								Description
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								<code class="literal">id</code>
							</p>
							 </td><td align="left" valign="top"> <p>
								A user-chosen name for the constraint itself.
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								<code class="literal">rsc</code>
							</p>
							 </td><td align="left" valign="top"> <p>
								A resource name
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								<code class="literal">node</code>
							</p>
							 </td><td align="left" valign="top"> <p>
								A node’s name
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								<code class="literal">score</code>
							</p>
							 </td><td align="left" valign="top"> <p>
								Integer value to indicate the degree of preference for whether the given resource should prefer or avoid the given node. A positive value for score corresponds to a basic location constraint that configures a resource to prefer a node, while a negative value for score corresponds to a basic location constraint that configures a resource to avoid a node.
							</p>
							 <p>
								A value of <code class="literal">INFINITY</code> for <span class="emphasis"><em>score</em></span> indicates that the resource will prefer that node if the node is available, but does not prevent the resource from running on another node if the specified node is unavailable. A value of <code class="literal">-INFINITY</code> in a command that configures a resource to avoid a node indicates that the resource will never run on that node, even if no other node is available.
							</p>
							 <p>
								A numeric score (that is, not <code class="literal">INFINITY</code> or <code class="literal">-INFINITY</code>) means the constraint is optional, and will be honored unless some other factor outweighs it. For example, if the resource is already placed on a different node, and its <code class="literal">resource-stickiness</code> score is higher than a <code class="literal">prefers</code> location constraint’s score, then the resource will be left where it is.
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								<code class="literal">resource-discovery</code> options
							</p>
							 </td><td align="left" valign="top"> <p>
								* <code class="literal">always</code> - Always perform resource discovery for the specified resource on this node. This is the default <code class="literal">resource-discovery</code> value for a resource location constraint.
							</p>
							 <p>
								* <code class="literal">never</code> - Never perform resource discovery for the specified resource on this node.
							</p>
							 <p>
								* <code class="literal">exclusive</code> - Perform resource discovery for the specified resource only on this node (and other nodes similarly marked as <code class="literal">exclusive</code>). Multiple location constraints using <code class="literal">exclusive</code> discovery for the same resource across different nodes creates a subset of nodes <code class="literal">resource-discovery</code> is exclusive to. If a resource is marked for <code class="literal">exclusive</code> discovery on one or more nodes, that resource is only allowed to be placed within that subset of nodes.
							</p>
							 </td></tr></tbody></table></div></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					Setting <code class="literal">resource-discovery</code> to <code class="literal">never</code> or <code class="literal">exclusive</code> removes Pacemaker’s ability to detect and stop unwanted instances of a service running where it is not supposed to be. It is up to the system administrator to make sure that the service can never be active on nodes without resource discovery (such as by leaving the relevant software uninstalled).
				</p></div></div></section><section class="section" id="proc_configuring-location-constraint-strategy.adoc-determining-which-node-a-resource-runs-on"><div class="titlepage"><div><div><h2 class="title">11.3. Configuring a location constraint strategy</h2></div></div></div><p>
				When using location constraints, you can configure a general strategy for specifying which nodes a resource can run on:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Opt-In Clusters — Configure a cluster in which, by default, no resource can run anywhere and then selectively enable allowed nodes for specific resources.
					</li><li class="listitem">
						Opt-Out Clusters — Configure a cluster in which, by default, all resources can run anywhere and then create location constraints for resources that are not allowed to run on specific nodes.
					</li></ul></div><p>
				Whether you should choose to configure your cluster as an opt-in or opt-out cluster depends on both your personal preference and the make-up of your cluster. If most of your resources can run on most of the nodes, then an opt-out arrangement is likely to result in a simpler configuration. On the other hand, if most resources can only run on a small subset of nodes an opt-in configuration might be simpler.
			</p><section class="section" id="s3-optin-clusters-HAAR"><div class="titlepage"><div><div><h3 class="title">11.3.1. Configuring an "Opt-In" Cluster</h3></div></div></div><p>
					To create an opt-in cluster, set the <code class="literal">symmetric-cluster</code> cluster property to <code class="literal">false</code> to prevent resources from running anywhere by default.
				</p><pre class="literallayout"># <code class="literal">pcs property set symmetric-cluster=false</code></pre><p>
					Enable nodes for individual resources. The following commands configure location constraints so that the resource <code class="literal">Webserver</code> prefers node <code class="literal">example-1</code>, the resource <code class="literal">Database</code> prefers node <code class="literal">example-2</code>, and both resources can fail over to node <code class="literal">example-3</code> if their preferred node fails. When configuring location constraints for an opt-in cluster, setting a score of zero allows a resource to run on a node without indicating any preference to prefer or avoid the node.
				</p><pre class="literallayout"># <code class="literal">pcs constraint location Webserver prefers example-1=200</code>
# <code class="literal">pcs constraint location Webserver prefers example-3=0</code>
# <code class="literal">pcs constraint location Database prefers example-2=200</code>
# <code class="literal">pcs constraint location Database prefers example-3=0</code></pre></section><section class="section" id="s3-optout-clusters-HAAR"><div class="titlepage"><div><div><h3 class="title">11.3.2. Configuring an "Opt-Out" Cluster</h3></div></div></div><p>
					To create an opt-out cluster, set the <code class="literal">symmetric-cluster</code> cluster property to <code class="literal">true</code> to allow resources to run everywhere by default. This is the default configuration if <code class="literal">symmetric-cluster</code> is not set explicitly.
				</p><pre class="literallayout"># <code class="literal">pcs property set symmetric-cluster=true</code></pre><p>
					The following commands will then yield a configuration that is equivalent to the example in <a class="xref" href="index.html#s3-optin-clusters-HAAR" title="11.3.1. Configuring an &quot;Opt-In&quot; Cluster">Section 11.3.1, “Configuring an "Opt-In" Cluster”</a>. Both resources can fail over to node <code class="literal">example-3</code> if their preferred node fails, since every node has an implicit score of 0.
				</p><pre class="literallayout"># <code class="literal">pcs constraint location Webserver prefers example-1=200</code>
# <code class="literal">pcs constraint location Webserver avoids example-2=INFINITY</code>
# <code class="literal">pcs constraint location Database avoids example-1=INFINITY</code>
# <code class="literal">pcs constraint location Database prefers example-2=200</code></pre><p>
					Note that it is not necessary to specify a score of INFINITY in these commands, since that is the default value for the score.
				</p></section></section></section><section class="chapter" id="assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 12. Determining the order in which cluster resources are run</h1></div></div></div><p>
			To determine the order in which the resources run, you configure an ordering constraint.
		</p><p>
			The following shows the format for the command to configure an ordering constraint.
		</p><pre class="literallayout">pcs constraint order [<span class="emphasis"><em>action</em></span>] <span class="emphasis"><em>resource_id</em></span> then [<span class="emphasis"><em>action</em></span>] <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>options</em></span>]</pre><p>
			<a class="xref" href="index.html#tb-orderconstraint-options-HAAR" title="Table 12.1. Properties of an Order Constraint">Table 12.1, “Properties of an Order Constraint”</a>, summarizes the properties and options for configuring ordering constraints.
		</p><div class="table" id="tb-orderconstraint-options-HAAR"><p class="title"><strong>Table 12.1. Properties of an Order Constraint</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 40%; " class="col_1"><!--Empty--></col><col style="width: 60%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319459891648" scope="col">Field</th><th align="left" valign="top" id="idm140319459890560" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319459891648"> <p>
							resource_id
						</p>
						 </td><td align="left" valign="top" headers="idm140319459890560"> <p>
							The name of a resource on which an action is performed.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140319459891648"> <p>
							action
						</p>
						 </td><td align="left" valign="top" headers="idm140319459890560"> <p>
							The action to perform on a resource. Possible values of the <span class="emphasis"><em>action</em></span> property are as follows:
						</p>
						 <p>
							* <code class="literal">start</code> - Start the resource.
						</p>
						 <p>
							* <code class="literal">stop</code> - Stop the resource.
						</p>
						 <p>
							* <code class="literal">promote</code> - Promote the resource from a slave resource to a master resource.
						</p>
						 <p>
							* <code class="literal">demote</code> - Demote the resource from a master resource to a slave resource.
						</p>
						 <p>
							If no action is specified, the default action is <code class="literal">start</code>.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140319459891648"> <p>
							<code class="literal">kind</code> option
						</p>
						 </td><td align="left" valign="top" headers="idm140319459890560"> <p>
							How to enforce the constraint. The possible values of the <code class="literal">kind</code> option are as follows:
						</p>
						 <p>
							* <code class="literal">Optional</code> - Only applies if both resources are executing the specified action. For information on optional ordering, see <a class="link" href="index.html#proc_configuring-advisory-ordering.adoc-determining-resource-order" title="12.2. Configuring advisory ordering">Configuring advisory ordering</a>.
						</p>
						 <p>
							* <code class="literal">Mandatory</code> - Always (default value). If the first resource you specified is stopping or cannot be started, the second resource you specified must be stopped. For information on mandatory ordering, see <a class="link" href="index.html#proc_configuring-mandatory-ordering.adoc-determining-resource-order" title="12.1. Configuring mandatory ordering">Configuring mandatory ordering</a>.
						</p>
						 <p>
							* <code class="literal">Serialize</code> - Ensure that no two stop/start actions occur concurrently for the resources you specify. The first and second resource you specify can start in either order, but one must complete starting before the other can be started. A typical use case is when resource startup puts a high load on the host.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140319459891648"> <p>
							<code class="literal">symmetrical</code> option
						</p>
						 </td><td align="left" valign="top" headers="idm140319459890560"> <p>
							If true, the reverse of the constraint applies for the opposite action (for example, if B starts after A starts, then B stops before Ordering constraints for which <code class="literal">kind</code> is <code class="literal">Serialize</code> cannot be symmetrical. The default value is <code class="literal">true</code> for <code class="literal">Mandatory</code> and <code class="literal">Ordering</code> kinds, <code class="literal">false</code> for <code class="literal">Serialize</code>.
						</p>
						 </td></tr></tbody></table></div></div><p>
			Use the following command to remove resources from any ordering constraint.
		</p><pre class="literallayout">pcs constraint order remove <span class="emphasis"><em>resource1</em></span> [<span class="emphasis"><em>resourceN</em></span>]...</pre><section class="section" id="proc_configuring-mandatory-ordering.adoc-determining-resource-order"><div class="titlepage"><div><div><h2 class="title">12.1. Configuring mandatory ordering</h2></div></div></div><p>
				A mandatory ordering constraint indicates that the second action should not be initiated for the second resource unless and until the first action successfully completes for the first resource. Actions that may be ordered are <code class="literal">stop</code>, <code class="literal">start</code>, and additionally for promotable clones, <code class="literal">demote</code> and <code class="literal">promote</code>. For example, "A then B" (which is equivalent to "start A then start B") means that B will not be started unless and until A successfully starts. An ordering constraint is mandatory if the <code class="literal">kind</code> option for the constraint is set to <code class="literal">Mandatory</code> or left as default.
			</p><p>
				If the <code class="literal">symmetrical</code> option is set to <code class="literal">true</code> or left to default, the opposite actions will be ordered in reverse. The <code class="literal">start</code> and <code class="literal">stop</code> actions are opposites, and <code class="literal">demote</code> and <code class="literal">promote</code> are opposites. For example, a symmetrical "promote A then start B" ordering implies "stop B then demote A", which means that A cannot be demoted until and unless B successfully stops. A symmetrical ordering means that changes in A’s state can cause actions to be scheduled for B. For example, given "A then B", if A restarts due to failure, B will be stopped first, then A will be stopped, then A will be started, then B will be started.
			</p><p>
				Note that the cluster reacts to each state change. If the first resource is restarted and is in a started state again before the second resource initiated a stop operation, the second resource will not need to be restarted.
			</p></section><section class="section" id="proc_configuring-advisory-ordering.adoc-determining-resource-order"><div class="titlepage"><div><div><h2 class="title">12.2. Configuring advisory ordering</h2></div></div></div><p>
				When the <code class="literal">kind=Optional</code> option is specified for an ordering constraint, the constraint is considered optional and only applies if both resources are executing the specified actions. Any change in state by the first resource you specify will have no effect on the second resource you specify.
			</p><p>
				The following command configures an advisory ordering constraint for the resources named <code class="literal">VirtualIP</code> and <code class="literal">dummy_resource</code>.
			</p><pre class="literallayout"># <code class="literal">pcs constraint order VirtualIP then dummy_resource kind=Optional</code></pre></section><section class="section" id="proc_configuring-ordered-resource-sets.adocdetermining-resource-order"><div class="titlepage"><div><div><h2 class="title">12.3. Configuring ordered resource sets</h2></div></div></div><p>
				A common situation is for an administrator to create a chain of ordered resources, where, for example, resource A starts before resource B which starts before resource C. If your configuration requires that you create a set of resources that is colocated and started in order, you can configure a resource group that contains those resources, as described in <a class="link" href="index.html#assembly_resource-groups-configuring-cluster-resources" title="10.4. Configuring resource groups">Configuring resource groups</a>.
			</p><p>
				There are some situations, however, where configuring the resources that need to start in a specified order as a resource group is not appropriate:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You may need to configure resources to start in order and the resources are not necessarily colocated.
					</li><li class="listitem">
						You may have a resource C that must start after either resource A or B has started but there is no relationship between A and B.
					</li><li class="listitem">
						You may have resources C and D that must start after both resources A and B have started, but there is no relationship between A and B or between C and D.
					</li></ul></div><p>
				In these situations, you can create an ordering constraint on a set or sets of resources with the <code class="literal command">pcs constraint order set</code> command.
			</p><p>
				You can set the following options for a set of resources with the <code class="literal command">pcs constraint order set</code> command.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						<code class="literal">sequential</code>, which can be set to <code class="literal">true</code> or <code class="literal">false</code> to indicate whether the set of resources must be ordered relative to each other. The default value is <code class="literal">true</code>.
					</p><p class="simpara">
						Setting <code class="literal">sequential</code> to <code class="literal">false</code> allows a set to be ordered relative to other sets in the ordering constraint, without its members being ordered relative to each other. Therefore, this option makes sense only if multiple sets are listed in the constraint; otherwise, the constraint has no effect.
					</p></li><li class="listitem">
						<code class="literal">require-all</code>, which can be set to <code class="literal">true</code> or <code class="literal">false</code> to indicate whether all of the resources in the set must be active before continuing. Setting <code class="literal">require-all</code> to <code class="literal">false</code> means that only one resource in the set needs to be started before continuing on to the next set. Setting <code class="literal">require-all</code> to <code class="literal">false</code> has no effect unless used in conjunction with unordered sets, which are sets for which <code class="literal">sequential</code> is set to <code class="literal">false</code>. The default value is <code class="literal">true</code>.
					</li><li class="listitem">
						<code class="literal">action</code>, which can be set to <code class="literal">start</code>, <code class="literal">promote</code>, <code class="literal">demote</code> or <code class="literal">stop</code>, as described in <a class="link" href="index.html#tb-orderconstraint-options-HAAR" title="Table 12.1. Properties of an Order Constraint">Properties of an Order Constraint</a>.
					</li><li class="listitem">
						<code class="literal">role</code>, which can be set to <code class="literal">Stopped</code>, <code class="literal">Started</code>, <code class="literal">Master</code>, or <code class="literal">Slave</code>.
					</li></ul></div><p>
				You can set the following constraint options for a set of resources following the <code class="literal">setoptions</code> parameter of the <code class="literal command">pcs constraint order set</code> command.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">id</code>, to provide a name for the constraint you are defining.
					</li><li class="listitem">
						<code class="literal">kind</code>, which indicates how to enforce the constraint, as described in <a class="link" href="index.html#tb-orderconstraint-options-HAAR" title="Table 12.1. Properties of an Order Constraint">Properties of an Order Constraint</a>.
					</li><li class="listitem">
						<code class="literal">symmetrical</code>, to set whether the reverse of the constraint applies for the opposite action, as described in <a class="link" href="index.html#tb-orderconstraint-options-HAAR" title="Table 12.1. Properties of an Order Constraint">Properties of an Order Constraint</a>.
					</li></ul></div><pre class="literallayout">pcs constraint order set <span class="emphasis"><em>resource1 resource2</em></span> [<span class="emphasis"><em>resourceN</em></span>]... [<span class="emphasis"><em>options</em></span>] [set <span class="emphasis"><em>resourceX</em></span> <span class="emphasis"><em>resourceY</em></span> ... [<span class="emphasis"><em>options</em></span>]] [setoptions [<span class="emphasis"><em>constraint_options</em></span>]]</pre><p>
				If you have three resources named <code class="literal">D1</code>, <code class="literal">D2</code>, and <code class="literal">D3</code>, the following command configures them as an ordered resource set.
			</p><pre class="literallayout"># <code class="literal">pcs constraint order set D1 D2 D3</code></pre><p>
				If you have six resources named <code class="literal">A</code>, <code class="literal">B</code>, <code class="literal">C</code>, <code class="literal">D</code>, <code class="literal">E</code>, and <code class="literal">F</code>, this example configures an ordering constraint for the set of resources that will start as follows:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">A</code> and <code class="literal">B</code> start independently of each other
					</li><li class="listitem">
						<code class="literal">C</code> starts once either <code class="literal">A</code> or <code class="literal">B</code> has started
					</li><li class="listitem">
						<code class="literal">D</code> starts once <code class="literal">C</code> has started
					</li><li class="listitem">
						<code class="literal">E</code> and <code class="literal">F</code> start independently of each other once <code class="literal">D</code> has started
					</li></ul></div><p>
				Stopping the resources is not influenced by this constraint since <code class="literal">symmetrical=false</code> is set.
			</p><pre class="literallayout"># <code class="literal">pcs constraint order set A B sequential=false require-all=false set C D set E F sequential=false setoptions symmetrical=false</code></pre></section><section class="section" id="proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order"><div class="titlepage"><div><div><h2 class="title">12.4. Configuring startup order for resource dependencies not managed by Pacemaker</h2></div></div></div><p>
				It is possible for a cluster to include resources with dependencies that are not themselves managed by the cluster. In this case, you must ensure that those dependencies are started before Pacemaker is started and stopped after Pacemaker is stopped.
			</p><p>
				You can configure your startup order to account for this situation by means of the <code class="literal">systemd</code> <code class="literal">resource-agents-deps</code> target. You can create a <code class="literal">systemd</code> drop-in unit for this target and Pacemaker will order itself appropriately relative to this target.
			</p><p>
				For example, if a cluster includes a resource that depends on the external service <code class="literal">foo</code> that is not managed by the cluster, perform the following procedure.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create the drop-in unit <code class="literal">/etc/systemd/system/resource-agents-deps.target.d/foo.conf</code> that contains the following:
					</p><pre class="literallayout">[Unit]
Requires=foo.service
After=foo.service</pre></li><li class="listitem">
						Run the <code class="literal command">systemctl daemon-reload</code> command.
					</li></ol></div><p>
				A cluster dependency specified in this way can be something other than a service. For example, you may have a dependency on mounting a file system at <code class="literal">/srv</code>, in which case you would perform the following procedure:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Ensure that <code class="literal">/srv</code> is listed in the <code class="literal">/etc/fstab</code> file. This will be converted automatically to the <code class="literal">systemd</code> file <code class="literal">srv.mount</code> at boot when the configuration of the system manager is reloaded. For more information, see the <code class="literal">systemd.mount</code>(5) and the <code class="literal">systemd-fstab-generator</code>(8) man pages.
					</li><li class="listitem"><p class="simpara">
						To make sure that Pacemaker starts after the disk is mounted, create the drop-in unit <code class="literal">/etc/systemd/system/resource-agents-deps.target.d/srv.conf</code> that contains the following.
					</p><pre class="literallayout">[Unit]
Requires=srv.mount
After=srv.mount</pre></li><li class="listitem">
						Run the <code class="literal command">systemctl daemon-reload</code> command.
					</li></ol></div></section></section><section class="chapter" id="assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 13. Colocating cluster resources</h1></div></div></div><p>
			To specify that the location of one resource depends on the location of another resource, you configure a colocation constraint.
		</p><p>
			There is an important side effect of creating a colocation constraint between two resources: it affects the order in which resources are assigned to a node. This is because you cannot place resource A relative to resource B unless you know where resource B is. So when you are creating colocation constraints, it is important to consider whether you should colocate resource A with resource B or resource B with resource A.
		</p><p>
			Another thing to keep in mind when creating colocation constraints is that, assuming resource A is colocated with resource B, the cluster will also take into account resource A’s preferences when deciding which node to choose for resource B.
		</p><p>
			The following command creates a colocation constraint.
		</p><pre class="literallayout">pcs constraint colocation add [master|slave] <span class="emphasis"><em>source_resource</em></span> with [master|slave] <span class="emphasis"><em>target_resource</em></span> [<span class="emphasis"><em>score</em></span>] [<span class="emphasis"><em>options</em></span>]</pre><p>
			<a class="xref" href="index.html#tb-colocationconstraint-options-HAAR" title="Table 13.1. Properties of a Colocation Constraint">Table 13.1, “Properties of a Colocation Constraint”</a>, summarizes the properties and options for configuring colocation constraints.
		</p><div class="table" id="tb-colocationconstraint-options-HAAR"><p class="title"><strong>Table 13.1. Properties of a Colocation Constraint</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 40%; " class="col_1"><!--Empty--></col><col style="width: 60%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319462100560" scope="col">Field</th><th align="left" valign="top" id="idm140319462099472" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319462100560"> <p>
							source_resource
						</p>
						 </td><td align="left" valign="top" headers="idm140319462099472"> <p>
							The colocation source. If the constraint cannot be satisfied, the cluster may decide not to allow the resource to run at all.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140319462100560"> <p>
							target_resource
						</p>
						 </td><td align="left" valign="top" headers="idm140319462099472"> <p>
							The colocation target. The cluster will decide where to put this resource first and then decide where to put the source resource.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140319462100560"> <p>
							score
						</p>
						 </td><td align="left" valign="top" headers="idm140319462099472"> <p>
							Positive values indicate the resource should run on the same node. Negative values indicate the resources should not run on the same node. A value of +<code class="literal">INFINITY</code>, the default value, indicates that the <span class="emphasis"><em>source_resource</em></span> must run on the same node as the <span class="emphasis"><em>target_resource</em></span>. A value of -<code class="literal">INFINITY</code> indicates that the <span class="emphasis"><em>source_resource</em></span> must not run on the same node as the <span class="emphasis"><em>target_resource</em></span>.
						</p>
						 </td></tr></tbody></table></div></div><section class="section" id="proc_specifying-mandatory-placement.adoc-colocating-cluster-resources"><div class="titlepage"><div><div><h2 class="title">13.1. Specifying mandatory placement of resources</h2></div></div></div><p>
				Mandatory placement occurs any time the constraint’s score is <code class="literal">+INFINITY</code> or <code class="literal">-INFINITY</code>. In such cases, if the constraint cannot be satisfied, then the <span class="emphasis"><em>source_resource</em></span> is not permitted to run. For <code class="literal">score=INFINITY</code>, this includes cases where the <span class="emphasis"><em>target_resource</em></span> is not active.
			</p><p>
				If you need <code class="literal">myresource1</code> to always run on the same machine as <code class="literal">myresource2</code>, you would add the following constraint:
			</p><pre class="literallayout"># <code class="literal">pcs constraint colocation add myresource1 with myresource2 score=INFINITY</code></pre><p>
				Because <code class="literal">INFINITY</code> was used, if <code class="literal">myresource2</code> cannot run on any of the cluster nodes (for whatever reason) then <code class="literal">myresource1</code> will not be allowed to run.
			</p><p>
				Alternatively, you may want to configure the opposite, a cluster in which <code class="literal">myresource1</code> cannot run on the same machine as <code class="literal">myresource2</code>. In this case use <code class="literal">score=-INFINITY</code>
			</p><pre class="literallayout"># <code class="literal">pcs constraint colocation add myresource1 with myresource2 score=-INFINITY</code></pre><p>
				Again, by specifying <code class="literal">-INFINITY</code>, the constraint is binding. So if the only place left to run is where <code class="literal">myresource2</code> already is, then <code class="literal">myresource1</code> may not run anywhere.
			</p></section><section class="section" id="proc_specifying-advisory-placement.adoc-colocating-cluster-resources"><div class="titlepage"><div><div><h2 class="title">13.2. Specifying advisory placement of resources</h2></div></div></div><p>
				If mandatory placement is about "must" and "must not", then advisory placement is the "I would prefer if" alternative. For constraints with scores greater than <code class="literal">-INFINITY</code> and less than <code class="literal">INFINITY</code>, the cluster will try to accommodate your wishes but may ignore them if the alternative is to stop some of the cluster resources.
			</p></section><section class="section" id="proc_colocating-resource-sets.adoc-colocating-cluster-resources"><div class="titlepage"><div><div><h2 class="title">13.3. Colocating sets of resources</h2></div></div></div><p>
				If your configuration requires that you create a set of resources that are colocated and started in order, you can configure a resource group that contains those resources, as described in <a class="link" href="index.html#assembly_resource-groups-configuring-cluster-resources" title="10.4. Configuring resource groups">Configuring resource groups</a>. There are some situations, however, where configuring the resources that need to be colocated as a resource group is not appropriate:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You may need to colocate a set of resources but the resources do not necessarily need to start in order.
					</li><li class="listitem">
						You may have a resource C that must be colocated with either resource A or B, but there is no relationship between A and B.
					</li><li class="listitem">
						You may have resources C and D that must be colocated with both resources A and B, but there is no relationship between A and B or between C and D.
					</li></ul></div><p>
				In these situations, you can create a colocation constraint on a set or sets of resources with the <code class="literal command">pcs constraint colocation set</code> command.
			</p><p>
				You can set the following options for a set of resources with the <code class="literal command">pcs constraint colocation set</code> command.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						<code class="literal">sequential</code>, which can be set to <code class="literal">true</code> or <code class="literal">false</code> to indicate whether the members of the set must be colocated with each other.
					</p><p class="simpara">
						Setting <code class="literal">sequential</code> to <code class="literal">false</code> allows the members of this set to be colocated with another set listed later in the constraint, regardless of which members of this set are active. Therefore, this option makes sense only if another set is listed after this one in the constraint; otherwise, the constraint has no effect.
					</p></li><li class="listitem">
						<code class="literal">role</code>, which can be set to <code class="literal">Stopped</code>, <code class="literal">Started</code>, <code class="literal">Master</code>, or <code class="literal">Slave</code>.
					</li></ul></div><p>
				You can set the following constraint option for a set of resources following the <code class="literal">setoptions</code> parameter of the <code class="literal command">pcs constraint colocation set</code> command.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">id</code>, to provide a name for the constraint you are defining.
					</li><li class="listitem">
						<code class="literal">score</code>, to indicate the degree of preference for this constraint. For information on this option, see <a class="link" href="index.html#tb-locationconstraint-options-HAAR-determining-which-node-a-resource-runs-on" title="Table 11.1. Location Constraint Options">Location Constraint Options</a>.
					</li></ul></div><p>
				When listing members of a set, each member is colocated with the one before it. For example, "set A B" means "B is colocated with A". However, when listing multiple sets, each set is colocated with the one after it. For example, "set C D sequential=false set A B" means "set C D (where C and D have no relation between each other) is colocated with set A B (where B is colocated with A)".
			</p><p>
				The following command creates a colocation constraint on a set or sets of resources.
			</p><pre class="literallayout">pcs constraint colocation set <span class="emphasis"><em>resource1 resource2</em></span> [<span class="emphasis"><em>resourceN</em></span>]... [<span class="emphasis"><em>options</em></span>] [set <span class="emphasis"><em>resourceX</em></span> <span class="emphasis"><em>resourceY</em></span> ... [<span class="emphasis"><em>options</em></span>]] [setoptions [<span class="emphasis"><em>constraint_options</em></span>]]</pre></section><section class="section" id="removing_colocation_constraints"><div class="titlepage"><div><div><h2 class="title">13.4. Removing Colocation Constraints</h2></div></div></div><p>
				Use the following command to remove colocation constraints with <span class="emphasis"><em>source_resource</em></span>.
			</p><pre class="literallayout">pcs constraint colocation remove <span class="emphasis"><em>source_resource</em></span> <span class="emphasis"><em>target_resource</em></span></pre></section></section><section class="chapter" id="assembly_displaying-resource-constraints.adoc-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 14. Displaying resource constraints</h1></div></div></div><p>
			There are a several commands you can use to display constraints that have been configured.
		</p><section class="section" id="proc_displaying-resource-constraints.adoc-displaying-resource-constraints"><div class="titlepage"><div><div><h2 class="title">14.1. Displaying all configured constraints</h2></div></div></div><p>
				The following command lists all current location, order, and colocation constraints. If the <code class="literal">--full</code> option is specified, show the internal constraint IDs.
			</p><pre class="literallayout">pcs constraint [list|show] [--full]</pre><p>
				As of RHEL 8.2, listing resource constraints no longer by default displays expired constraints. To include expired constaints, use the <code class="literal">--all</code> option of the <code class="literal">pcs constraint</code> command. This will list expired constraints, noting the constraints and their associated rules as <code class="literal">(expired)</code> in the display.
			</p></section><section class="section" id="displaying_location_constraints"><div class="titlepage"><div><div><h2 class="title">14.2. Displaying location constraints</h2></div></div></div><p>
				The following command lists all current location constraints.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						If <code class="literal">resources</code> is specified, location constraints are displayed per resource. This is the default behavior.
					</li><li class="listitem">
						If <code class="literal">nodes</code> is specified, location constraints are displayed per node.
					</li><li class="listitem">
						If specific resources or nodes are specified, then only information about those resources or nodes is displayed.
					</li></ul></div><pre class="literallayout">pcs constraint location [show [resources [<span class="emphasis"><em>resource</em></span>...]] | [nodes [<span class="emphasis"><em>node</em></span>...]]] [--full]</pre></section><section class="section" id="displaying_ordering_constraints"><div class="titlepage"><div><div><h2 class="title">14.3. Displaying ordering constraints</h2></div></div></div><p>
				The following command lists all current ordering constraints.
			</p><pre class="literallayout">pcs constraint order [show]</pre></section><section class="section" id="displaying_colocation_constraints"><div class="titlepage"><div><div><h2 class="title">14.4. Displaying colocation constraints</h2></div></div></div><p>
				The following command lists all current colocation constraints.
			</p><pre class="literallayout">pcs constraint colocation [show]</pre></section><section class="section" id="displaying_resource_specific_constraints"><div class="titlepage"><div><div><h2 class="title">14.5. Displaying resource-specific constraints</h2></div></div></div><p>
				The following command lists the constraints that reference specific resources.
			</p><pre class="literallayout">pcs constraint ref <span class="emphasis"><em>resource</em></span> ...</pre></section><section class="section" id="displaying_resource_dependencies_red_hat_enterprise_linux_8_2_and_later"><div class="titlepage"><div><div><h2 class="title">14.6. Displaying resource dependencies (Red Hat Enterprise Linux 8.2 and later)</h2></div></div></div><p>
				The following command displays the relations between cluster resources in a tree structure.
			</p><pre class="literallayout">pcs resource relations <span class="emphasis"><em>resource</em></span> [--full]</pre><p>
				If the <code class="literal">--full</code> option is used, the command displays additional information, including the constraint IDs and the resource types.
			</p><p>
				In the following example, there are 3 configured resources: C, D, and E.
			</p><pre class="literallayout"># <code class="literal">pcs constraint order start C then start D</code>
Adding C D (kind: Mandatory) (Options: first-action=start then-action=start)
# <code class="literal">pcs constraint order start D then start E</code>
Adding D E (kind: Mandatory) (Options: first-action=start then-action=start)

# <code class="literal">pcs resource relations C</code>
C
`- order
   |  start C then start D
   `- D
      `- order
         |  start D then start E
         `- E
# <code class="literal">pcs resource relations D</code>
D
|- order
|  |  start C then start D
|  `- C
`- order
   |  start D then start E
   `- E
# pcs <code class="literal">resource relations E</code>
E
`- order
   |  start D then start E
   `- D
      `- order
         |  start C then start D
         `- C</pre><p>
				In the following example, there are 2 configured resources: A and B. Resources A and B are part of resource group G.
			</p><pre class="literallayout"># <code class="literal">pcs resource relations A</code>
A
`- outer resource
   `- G
      `- inner resource(s)
         |  members: A B
         `- B
# <code class="literal">pcs resource relations B</code>
B
`- outer resource
   `- G
      `- inner resource(s)
         |  members: A B
         `- A
# <code class="literal">pcs resource relations G</code>
G
`- inner resource(s)
   |  members: A B
   |- A
   `- B</pre></section></section><section class="chapter" id="assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 15. Determining resource location with rules</h1></div></div></div><p>
			For more complicated location constraints, you can use Pacemaker rules to determine a resource’s location.
		</p><section class="section" id="ref_pacemaker-rules.adoc-determining-resource-location-with-rules"><div class="titlepage"><div><div><h2 class="title">15.1. Pacemaker rules</h2></div></div></div><p>
				Rules can be used to make your configuration more dynamic. One use of rules might be to assign machines to different processing groups (using a node attribute) based on time and to then use that attribute when creating location constraints.
			</p><p>
				Each rule can contain a number of expressions, date-expressions and even other rules. The results of the expressions are combined based on the rule’s <code class="literal">boolean-op</code> field to determine if the rule ultimately evaluates to <code class="literal">true</code> or <code class="literal">false</code>. What happens next depends on the context in which the rule is being used.
			</p><div class="table" id="tb-rule-props-HAAR"><p class="title"><strong>Table 15.1. Properties of a Rule</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319459452368" scope="col">Field</th><th align="left" valign="top" id="idm140319459451280" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319459452368"> <p>
								<code class="literal">role</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459451280"> <p>
								Limits the rule to apply only when the resource is in that role. Allowed values: <code class="literal">Started</code>, <code class="literal">Slave,</code> and <code class="literal">Master</code>. NOTE: A rule with <code class="literal">role="Master"</code> cannot determine the initial location of a clone instance. It will only affect which of the active instances will be promoted.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319459452368"> <p>
								<code class="literal">score</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459451280"> <p>
								The score to apply if the rule evaluates to <code class="literal">true</code>. Limited to use in rules that are part of location constraints.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319459452368"> <p>
								<code class="literal">score-attribute</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459451280"> <p>
								The node attribute to look up and use as a score if the rule evaluates to <code class="literal">true</code>. Limited to use in rules that are part of location constraints.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319459452368"> <p>
								<code class="literal">boolean-op</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459451280"> <p>
								How to combine the result of multiple expression objects. Allowed values: <code class="literal">and</code> and <code class="literal">or</code>. The default value is <code class="literal">and</code>.
							</p>
							 </td></tr></tbody></table></div></div><section class="section" id="node_attribute_expressions"><div class="titlepage"><div><div><h3 class="title">15.1.1. Node attribute expressions</h3></div></div></div><p>
					Node attribute expressions are used to control a resource based on the attributes defined by a node or nodes.
				</p><div class="table" id="tb-expressions-props-HAAR"><p class="title"><strong>Table 15.2. Properties of an Expression</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319462326224" scope="col">Field</th><th align="left" valign="top" id="idm140319462325136" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319462326224"> <p>
									<code class="literal">attribute</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319462325136"> <p>
									The node attribute to test
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319462326224"> <p>
									<code class="literal">type</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319462325136"> <p>
									Determines how the value(s) should be tested. Allowed values: <code class="literal">string</code>, <code class="literal">integer</code>, <code class="literal">version</code>. The default value is <code class="literal">string</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319462326224"> <p>
									<code class="literal">operation</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319462325136"> <p>
									The comparison to perform. Allowed values:
								</p>
								 <p>
									* <code class="literal">lt</code> - True if the node attribute’s value is less than <code class="literal">value</code>
								</p>
								 <p>
									* <code class="literal">gt</code> - True if the node attribute’s value is greater than <code class="literal">value</code>
								</p>
								 <p>
									* <code class="literal">lte</code> - True if the node attribute’s value is less than or equal to <code class="literal">value</code>
								</p>
								 <p>
									* <code class="literal">gte</code> - True if the node attribute’s value is greater than or equal to <code class="literal">value</code>
								</p>
								 <p>
									* <code class="literal">eq</code> - True if the node attribute’s value is equal to <code class="literal">value</code>
								</p>
								 <p>
									* <code class="literal">ne</code> - True if the node attribute’s value is not equal to <code class="literal">value</code>
								</p>
								 <p>
									* <code class="literal">defined</code> - True if the node has the named attribute
								</p>
								 <p>
									* <code class="literal">not_defined</code> - True if the node does not have the named attribute
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319462326224"> <p>
									<code class="literal">value</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319462325136"> <p>
									User supplied value for comparison (required unless <code class="literal">operation</code> is <code class="literal">defined</code> or <code class="literal">not_defined</code>)
								</p>
								 </td></tr></tbody></table></div></div><p>
					In addition to any attributes added by the administrator, the cluster defines special, built-in node attributes for each node that can also be used, as described in <a class="xref" href="index.html#tb-nodeattributes-HAAR" title="Table 15.3. Built-in Node Attributes">Table 15.3, “Built-in Node Attributes”</a>.
				</p><div class="table" id="tb-nodeattributes-HAAR"><p class="title"><strong>Table 15.3. Built-in Node Attributes</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319460865392" scope="col">Name</th><th align="left" valign="top" id="idm140319459219008" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319460865392"> <p>
									<code class="literal">#uname</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319459219008"> <p>
									Node name
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319460865392"> <p>
									<code class="literal">#id</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319459219008"> <p>
									Node ID
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319460865392"> <p>
									<code class="literal">#kind</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319459219008"> <p>
									Node type. Possible values are <code class="literal">cluster</code>, <code class="literal">remote</code>, and <code class="literal">container</code>. The value of <code class="literal">kind</code> is <code class="literal">remote</code> for Pacemaker Remote nodes created with the <code class="literal">ocf:pacemaker:remote</code> resource, and <code class="literal">container</code> for Pacemaker Remote guest nodes and bundle nodes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319460865392"> <p>
									<code class="literal">#is_dc</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319459219008"> <p>
									<code class="literal">true</code> if this node is a Designated Controller (DC), <code class="literal">false</code> otherwise
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319460865392"> <p>
									<code class="literal">#cluster_name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319459219008"> <p>
									The value of the <code class="literal">cluster-name</code> cluster property, if set
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319460865392"> <p>
									<code class="literal">#site_name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319459219008"> <p>
									The value of the <code class="literal">site-name</code> node attribute, if set, otherwise identical to <code class="literal">#cluster-name</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319460865392"> <p>
									<code class="literal">#role</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319459219008"> <p>
									The role the relevant promotable clone has on this node. Valid only within a rule for a location constraint for a promotable clone.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="time_date_based_expressions"><div class="titlepage"><div><div><h3 class="title">15.1.2. Time/date based expressions</h3></div></div></div><p>
					Date expressions are used to control a resource or cluster option based on the current date/time. They can contain an optional date specification.
				</p><div class="table" id="tb-dateexpress-props-HAAR"><p class="title"><strong>Table 15.4. Properties of a Date Expression</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319461127472" scope="col">Field</th><th align="left" valign="top" id="idm140319461126384" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319461127472"> <p>
									<code class="literal">start</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319461126384"> <p>
									A date/time conforming to the ISO8601 specification.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319461127472"> <p>
									<code class="literal">end</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319461126384"> <p>
									A date/time conforming to the ISO8601 specification.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319461127472"> <p>
									<code class="literal">operation</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319461126384"> <p>
									Compares the current date/time with the start or the end date or both the start and end date, depending on the context. Allowed values:
								</p>
								 <p>
									* <code class="literal">gt</code> - True if the current date/time is after <code class="literal">start</code>
								</p>
								 <p>
									* <code class="literal">lt</code> - True if the current date/time is before <code class="literal">end</code>
								</p>
								 <p>
									* <code class="literal">in_range</code> - True if the current date/time is after <code class="literal">start</code> and before <code class="literal">end</code>
								</p>
								 <p>
									* <code class="literal">date-spec</code> - performs a cron-like comparison to the current date/time
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="date_specifications"><div class="titlepage"><div><div><h3 class="title">15.1.3. Date specifications</h3></div></div></div><p>
					Date specifications are used to create cron-like expressions relating to time. Each field can contain a single number or a single range. Instead of defaulting to zero, any field not supplied is ignored.
				</p><p>
					For example, <code class="literal">monthdays="1"</code> matches the first day of every month and <code class="literal">hours="09-17"</code> matches the hours between 9 am and 5 pm (inclusive). However, you cannot specify <code class="literal">weekdays="1,2"</code> or <code class="literal">weekdays="1-2,5-6"</code> since they contain multiple ranges.
				</p><div class="table" id="tb-datespecs-props-HAAR"><p class="title"><strong>Table 15.5. Properties of a Date Specification</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319461258864" scope="col">Field</th><th align="left" valign="top" id="idm140319461257776" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319461258864"> <p>
									<code class="literal">id</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319461257776"> <p>
									A unique name for the date
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319461258864"> <p>
									<code class="literal">hours</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319461257776"> <p>
									Allowed values: 0-23
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319461258864"> <p>
									<code class="literal">monthdays</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319461257776"> <p>
									Allowed values: 0-31 (depending on month and year)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319461258864"> <p>
									<code class="literal">weekdays</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319461257776"> <p>
									Allowed values: 1-7 (1=Monday, 7=Sunday)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319461258864"> <p>
									<code class="literal">yeardays</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319461257776"> <p>
									Allowed values: 1-366 (depending on the year)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319461258864"> <p>
									<code class="literal">months</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319461257776"> <p>
									Allowed values: 1-12
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319461258864"> <p>
									<code class="literal">weeks</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319461257776"> <p>
									Allowed values: 1-53 (depending on <code class="literal">weekyear</code>)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319461258864"> <p>
									<code class="literal">years</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319461257776"> <p>
									Year according the Gregorian calendar
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319461258864"> <p>
									<code class="literal">weekyears</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319461257776"> <p>
									May differ from Gregorian years; for example, <code class="literal">2005-001 Ordinal</code> is also <code class="literal">2005-01-01 Gregorian</code> is also <code class="literal">2004-W53-6 Weekly</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319461258864"> <p>
									<code class="literal">moon</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319461257776"> <p>
									Allowed values: 0-7 (0 is new, 4 is full moon).
								</p>
								 </td></tr></tbody></table></div></div></section></section><section class="section" id="ref_configuring-constraint-using-rules.adoc-determining-resource-location-with-rules"><div class="titlepage"><div><div><h2 class="title">15.2. Configuring a pacemaker location constraint using rules</h2></div></div></div><p>
				Use the following command to configure a Pacemaker constraint that uses rules. If <code class="literal">score</code> is omitted, it defaults to INFINITY. If <code class="literal">resource-discovery</code> is omitted, it defaults to <code class="literal">always</code>.
			</p><p>
				For information on the <code class="literal">resource-discovery</code> option, see <a class="link" href="index.html#proc_limiting-resource-discovery-to-a-subset-of-nodes-determining-which-node-a-resource-runs-on" title="11.2. Limiting resource discovery to a subset of nodes">Limiting resource discovery to a subset of nodes</a>.
			</p><p>
				As with basic location constraints, you can use regular expressions for resources with these constraints as well.
			</p><p>
				When using rules to configure location constraints, the value of <code class="literal">score</code> can be positive or negative, with a positive value indicating "prefers" and a negative value indicating "avoids".
			</p><pre class="literallayout">pcs constraint location <span class="emphasis"><em>rsc</em></span> rule [resource-discovery=<span class="emphasis"><em>option</em></span>] [role=master|slave] [score=<span class="emphasis"><em>score</em></span> | score-attribute=<span class="emphasis"><em>attribute</em></span>] <span class="emphasis"><em>expression</em></span></pre><p>
				The <span class="emphasis"><em>expression</em></span> option can be one of the following where <span class="emphasis"><em>duration_options</em></span> and <span class="emphasis"><em>date_spec_options</em></span> are: hours, monthdays, weekdays, yeardays, months, weeks, years, weekyears, moon as described in <a class="link" href="index.html#tb-datespecs-props-HAAR" title="Table 15.5. Properties of a Date Specification">Properties of a Date Specification</a>.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">defined|not_defined <span class="emphasis"><em>attribute</em></span></code>
					</li><li class="listitem">
						<code class="literal"><span class="emphasis"><em>attribute</em></span> lt|gt|lte|gte|eq|ne [string|integer|version] <span class="emphasis"><em>value</em></span></code>
					</li><li class="listitem">
						<code class="literal">date gt|lt <span class="emphasis"><em>date</em></span></code>
					</li><li class="listitem">
						<code class="literal">date in_range <span class="emphasis"><em>date</em></span> to <span class="emphasis"><em>date</em></span></code>
					</li><li class="listitem">
						<code class="literal">date in_range <span class="emphasis"><em>date</em></span> to duration <span class="emphasis"><em>duration_options</em></span> …​</code>
					</li><li class="listitem">
						<code class="literal">date-spec <span class="emphasis"><em>date_spec_options</em></span></code>
					</li><li class="listitem">
						<code class="literal"><span class="emphasis"><em>expression</em></span> and|or <span class="emphasis"><em>expression</em></span></code>
					</li><li class="listitem">
						<code class="literal">(<span class="emphasis"><em>expression</em></span>)</code>
					</li></ul></div><p>
				Note that durations are an alternative way to specify an end for <code class="literal">in_range</code> operations by means of calculations. For example, you can specify a duration of 19 months.
			</p><p>
				The following location constraint configures an expression that is true if now is any time in the year 2018.
			</p><pre class="literallayout"># <code class="literal">pcs constraint location Webserver rule score=INFINITY date-spec years=2018</code></pre><p>
				The following command configures an expression that is true from 9 am to 5 pm, Monday through Friday. Note that the hours value of 16 matches up to 16:59:59, as the numeric value (hour) still matches.
			</p><pre class="literallayout"># <code class="literal">pcs constraint location Webserver rule score=INFINITY date-spec hours="9-16" weekdays="1-5"</code></pre><p>
				The following command configures an expression that is true when there is a full moon on Friday the thirteenth.
			</p><pre class="literallayout"># <code class="literal">pcs constraint location Webserver rule date-spec weekdays=5 monthdays=13 moon=4</code></pre><p>
				To remove a rule, use the following command. If the rule that you are removing is the last rule in its constraint, the constraint will be removed.
			</p><pre class="literallayout">pcs constraint rule remove <span class="emphasis"><em>rule_id</em></span></pre></section></section><section class="chapter" id="assembly_managing-cluster-resources-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 16. Managing cluster resources</h1></div></div></div><p>
			This section describes various commands you can use to manage cluster resources.
		</p><section class="section" id="proc_display-configured-resources-managing-cluster-resources"><div class="titlepage"><div><div><h2 class="title">16.1. Displaying configured resources</h2></div></div></div><p>
				To display a list of all configured resources, use the following command.
			</p><pre class="literallayout">pcs resource status</pre><p>
				For example, if your system is configured with a resource named <code class="literal">VirtualIP</code> and a resource named <code class="literal">WebSite</code>, the <code class="literal command">pcs resource show</code> command yields the following output.
			</p><pre class="literallayout"># <code class="literal">pcs resource status</code>
 VirtualIP	(ocf::heartbeat:IPaddr2):	Started
 WebSite	(ocf::heartbeat:apache):	Started</pre><p>
				To display a list of all configured resources and the parameters configured for those resources, use the <code class="literal">--full</code> option of the <code class="literal command">pcs resource config</code> command, as in the following example.
			</p><pre class="literallayout"># <code class="literal">pcs resource config</code>
 Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)
  Attributes: ip=192.168.0.120 cidr_netmask=24
  Operations: monitor interval=30s
 Resource: WebSite (type=apache class=ocf provider=heartbeat)
  Attributes: statusurl=http://localhost/server-status configfile=/etc/httpd/conf/httpd.conf
  Operations: monitor interval=1min</pre><p>
				To display the configured parameters for a resource, use the following command.
			</p><pre class="literallayout">pcs resource config <span class="emphasis"><em>resource_id</em></span></pre><p>
				For example, the following command displays the currently configured parameters for resource <code class="literal">VirtualIP</code>.
			</p><pre class="literallayout"># <code class="literal">pcs resource config VirtualIP</code>
 Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)
  Attributes: ip=192.168.0.120 cidr_netmask=24
  Operations: monitor interval=30s</pre></section><section class="section" id="proc_modify-resource-parameters-managing-cluster-resources"><div class="titlepage"><div><div><h2 class="title">16.2. Modifying resource parameters</h2></div></div></div><p>
				To modify the parameters of a configured resource, use the following command.
			</p><pre class="literallayout">pcs resource update <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>resource_options</em></span>]</pre><p>
				The following sequence of commands show the initial values of the configured parameters for resource <code class="literal">VirtualIP</code>, the command to change the value of the <code class="literal">ip</code> parameter, and the values following the update command.
			</p><pre class="literallayout"># <code class="literal">pcs resource config VirtualIP</code>
 Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)
  Attributes: ip=192.168.0.120 cidr_netmask=24
  Operations: monitor interval=30s
# <code class="literal">pcs resource update VirtualIP ip=192.169.0.120</code>
# <code class="literal">pcs resource config VirtualIP</code>
 Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)
  Attributes: ip=192.169.0.120 cidr_netmask=24
  Operations: monitor interval=30s</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					When you update a resource’s operation with the <code class="literal">pcs resource update</code> command, any options you do not specifically call out are reset to their default values.
				</p></div></div></section><section class="section" id="proc_cleanup-cluster-resources-managing-cluster-resources"><div class="titlepage"><div><div><h2 class="title">16.3. Clearing failure status of cluster resources</h2></div></div></div><p>
				If a resource has failed, a failure message appears when you display the cluster status. If you resolve that resource, you can clear that failure status with the <code class="literal command">pcs resource cleanup</code> command. This command resets the resource status and <code class="literal">failcount</code>, telling the cluster to forget the operation history of a resource and re-detect its current state.
			</p><p>
				The following command cleans up the resource specified by <span class="emphasis"><em>resource_id</em></span>.
			</p><pre class="literallayout">pcs resource cleanup <span class="emphasis"><em>resource_id</em></span></pre><p>
				If you do not specify a <span class="emphasis"><em>resource_id</em></span>, this command resets the resource status and <code class="literal">failcount</code>for all resources.
			</p><p>
				The <code class="literal command">pcs resource cleanup</code> command probes only the resources that display as a failed action. To probe all resources on all nodes you can enter the following command:
			</p><pre class="literallayout">pcs resource refresh</pre><p>
				By default, the <code class="literal command">pcs resource refresh</code> command probes only the nodes where a resource’s state is known. To probe all resources even if the state is not known, enter the following command:
			</p><pre class="literallayout">pcs resource refresh --full</pre></section><section class="section" id="assembly_moving-cluster-resources-managing-cluster-resources"><div class="titlepage"><div><div><h2 class="title">16.4. Moving resources in a cluster</h2></div></div></div><p>
				Pacemaker provides a variety of mechanisms for configuring a resource to move from one node to another and to manually move a resource when needed.
			</p><p>
				You can manually move resources in a cluster with the <code class="literal command">pcs resource move</code> and <code class="literal command">pcs resource relocate</code> commands, as described in <a class="link" href="index.html#assembly_manually-move-resources-cluster-maintenance" title="29.2. Manually moving cluster resources">Manually moving cluster resources</a>.
			</p><p>
				In addition to these commands, you can also control the behavior of cluster resources by enabling, disabling, and banning resources, as described in <a class="link" href="index.html#proc_disabling-resources-cluster-maintenance" title="29.3. Disabling, enabling, and banning cluster resources">Enabling, disabling, and banning cluster resources</a>.
			</p><p>
				You can configure a resource so that it will move to a new node after a defined number of failures, and you can configure a cluster to move resources when external connectivity is lost.
			</p><section class="section" id="proc_move-resource-from-failure-moving-cluster-resources"><div class="titlepage"><div><div><h3 class="title">16.4.1. Moving resources due to failure</h3></div></div></div><p>
					When you create a resource, you can configure the resource so that it will move to a new node after a defined number of failures by setting the <code class="literal">migration-threshold</code> option for that resource. Once the threshold has been reached, this node will no longer be allowed to run the failed resource until:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The administrator manually resets the resource’s <code class="literal">failcount</code> using the <code class="literal command">pcs resource cleanup</code> command.
						</li><li class="listitem">
							The resource’s <code class="literal">failure-timeout</code> value is reached.
						</li></ul></div><p>
					The value of <code class="literal">migration-threshold</code> is set to <code class="literal">INFINITY</code> by default. <code class="literal">INFINITY</code> is defined internally as a very large but finite number. A value of 0 disables the <code class="literal">migration-threshold</code> feature.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Setting a <code class="literal">migration-threshold</code> for a resource is not the same as configuring a resource for migration, in which the resource moves to another location without loss of state.
					</p></div></div><p>
					The following example adds a migration threshold of 10 to the resource named <code class="literal">dummy_resource</code>, which indicates that the resource will move to a new node after 10 failures.
				</p><pre class="literallayout"># <code class="literal">pcs resource meta dummy_resource migration-threshold=10</code></pre><p>
					You can add a migration threshold to the defaults for the whole cluster with the following command.
				</p><pre class="literallayout"># <code class="literal">pcs resource defaults migration-threshold=10</code></pre><p>
					To determine the resource’s current failure status and limits, use the <code class="literal command">pcs resource failcount show</code> command.
				</p><p>
					There are two exceptions to the migration threshold concept; they occur when a resource either fails to start or fails to stop. If the cluster property <code class="literal">start-failure-is-fatal</code> is set to <code class="literal">true</code> (which is the default), start failures cause the <code class="literal">failcount</code> to be set to <code class="literal">INFINITY</code> and thus always cause the resource to move immediately.
				</p><p>
					Stop failures are slightly different and crucial. If a resource fails to stop and STONITH is enabled, then the cluster will fence the node in order to be able to start the resource elsewhere. If STONITH is not enabled, then the cluster has no way to continue and will not try to start the resource elsewhere, but will try to stop it again after the failure timeout.
				</p></section><section class="section" id="proc_move-resource-from-connectivity-moving-cluster-resources"><div class="titlepage"><div><div><h3 class="title">16.4.2. Moving resources due to connectivity changes</h3></div></div></div><p>
					Setting up the cluster to move resources when external connectivity is lost is a two step process.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Add a <code class="literal">ping</code> resource to the cluster. The <code class="literal">ping</code> resource uses the system utility of the same name to test if a list of machines (specified by DNS host name or IPv4/IPv6 address) are reachable and uses the results to maintain a node attribute called <code class="literal">pingd</code>.
						</li><li class="listitem">
							Configure a location constraint for the resource that will move the resource to a different node when connectivity is lost.
						</li></ol></div><p>
					<a class="xref" href="index.html#tb-resource-props-summary-HAAR" title="Table 10.1. Resource Agent Identifiers">Table 10.1, “Resource Agent Identifiers”</a> describes the properties you can set for a <code class="literal">ping</code> resource.
				</p><div class="table" id="tb-pingoptions-HAAR"><p class="title"><strong>Table 16.1. Properties of a ping resources</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319458569152" scope="col">Field</th><th align="left" valign="top" id="idm140319458568064" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319458569152"> <p>
									<code class="literal">dampen</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319458568064"> <p>
									The time to wait (dampening) for further changes to occur. This prevents a resource from bouncing around the cluster when cluster nodes notice the loss of connectivity at slightly different times.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319458569152"> <p>
									<code class="literal">multiplier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319458568064"> <p>
									The number of connected ping nodes gets multiplied by this value to get a score. Useful when there are multiple ping nodes configured.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319458569152"> <p>
									<code class="literal">host_list</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319458568064"> <p>
									The machines to contact in order to determine the current connectivity status. Allowed values include resolvable DNS host names, IPv4 and IPv6 addresses. The entries in the host list are space separated.
								</p>
								 </td></tr></tbody></table></div></div><p>
					The following example command creates a <code class="literal">ping</code> resource that verifies connectivity to <code class="literal">gateway.example.com</code>. In practice, you would verify connectivity to your network gateway/router. You configure the <code class="literal">ping</code> resource as a clone so that the resource will run on all cluster nodes.
				</p><pre class="literallayout"># <code class="literal">pcs resource create ping ocf:pacemaker:ping dampen=5s multiplier=1000 host_list=gateway.example.com clone</code></pre><p>
					The following example configures a location constraint rule for the existing resource named <code class="literal">Webserver</code>. This will cause the <code class="literal">Webserver</code> resource to move to a host that is able to ping <code class="literal">gateway.example.com</code> if the host that it is currently running on cannot ping <code class="literal">gateway.example.com</code>.
				</p><pre class="literallayout"># <code class="literal">pcs constraint location Webserver rule score=-INFINITY pingd lt 1 or not_defined pingd</code></pre><pre class="literallayout"> Module included in the following assemblies:
//
// &lt;List assemblies here, each on a new line&gt;
// rhel-8-docs/enterprise/assemblies/assembly_managing-cluster-resources.adoc</pre></section></section><section class="section" id="proc_disabling-monitor-operationmanaging-cluster-resources"><div class="titlepage"><div><div><h2 class="title">16.5. Disabling a monitor operation</h2></div></div></div><p>
				The easiest way to stop a recurring monitor is to delete it. However, there can be times when you only want to disable it temporarily. In such cases, add <code class="literal">enabled="false"</code> to the operation’s definition. When you want to reinstate the monitoring operation, set <code class="literal">enabled="true"</code> to the operation’s definition.
			</p><p>
				When you update a resource’s operation with the <code class="literal">pcs resource update</code> command, any options you do not specifically call out are reset to their default values. For example, if you have configured a monitoring operation with a custom timeout value of 600, running the following commands will reset the timeout value to the default value of 20 (or whatever you have set the default value to with the <code class="literal">pcs resource ops default</code> command).
			</p><pre class="literallayout"># <code class="literal">pcs resource update resourceXZY op monitor enabled=false</code>
# <code class="literal">pcs resource update resourceXZY op monitor enabled=true</code></pre><p>
				In order to maintain the original value of 600 for this option, when you reinstate the monitoring operation you must specify that value, as in the following example.
			</p><pre class="literallayout"># <code class="literal">pcs resource update resourceXZY op monitor timeout=600 enabled=true</code></pre></section></section><section class="chapter" id="assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 17. Creating cluster resources that are active on multiple nodes (cloned resources)</h1></div></div></div><p>
			You can clone a cluster resource so that the resource can be active on multiple nodes. For example, you can use cloned resources to configure multiple instances of an IP resource to distribute throughout a cluster for node balancing. You can clone any resource provided the resource agent supports it. A clone consists of one resource or one resource group.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				Only resources that can be active on multiple nodes at the same time are suitable for cloning. For example, a <code class="literal">Filesystem</code> resource mounting a non-clustered file system such as <code class="literal">ext4</code> from a shared memory device should not be cloned. Since the <code class="literal">ext4</code> partition is not cluster aware, this file system is not suitable for read/write operations occurring from multiple nodes at the same time.
			</p></div></div><section class="section" id="proc_creating-cloned-resource-creating-multinode-resources"><div class="titlepage"><div><div><h2 class="title">17.1. Creating and removing a cloned resource</h2></div></div></div><p>
				You can create a resource and a clone of that resource at the same time with the following command.
			</p><pre class="literallayout">pcs resource create <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>standard</em></span>:[<span class="emphasis"><em>provider</em></span>:]]<span class="emphasis"><em>type</em></span> [<span class="emphasis"><em>resource options</em></span>] [meta <span class="emphasis"><em>resource meta options</em></span>] clone [<span class="emphasis"><em>clone options</em></span>]</pre><p>
				The name of the clone will be <code class="literal"><span class="emphasis"><em>resource_id</em></span>-clone</code>.
			</p><p>
				You cannot create a resource group and a clone of that resource group in a single command.
			</p><p>
				Alternately, you can create a clone of a previously-created resource or resource group with the following command.
			</p><pre class="literallayout">pcs resource clone <span class="emphasis"><em>resource_id</em></span> | <span class="emphasis"><em>group_name</em></span> [<span class="emphasis"><em>clone options</em></span>]...</pre><p>
				The name of the clone will be <code class="literal"><span class="emphasis"><em>resource_id</em></span>-clone</code> or <code class="literal"><span class="emphasis"><em>group_name</em></span>-clone</code>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You need to configure resource configuration changes on one node only.
				</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					When configuring constraints, always use the name of the group or clone.
				</p></div></div><p>
				When you create a clone of a resource, the clone takes on the name of the resource with <code class="literal">-clone</code> appended to the name. The following commands creates a resource of type <code class="literal">apache</code> named <code class="literal">webfarm</code> and a clone of that resource named <code class="literal">webfarm-clone</code>.
			</p><pre class="literallayout"># <code class="literal">pcs resource create webfarm apache clone</code></pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					When you create a resource or resource group clone that will be ordered after another clone, you should almost always set the <code class="literal">interleave=true</code> option. This ensures that copies of the dependent clone can stop or start when the clone it depends on has stopped or started on the same node. If you do not set this option, if a cloned resource B depends on a cloned resource A and a node leaves the cluster, when the node returns to the cluster and resource A starts on that node, then all of the copies of resource B on all of the nodes will restart. This is because when a dependent cloned resource does not have the <code class="literal">interleave</code> option set, all instances of that resource depend on any running instance of the resource it depends on.
				</p></div></div><p>
				Use the following command to remove a clone of a resource or a resource group. This does not remove the resource or resource group itself.
			</p><pre class="literallayout">pcs resource unclone <span class="emphasis"><em>resource_id</em></span> | <span class="emphasis"><em>group_name</em></span></pre><p>
				<a class="xref" href="index.html#tb-resourcecloneoptions-HAAR" title="Table 17.1. Resource Clone Options">Table 17.1, “Resource Clone Options”</a> describes the options you can specify for a cloned resource.
			</p><div class="table" id="tb-resourcecloneoptions-HAAR"><p class="title"><strong>Table 17.1. Resource Clone Options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319466252704" scope="col">Field</th><th align="left" valign="top" id="idm140319466251616" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319466252704"> <p>
								<code class="literal">priority, target-role, is-managed</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319466251616"> <p>
								Options inherited from resource that is being cloned, as described in <a class="xref" href="index.html#tb-resource-options-HAAR" title="Table 10.3. Resource Meta Options">Table 10.3, “Resource Meta Options”</a>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319466252704"> <p>
								<code class="literal">clone-max</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319466251616"> <p>
								How many copies of the resource to start. Defaults to the number of nodes in the cluster.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319466252704"> <p>
								<code class="literal">clone-node-max</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319466251616"> <p>
								How many copies of the resource can be started on a single node; the default value is <code class="literal">1</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319466252704"> <p>
								<code class="literal">notify</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319466251616"> <p>
								When stopping or starting a copy of the clone, tell all the other copies beforehand and when the action was successful. Allowed values: <code class="literal">false</code>, <code class="literal">true</code>. The default value is <code class="literal">false</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319466252704"> <p>
								<code class="literal">globally-unique</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319466251616"> <p>
								Does each copy of the clone perform a different function? Allowed values: <code class="literal">false</code>, <code class="literal">true</code>
							</p>
							 <p>
								If the value of this option is <code class="literal">false</code>, these resources behave identically everywhere they are running and thus there can be only one copy of the clone active per machine.
							</p>
							 <p>
								If the value of this option is <code class="literal">true</code>, a copy of the clone running on one machine is not equivalent to another instance, whether that instance is running on another node or on the same node. The default value is <code class="literal">true</code> if the value of <code class="literal">clone-node-max</code> is greater than one; otherwise the default value is <code class="literal">false</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319466252704"> <p>
								<code class="literal">ordered</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319466251616"> <p>
								Should the copies be started in series (instead of in parallel). Allowed values: <code class="literal">false</code>, <code class="literal">true</code>. The default value is <code class="literal">false</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319466252704"> <p>
								<code class="literal">interleave</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319466251616"> <p>
								Changes the behavior of ordering constraints (between clones) so that copies of the first clone can start or stop as soon as the copy on the same node of the second clone has started or stopped (rather than waiting until every instance of the second clone has started or stopped). Allowed values: <code class="literal">false</code>, <code class="literal">true</code>. The default value is <code class="literal">false</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319466252704"> <p>
								<code class="literal">clone-min</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319466251616"> <p>
								If a value is specified, any clones which are ordered after this clone will not be able to start until the specified number of instances of the original clone are running, even if the <code class="literal">interleave</code> option is set to <code class="literal">true</code>.
							</p>
							 </td></tr></tbody></table></div></div><p>
				To achieve a stable allocation pattern, clones are slightly sticky by default, which indicates that they have a slight preference for staying on the node where they are running. If no value for <code class="literal">resource-stickiness</code> is provided, the clone will use a value of 1. Being a small value, it causes minimal disturbance to the score calculations of other resources but is enough to prevent Pacemaker from needlessly moving copies around the cluster. For information on setting the <code class="literal">resource-stickiness</code> resource meta-option, see <a class="link" href="index.html#proc_configuring-resource-meta-options-configuring-cluster-resources" title="10.3. Configuring resource meta options">Configuring resource meta options</a>.
			</p></section><section class="section" id="proc_configuring-clone-constraints-creating-multinode-resources"><div class="titlepage"><div><div><h2 class="title">17.2. Configuring clone resource constraints</h2></div></div></div><p>
				In most cases, a clone will have a single copy on each active cluster node. You can, however, set <code class="literal">clone-max</code> for the resource clone to a value that is less than the total number of nodes in the cluster. If this is the case, you can indicate which nodes the cluster should preferentially assign copies to with resource location constraints. These constraints are written no differently to those for regular resources except that the clone’s id must be used.
			</p><p>
				The following command creates a location constraint for the cluster to preferentially assign resource clone <code class="literal">webfarm-clone</code> to <code class="literal">node1</code>.
			</p><pre class="literallayout"># <code class="literal">pcs constraint location webfarm-clone prefers node1</code></pre><p>
				Ordering constraints behave slightly differently for clones. In the example below, because the <code class="literal">interleave</code> clone option is left to default as <code class="literal">false</code>, no instance of <code class="literal">webfarm-stats</code> will start until all instances of <code class="literal">webfarm-clone</code> that need to be started have done so. Only if no copies of <code class="literal">webfarm-clone</code> can be started then <code class="literal">webfarm-stats</code> will be prevented from being active. Additionally, <code class="literal">webfarm-clone</code> will wait for <code class="literal">webfarm-stats</code> to be stopped before stopping itself.
			</p><pre class="literallayout"># <code class="literal">pcs constraint order start webfarm-clone then webfarm-stats</code></pre><p>
				Colocation of a regular (or group) resource with a clone means that the resource can run on any machine with an active copy of the clone. The cluster will choose a copy based on where the clone is running and the resource’s own location preferences.
			</p><p>
				Colocation between clones is also possible. In such cases, the set of allowed locations for the clone is limited to nodes on which the clone is (or will be) active. Allocation is then performed as normally.
			</p><p>
				The following command creates a colocation constraint to ensure that the resource <code class="literal">webfarm-stats</code> runs on the same node as an active copy of <code class="literal">webfarm-clone</code>.
			</p><pre class="literallayout"># <code class="literal">pcs constraint colocation add webfarm-stats with webfarm-clone</code></pre></section><section class="section" id="assembly_creating-promotable-clone-resources-creating-multinode-resources"><div class="titlepage"><div><div><h2 class="title">17.3. Creating promotable clone resources</h2></div></div></div><p>
				Promotable clone resources are clone resources with the <code class="literal">promotable</code> meta attribute set to <code class="literal">true</code>. They allow the instances to be in one of two operating modes; these are called <code class="literal">Master</code> and <code class="literal">Slave</code>. The names of the modes do not have specific meanings, except for the limitation that when an instance is started, it must come up in the <code class="literal">Slave</code> state.
			</p><section class="section" id="proc_-creating-promotable-resource-creating-promotable-clone-resources"><div class="titlepage"><div><div><h3 class="title">17.3.1. Creating a promotable resource</h3></div></div></div><p>
					You can create a resource as a promotable clone with the following single command.
				</p><pre class="literallayout">pcs resource create <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>standard</em></span>:[<span class="emphasis"><em>provider</em></span>:]]<span class="emphasis"><em>type</em></span> [<span class="emphasis"><em>resource options</em></span>] promotable [<span class="emphasis"><em>clone options</em></span>]</pre><p>
					The name of the promotable clone will be <code class="literal"><span class="emphasis"><em>resource_id</em></span>-clone</code>.
				</p><p>
					Alternately, you can create a promotable resource from a previously-created resource or resource group with the following command. The name of the promotable clone will be <code class="literal"><span class="emphasis"><em>resource_id</em></span>-clone</code> or <code class="literal"><span class="emphasis"><em>group_name</em></span>-clone</code>.
				</p><pre class="literallayout">pcs resource promotable <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>clone options</em></span>]</pre><p>
					<a class="xref" href="index.html#tb-promotablecloneoptions-HAAR" title="Table 17.2. Extra Clone Options Available for Promotable Clones">Table 17.2, “Extra Clone Options Available for Promotable Clones”</a> describes the extra clone options you can specify for a promotable resource.
				</p><div class="table" id="tb-promotablecloneoptions-HAAR"><p class="title"><strong>Table 17.2. Extra Clone Options Available for Promotable Clones</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319462161440" scope="col">Field</th><th align="left" valign="top" id="idm140319462160352" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319462161440"> <p>
									<code class="literal">promoted-max</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319462160352"> <p>
									How many copies of the resource can be promoted; default 1.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319462161440"> <p>
									<code class="literal">promoted-node-max</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319462160352"> <p>
									How many copies of the resource can be promoted on a single node; default 1.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="proc_-configuring-promotable-resource-constraints-creating-promotable-clone-resources"><div class="titlepage"><div><div><h3 class="title">17.3.2. Configuring promotable resource constraints</h3></div></div></div><p>
					In most cases, a promotable resources will have a single copy on each active cluster node. If this is not the case, you can indicate which nodes the cluster should preferentially assign copies to with resource location constraints. These constraints are written no differently than those for regular resources.
				</p><p>
					You can create a colocation constraint which specifies whether the resources are operating in a master or slave role. The following command creates a resource colocation constraint.
				</p><pre class="literallayout">pcs constraint colocation add [master|slave] <span class="emphasis"><em>source_resource</em></span> with [master|slave] <span class="emphasis"><em>target_resource</em></span> [<span class="emphasis"><em>score</em></span>] [<span class="emphasis"><em>options</em></span>]</pre><p>
					For information on colocation constraints, see <a class="link" href="index.html#assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters" title="Chapter 13. Colocating cluster resources">Colocating cluster resources</a>.
				</p><p>
					When configuring an ordering constraint that includes promotable resources, one of the actions that you can specify for the resources is <code class="literal">promote</code>, indicating that the resource be promoted from slave role to master role. Additionally, you can specify an action of <code class="literal">demote</code>, indicated that the resource be demoted from master role to slave role.
				</p><p>
					The command for configuring an order constraint is as follows.
				</p><pre class="literallayout">pcs constraint order [<span class="emphasis"><em>action</em></span>] <span class="emphasis"><em>resource_id</em></span> then [<span class="emphasis"><em>action</em></span>] <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>options</em></span>]</pre><p>
					For information on resource order constraints, see ifdef:: <a class="link" href="index.html#assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters" title="Chapter 12. Determining the order in which cluster resources are run">Determining the order in which cluster resources are run</a>.
				</p></section></section></section><section class="chapter" id="assembly_clusternode-management-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 18. Managing cluster nodes</h1></div></div></div><p>
			The following sections describe the commands you use to manage cluster nodes, including commands to start and stop cluster services and to add and remove cluster nodes.
		</p><section class="section" id="proc_cluster-stop-clusternode-management"><div class="titlepage"><div><div><h2 class="title">18.1. Stopping cluster services</h2></div></div></div><p>
				The following command stops cluster services on the specified node or nodes. As with the <code class="literal command">pcs cluster start</code>, the <code class="literal">--all</code> option stops cluster services on all nodes and if you do not specify any nodes, cluster services are stopped on the local node only.
			</p><pre class="literallayout">pcs cluster stop [--all | <span class="emphasis"><em>node</em></span>] [...]</pre><p>
				You can force a stop of cluster services on the local node with the following command, which performs a <code class="literal command">kill -9</code> command.
			</p><pre class="literallayout">pcs cluster kill</pre></section><section class="section" id="proc_cluster-enable-clusternode-management"><div class="titlepage"><div><div><h2 class="title">18.2. Enabling and disabling cluster services</h2></div></div></div><p>
				Use the following command to enables the cluster services, which configures the cluster services to run on startup on the specified node or nodes. Enabling allows nodes to automatically rejoin the cluster after they have been fenced, minimizing the time the cluster is at less than full strength. If the cluster services are not enabled, an administrator can manually investigate what went wrong before starting the cluster services manually, so that, for example, a node with hardware issues in not allowed back into the cluster when it is likely to fail again.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						If you specify the <code class="literal">--all</code> option, the command enables cluster services on all nodes.
					</li><li class="listitem">
						If you do not specify any nodes, cluster services are enabled on the local node only.
					</li></ul></div><pre class="literallayout">pcs cluster enable [--all | <span class="emphasis"><em>node</em></span>] [...]</pre><p>
				Use the following command to configure the cluster services not to run on startup on the specified node or nodes.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						If you specify the <code class="literal">--all</code> option, the command disables cluster services on all nodes.
					</li><li class="listitem">
						If you do not specify any nodes, cluster services are disabled on the local node only.
					</li></ul></div><pre class="literallayout">pcs cluster disable [--all | <span class="emphasis"><em>node</em></span>] [...]</pre></section><section class="section" id="proc_cluster-nodeadd-clusternode-management"><div class="titlepage"><div><div><h2 class="title">18.3. Adding cluster nodes</h2></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					It is highly recommended that you add nodes to existing clusters only during a production maintenance window. This allows you to perform appropriate resource and deployment testing for the new node and its fencing configuration.
				</p></div></div><p>
				Use the following procedure to add a new node to an existing cluster. This procedure adds standard clusters nodes running <code class="literal">corosync</code>. For information on integrating non-corosync nodes into a cluster, see <a class="link" href="index.html#assembly_remote-node-management-configuring-and-managing-high-availability-clusters" title="Chapter 28. Integrating non-corosync nodes into a cluster: the pacemaker_remote service">Integrating non-corosync nodes into a cluster: the pacemaker_remote service</a>.
			</p><p>
				In this example, the existing cluster nodes are <code class="literal">clusternode-01.example.com</code>, <code class="literal">clusternode-02.example.com</code>, and <code class="literal">clusternode-03.example.com</code>. The new node is <code class="literal">newnode.example.com</code>.
			</p><p>
				On the new node to add to the cluster, perform the following tasks.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Install the cluster packages. If the cluster uses SBD, the Booth ticket manager, or a quorum device, you must manually install the respective packages (<code class="literal">sbd</code>, <code class="literal">booth-site</code>, <code class="literal">corosync-qdevice</code>) on the new node as well.
					</p><pre class="literallayout">[root@newnode ~]# <code class="literal">yum install -y pcs fence-agents-all</code></pre><p class="simpara">
						In addition to the cluster packages, you will also need to install and configure all of the services that you are running in the cluster, which you have installed on the existing cluster nodes. For example, if you are running an Apache HTTP server in a Red Hat high availability cluster, you will need to install the server on the node you are adding, as well as the <code class="literal">wget</code> tool that checks the status of the server.
					</p></li><li class="listitem"><p class="simpara">
						If you are running the <code class="literal command">firewalld</code> daemon, execute the following commands to enable the ports that are required by the Red Hat High Availability Add-On.
					</p><pre class="literallayout"># <code class="literal">firewall-cmd --permanent --add-service=high-availability</code>
# <code class="literal">firewall-cmd --add-service=high-availability</code></pre></li><li class="listitem"><p class="simpara">
						Set a password for the user ID <code class="literal">hacluster</code>. It is recommended that you use the same password for each node in the cluster.
					</p><pre class="literallayout">[root@newnode ~]# <code class="literal">passwd hacluster</code>
Changing password for user hacluster.
New password:
Retype new password:
passwd: all authentication tokens updated successfully.</pre></li><li class="listitem"><p class="simpara">
						Execute the following commands to start the <code class="literal">pcsd</code> service and to enable <code class="literal">pcsd</code> at system start.
					</p><pre class="literallayout"># <code class="literal">systemctl start pcsd.service</code>
# <code class="literal">systemctl enable pcsd.service</code></pre></li></ol></div><p>
				On a node in the existing cluster, perform the following tasks.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Authenticate user <code class="literal">hacluster</code> on the new cluster node.
					</p><pre class="literallayout">[root@clusternode-01 ~]# <code class="literal">pcs host auth newnode.example.com</code>
Username: hacluster
Password:
newnode.example.com: Authorized</pre></li><li class="listitem"><p class="simpara">
						Add the new node to the existing cluster. This command also syncs the cluster configuration file <code class="literal">corosync.conf</code> to all nodes in the cluster, including the new node you are adding.
					</p><pre class="literallayout">[root@clusternode-01 ~]# <code class="literal">pcs cluster node add newnode.example.com</code></pre></li></ol></div><p>
				On the new node to add to the cluster, perform the following tasks.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Start and enable cluster services on the new node.
					</p><pre class="literallayout">[root@newnode ~]# <code class="literal">pcs cluster start</code>
Starting Cluster...
[root@newnode ~]# <code class="literal">pcs cluster enable</code></pre></li><li class="listitem">
						Ensure that you configure and test a fencing device for the new cluster node.
					</li></ol></div></section><section class="section" id="proc_cluster-noderemove-clusternode-management"><div class="titlepage"><div><div><h2 class="title">18.4. Removing cluster nodes</h2></div></div></div><p>
				The following command shuts down the specified node and removes it from the cluster configuration file, <code class="literal">corosync.conf</code>, on all of the other nodes in the cluster.
			</p><pre class="literallayout">pcs cluster node remove <span class="emphasis"><em>node</em></span></pre></section></section><section class="chapter" id="assembly_cluster-permissions-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 19. Setting user permissions for a Pacemaker cluster</h1></div></div></div><p>
			You can grant permission for specific users other than user <code class="literal">hacluster</code> to manage a Pacemaker cluster. There are two sets of permissions that you can grant to individual users:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Permissions that allow individual users to manage the cluster through the Web UI and to run <code class="literal command">pcs</code> commands that connect to nodes over a network. Commands that connect to nodes over a network include commands to set up a cluster, or to add or remove nodes from a cluster.
				</li><li class="listitem">
					Permissions for local users to allow read-only or read-write access to the cluster configuration. Commands that do not require connecting over a network include commands that edit the cluster configuration, such as those that create resources and configure constraints.
				</li></ul></div><p>
			In situations where both sets of permissions have been assigned, the permissions for commands that connect over a network are applied first, and then permissions for editing the cluster configuration on the local node are applied. Most <code class="literal command">pcs</code> commands do not require network access and in those cases the network permissions will not apply.
		</p><section class="section" id="proc_setting-cluster-access-over-network-cluster-permissions"><div class="titlepage"><div><div><h2 class="title">19.1. Setting permissions for node access over a network</h2></div></div></div><p>
				To grant permission for specific users to manage the cluster through the Web UI and to run <code class="literal command">pcs</code> commands that connect to nodes over a network, add those users to the group <code class="literal">haclient</code>. This must be done on every node in the cluster.
			</p></section><section class="section" id="proc_setting-local-cluster-permissions-cluster-permissions"><div class="titlepage"><div><div><h2 class="title">19.2. Setting local permissions using ACLs</h2></div></div></div><p>
				You can use the <code class="literal command">pcs acl</code> command to set permissions for local users to allow read-only or read-write access to the cluster configuration by using access control lists (ACLs).
			</p><p>
				By default, ACLs are not enabled. When ACLs are not enabled, the root user and any user who is a member of the group <code class="literal">haclient</code> on all nodes has full local read/write access to the cluster configuration while users who are not members of <code class="literal">haclient</code> have no access. When ACLs are enabled, however, even users who are members of the <code class="literal">haclient</code> group have access only to what has been granted to that user by the ACLs.
			</p><p>
				Setting permissions for local users is a two step process:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Execute the <code class="literal command">pcs acl role create…​</code> command to create a <span class="emphasis"><em>role</em></span> which defines the permissions for that role.
					</li><li class="listitem">
						Assign the role you created to a user with the <code class="literal command">pcs acl user create</code> command. If you assign multiple roles to the same user, any <code class="literal">deny</code> permission takes precedence, then <code class="literal">write</code>, then <code class="literal">read</code>.
					</li></ol></div><p>
				The following example procedure provides read-only access for a cluster configuration to a local user named <code class="literal">rouser</code>. Note that it is also possible to restrict access to certain portions of the configuration only.
			</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					It is important to perform this procedure as root or to save all of the configuration updates to a working file which you can then push to the active CIB when you are finished. Otherwise, you can lock yourself out of making any further changes. For information on saving configuration updates to a working file, see <a class="link" href="index.html#proc_configure-testfile-pcs-operation" title="3.3. Saving a configuration change to a working file">Saving a configuration change to a working file</a>.
				</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						This procedure requires that the user <code class="literal">rouser</code> exists on the local system and that the user <code class="literal">rouser</code> is a member of the group <code class="literal">haclient</code>.
					</p><pre class="literallayout"># <code class="literal">adduser rouser</code>
# <code class="literal">usermod -a -G haclient rouser</code></pre></li><li class="listitem"><p class="simpara">
						Enable Pacemaker ACLs with the <code class="literal command">pcs acl enable</code> command.
					</p><pre class="literallayout"># <code class="literal">pcs acl enable</code></pre></li><li class="listitem"><p class="simpara">
						Create a role named <code class="literal">read-only</code> with read-only permissions for the cib.
					</p><pre class="literallayout"># <code class="literal">pcs acl role create read-only description="Read access to cluster" read xpath /cib</code></pre></li><li class="listitem"><p class="simpara">
						Create the user <code class="literal">rouser</code> in the pcs ACL system and assign that user the <code class="literal">read-only</code> role.
					</p><pre class="literallayout"># <code class="literal">pcs acl user create rouser read-only</code></pre></li><li class="listitem"><p class="simpara">
						View the current ACLs.
					</p><pre class="literallayout"># <code class="literal">pcs acl</code>
User: rouser
  Roles: read-only
Role: read-only
  Description: Read access to cluster
  Permission: read xpath /cib (read-only-read)</pre></li></ol></div></section></section><section class="chapter" id="assembly_resource-monitoring-operations-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 20. Resource monitoring operations</h1></div></div></div><p>
			To ensure that resources remain healthy, you can add a monitoring operation to a resource’s definition. If you do not specify a monitoring operation for a resource, by default the <code class="literal command">pcs</code> command will create a monitoring operation, with an interval that is determined by the resource agent. If the resource agent does not provide a default monitoring interval, the pcs command will create a monitoring operation with an interval of 60 seconds.
		</p><p>
			<a class="xref" href="index.html#tb-resource-operation-HAAR" title="Table 20.1. Properties of an Operation">Table 20.1, “Properties of an Operation”</a> summarizes the properties of a resource monitoring operation.
		</p><div class="table" id="tb-resource-operation-HAAR"><p class="title"><strong>Table 20.1. Properties of an Operation</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 75%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319459044560" scope="col">Field</th><th align="left" valign="top" id="idm140319459043472" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319459044560"> <p>
							<code class="literal">id</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140319459043472"> <p>
							Unique name for the action. The system assigns this when you configure an operation.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140319459044560"> <p>
							<code class="literal">name</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140319459043472"> <p>
							The action to perform. Common values: <code class="literal">monitor</code>, <code class="literal">start</code>, <code class="literal">stop</code>
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140319459044560"> <p>
							<code class="literal">interval</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140319459043472"> <p>
							If set to a nonzero value, a recurring operation is created that repeats at this frequency, in seconds. A nonzero value makes sense only when the action <code class="literal">name</code> is set to <code class="literal">monitor</code>. A recurring monitor action will be executed immediately after a resource start completes, and subsequent monitor actions are scheduled starting at the time the previous monitor action completed. For example, if a monitor action with <code class="literal">interval=20s</code> is executed at 01:00:00, the next monitor action does not occur at 01:00:20, but at 20 seconds after the first monitor action completes.
						</p>
						 <p>
							If set to zero, which is the default value, this parameter allows you to provide values to be used for operations created by the cluster. For example, if the <code class="literal">interval</code> is set to zero, the <code class="literal">name</code> of the operation is set to <code class="literal">start</code>, and the <code class="literal">timeout</code> value is set to 40, then Pacemaker will use a timeout of 40 seconds when starting this resource. A <code class="literal">monitor</code> operation with a zero interval allows you to set the <code class="literal">timeout</code>/<code class="literal">on-fail</code>/<code class="literal">enabled</code> values for the probes that Pacemaker does at startup to get the current status of all resources when the defaults are not desirable.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140319459044560"> <p>
							<code class="literal">timeout</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140319459043472"> <p>
							If the operation does not complete in the amount of time set by this parameter, abort the operation and consider it failed. The default value is the value of <code class="literal">timeout</code> if set with the <code class="literal command">pcs resource op defaults</code> command, or 20 seconds if it is not set. If you find that your system includes a resource that requires more time than the system allows to perform an operation (such as <code class="literal">start</code>, <code class="literal">stop</code>, or <code class="literal">monitor</code>), investigate the cause and if the lengthy execution time is expected you can increase this value.
						</p>
						 <p>
							The <code class="literal">timeout</code> value is not a delay of any kind, nor does the cluster wait the entire timeout period if the operation returns before the timeout period has completed.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140319459044560"> <p>
							<code class="literal">on-fail</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140319459043472"> <p>
							The action to take if this action ever fails. Allowed values:
						</p>
						 <p>
							* <code class="literal">ignore</code> - Pretend the resource did not fail
						</p>
						 <p>
							* <code class="literal">block</code> - Do not perform any further operations on the resource
						</p>
						 <p>
							* <code class="literal">stop</code> - Stop the resource and do not start it elsewhere
						</p>
						 <p>
							* <code class="literal">restart</code> - Stop the resource and start it again (possibly on a different node)
						</p>
						 <p>
							* <code class="literal">fence</code> - STONITH the node on which the resource failed
						</p>
						 <p>
							* <code class="literal">standby</code> - Move <span class="emphasis"><em>all</em></span> resources away from the node on which the resource failed
						</p>
						 <p>
							The default for the <code class="literal">stop</code> operation is <code class="literal">fence</code> when STONITH is enabled and <code class="literal">block</code> otherwise. All other operations default to <code class="literal">restart</code>.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140319459044560"> <p>
							<code class="literal">enabled</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140319459043472"> <p>
							If <code class="literal">false</code>, the operation is treated as if it does not exist. Allowed values: <code class="literal">true</code>, <code class="literal">false</code>
						</p>
						 </td></tr></tbody></table></div></div><section class="section" id="proc_configuring-resource-monitoring-operations-resource-monitoring-operations"><div class="titlepage"><div><div><h2 class="title">20.1. Configuring resource monitoring operations</h2></div></div></div><p>
				You can configure monitoring operations when you create a resource, using the following command.
			</p><pre class="literallayout">pcs resource create <span class="emphasis"><em>resource_id</em></span> <span class="emphasis"><em>standard:provider:type|type</em></span> [<span class="emphasis"><em>resource_options</em></span>] [op <span class="emphasis"><em>operation_action</em></span> <span class="emphasis"><em>operation_options</em></span> [<span class="emphasis"><em>operation_type</em></span> <span class="emphasis"><em>operation_options</em></span>]...]</pre><p>
				For example, the following command creates an <code class="literal">IPaddr2</code> resource with a monitoring operation. The new resource is called <code class="literal">VirtualIP</code> with an IP address of 192.168.0.99 and a netmask of 24 on <code class="literal">eth2</code>. A monitoring operation will be performed every 30 seconds.
			</p><pre class="literallayout"># <code class="literal">pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.99 cidr_netmask=24 nic=eth2 op monitor interval=30s</code></pre><p>
				Alternately, you can add a monitoring operation to an existing resource with the following command.
			</p><pre class="literallayout">pcs resource op add <span class="emphasis"><em>resource_id</em></span> <span class="emphasis"><em>operation_action</em></span> [<span class="emphasis"><em>operation_properties</em></span>]</pre><p>
				Use the following command to delete a configured resource operation.
			</p><pre class="literallayout">pcs resource op remove <span class="emphasis"><em>resource_id</em></span> <span class="emphasis"><em>operation_name</em></span> <span class="emphasis"><em>operation_properties</em></span></pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You must specify the exact operation properties to properly remove an existing operation.
				</p></div></div><p>
				To change the values of a monitoring option, you can update the resource. For example, you can create a <code class="literal">VirtualIP</code> with the following command.
			</p><pre class="literallayout"># <code class="literal">pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.99 cidr_netmask=24 nic=eth2</code></pre><p>
				By default, this command creates these operations.
			</p><pre class="literallayout">Operations: start interval=0s timeout=20s (VirtualIP-start-timeout-20s)
            stop interval=0s timeout=20s (VirtualIP-stop-timeout-20s)
            monitor interval=10s timeout=20s (VirtualIP-monitor-interval-10s)</pre><p>
				To change the stop timeout operation, execute the following command.
			</p><pre class="literallayout"># <code class="literal">pcs resource update VirtualIP op stop interval=0s timeout=40s</code>

# <code class="literal">pcs resource show VirtualIP</code>
 Resource: VirtualIP (class=ocf provider=heartbeat type=IPaddr2)
  Attributes: ip=192.168.0.99 cidr_netmask=24 nic=eth2
  Operations: start interval=0s timeout=20s (VirtualIP-start-timeout-20s)
              monitor interval=10s timeout=20s (VirtualIP-monitor-interval-10s)
              stop interval=0s timeout=40s (VirtualIP-name-stop-interval-0s-timeout-40s)</pre></section><section class="section" id="proc_configuring-global-resource-operation-defaults-resource-monitoring-operations"><div class="titlepage"><div><div><h2 class="title">20.2. Configuring global resource operation defaults</h2></div></div></div><p>
				You can use the following command to set global default values for monitoring operations.
			</p><pre class="literallayout">pcs resource op defaults [<span class="emphasis"><em>options</em></span>]</pre><p>
				For example, the following command sets a global default of a <code class="literal">timeout</code> value of 240 seconds for all monitoring operations.
			</p><pre class="literallayout"># <code class="literal">pcs resource op defaults timeout=240s</code></pre><p>
				To display the currently configured default values for monitoring operations, do not specify any options when you execute the <code class="literal command">pcs resource op defaults</code> command.
			</p><p>
				For example, following command displays the default monitoring operation values for a cluster which has been configured with a <code class="literal">timeout</code> value of 240 seconds.
			</p><pre class="literallayout"># <code class="literal">pcs resource op defaults</code>
timeout: 240s</pre><p>
				Note that a cluster resource will use the global default only when the option is not specified in the cluster resource definition. By default, resource agents define the <code class="literal">timeout</code> option for all operations. For the global operation timeout value to be honored, you must create the cluster resource without the <code class="literal">timeout</code> option explicitly or you must remove the <code class="literal">timeout</code> option by updating the cluster resource, as in the following command.
			</p><pre class="literallayout"># <code class="literal">pcs resource update VirtualIP op monitor interval=10s</code></pre><p>
				For example, after setting a global default of a <code class="literal">timeout</code> value of 240 seconds for all monitoring operations and updating the cluster resource <code class="literal">VirtualIP</code> to remove the timeout value for the <code class="literal">monitor</code> operation, the resource <code class="literal">VirtualIP</code> will then have timeout values for <code class="literal">start</code>, <code class="literal">stop</code>, and <code class="literal">monitor</code> operations of 20s, 40s and 240s, respectively. The global default value for timeout operations is applied here only on the <code class="literal">monitor</code> operation, where the default <code class="literal">timeout</code> option was removed by the previous command.
			</p><pre class="literallayout"># <code class="literal">pcs resource show VirtualIP</code>
 Resource: VirtualIP (class=ocf provider=heartbeat type=IPaddr2)
   Attributes: ip=192.168.0.99 cidr_netmask=24 nic=eth2
   Operations: start interval=0s timeout=20s (VirtualIP-start-timeout-20s)
               monitor interval=10s (VirtualIP-monitor-interval-10s)
               stop interval=0s timeout=40s (VirtualIP-name-stop-interval-0s-timeout-40s)</pre></section><section class="section" id="proc_configuring-multiple-monitoring-operations-resource-monitoring-operations"><div class="titlepage"><div><div><h2 class="title">20.3. Configuring multiple monitoring operations</h2></div></div></div><p>
				You can configure a single resource with as many monitor operations as a resource agent supports. In this way you can do a superficial health check every minute and progressively more intense ones at higher intervals.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					When configuring multiple monitor operations, you must ensure that no two operations are performed at the same interval.
				</p></div></div><p>
				To configure additional monitoring operations for a resource that supports more in-depth checks at different levels, you add an <code class="literal">OCF_CHECK_LEVEL=<span class="emphasis"><em>n</em></span></code> option.
			</p><p>
				For example, if you configure the following <code class="literal">IPaddr2</code> resource, by default this creates a monitoring operation with an interval of 10 seconds and a timeout value of 20 seconds.
			</p><pre class="literallayout"># <code class="literal">pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.99 cidr_netmask=24 nic=eth2</code></pre><p>
				If the Virtual IP supports a different check with a depth of 10, the following command causes Pacemaker to perform the more advanced monitoring check every 60 seconds in addition to the normal Virtual IP check every 10 seconds. (As noted, you should not configure the additional monitoring operation with a 10-second interval as well.)
			</p><pre class="literallayout"># <code class="literal">pcs resource op add VirtualIP monitor interval=60s OCF_CHECK_LEVEL=10</code></pre></section></section><section class="chapter" id="assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 21. Pacemaker cluster properties</h1></div></div></div><p>
			Cluster properties control how the cluster behaves when confronted with situations that may occur during cluster operation.
		</p><section class="section" id="ref_cluster-properties-options-controlling-cluster-behavior"><div class="titlepage"><div><div><h2 class="title">21.1. Summary of cluster properties and options</h2></div></div></div><p>
				<a class="xref" href="index.html#tb-clusterprops-HAAR" title="Table 21.1. Cluster Properties">Table 21.1, “Cluster Properties”</a> summaries the Pacemaker cluster properties, showing the default values of the properties and the possible values you can set for those properties.
			</p><p>
				There are additional cluster properties that determine fencing behavior. For information on these properties, see <a class="link" href="index.html#ref_advanced-fence-device-properties-configuring-fencing" title="9.4. Advanced fencing configuration options">Advanced fencing configuration options</a>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					In addition to the properties described in this table, there are additional cluster properties that are exposed by the cluster software. For these properties, it is recommended that you not change their values from their defaults.
				</p></div></div><div class="table" id="tb-clusterprops-HAAR"><p class="title"><strong>Table 21.1. Cluster Properties</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 29%; " class="col_1"><!--Empty--></col><col style="width: 29%; " class="col_2"><!--Empty--></col><col style="width: 43%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319460192704" scope="col">Option</th><th align="left" valign="top" id="idm140319460191616" scope="col">Default</th><th align="left" valign="top" id="idm140319460190528" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">batch-limit</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								0
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								The number of resource actions that the cluster is allowed to execute in parallel. The "correct" value will depend on the speed and load of your network and cluster nodes. The default value of 0 means that the cluster will dynamically impose a limit when any node has a high CPU load.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">migration-limit</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								-1 (unlimited)
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								The number of migration jobs that the cluster is allowed to execute in parallel on a node.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">no-quorum-policy</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								stop
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								What to do when the cluster does not have quorum. Allowed values:
							</p>
							 <p>
								* ignore - continue all resource management
							</p>
							 <p>
								* freeze - continue resource management, but do not recover resources from nodes not in the affected partition
							</p>
							 <p>
								* stop - stop all resources in the affected cluster partition
							</p>
							 <p>
								* suicide - fence all nodes in the affected cluster partition
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">symmetric-cluster</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								true
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								Indicates whether resources can run on any node by default.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">cluster-delay</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								Round trip delay over the network (excluding action execution). The "correct" value will depend on the speed and load of your network and cluster nodes.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">stop-orphan-resources</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								true
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								Indicates whether deleted resources should be stopped.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">stop-orphan-actions</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								true
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								Indicates whether deleted actions should be canceled.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">start-failure-is-fatal</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								true
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								Indicates whether a failure to start a resource on a particular node prevents further start attempts on that node. When set to <code class="literal">false</code>, the cluster will decide whether to try starting on the same node again based on the resource’s current failure count and migration threshold. For information on setting the <code class="literal">migration-threshold</code> option for a resource, see <a class="link" href="index.html#proc_configuring-resource-meta-options-configuring-cluster-resources" title="10.3. Configuring resource meta options">Configuring resource meta options</a>.
							</p>
							 <p>
								Setting <code class="literal">start-failure-is-fatal</code> to <code class="literal">false</code> incurs the risk that this will allow one faulty node that is unable to start a resource to hold up all dependent actions. This is why <code class="literal">start-failure-is-fatal</code> defaults to true. The risk of setting <code class="literal">start-failure-is-fatal=false</code> can be mitigated by setting a low migration threshold so that other actions can proceed after that many failures.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">pe-error-series-max</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								-1 (all)
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								The number of scheduler inputs resulting in ERRORs to save. Used when reporting problems.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">pe-warn-series-max</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								-1 (all)
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								The number of scheduler inputs resulting in WARNINGs to save. Used when reporting problems.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">pe-input-series-max</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								-1 (all)
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								The number of "normal" scheduler inputs to save. Used when reporting problems.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">cluster-infrastructure</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								The messaging stack on which Pacemaker is currently running. Used for informational and diagnostic purposes; not user-configurable.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">dc-version</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								Version of Pacemaker on the cluster’s Designated Controller (DC). Used for diagnostic purposes; not user-configurable.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">cluster-recheck-interval</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								15 minutes
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								Polling interval for time-based changes to options, resource parameters and constraints. Allowed values: Zero disables polling, positive values are an interval in seconds (unless other SI units are specified, such as 5min). Note that this value is the maximum time between checks; if a cluster event occurs sooner than the time specified by this value, the check will be done sooner.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">maintenance-mode</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								false
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								Maintenance Mode tells the cluster to go to a "hands off" mode, and not start or stop any services until told otherwise. When maintenance mode is completed, the cluster does a sanity check of the current state of any services, and then stops or starts any that need it.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">shutdown-escalation</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								20min
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								The time after which to give up trying to shut down gracefully and just exit. Advanced use only.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">stop-all-resources</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								false
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								Should the cluster stop all resources.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">enable-acl</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								false
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								Indicates whether the cluster can use access control lists, as set with the <code class="literal command">pcs acl</code> command.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319460192704"> <p>
								<code class="literal">placement-strategy</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460191616"> <p>
								<code class="literal">default</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319460190528"> <p>
								Indicates whether and how the cluster will take utilization attributes into account when determining resource placement on cluster nodes.
							</p>
							 </td></tr></tbody></table></div></div></section><section class="section" id="setting-cluster-properties-controlling-cluster-behavior"><div class="titlepage"><div><div><h2 class="title">21.2. Setting and removing cluster properties</h2></div></div></div><p>
				To set the value of a cluster property, use the following <span class="strong strong"><strong><span class="application application">pcs</span></strong></span> command.
			</p><pre class="literallayout">pcs property set <span class="emphasis"><em>property</em></span>=<span class="emphasis"><em>value</em></span></pre><p>
				For example, to set the value of <code class="literal">symmetric-cluster</code> to <code class="literal">false</code>, use the following command.
			</p><pre class="literallayout"># <code class="literal">pcs property set symmetric-cluster=false</code></pre><p>
				You can remove a cluster property from the configuration with the following command.
			</p><pre class="literallayout">pcs property unset <span class="emphasis"><em>property</em></span></pre><p>
				Alternately, you can remove a cluster property from a configuration by leaving the value field of the <code class="literal command">pcs property set</code> command blank. This restores that property to its default value. For example, if you have previously set the <code class="literal">symmetric-cluster</code> property to <code class="literal">false</code>, the following command removes the value you have set from the configuration and restores the value of <code class="literal">symmetric-cluster</code> to <code class="literal">true</code>, which is its default value.
			</p><pre class="literallayout"># <code class="literal">pcs property set symmetic-cluster=</code></pre></section><section class="section" id="proc_querying-cluster-property-settings-controlling-cluster-behavior"><div class="titlepage"><div><div><h2 class="title">21.3. Querying cluster property settings</h2></div></div></div><p>
				In most cases, when you use the <code class="literal command">pcs</code> command to display values of the various cluster components, you can use <code class="literal command">pcs list</code> or <code class="literal command">pcs show</code> interchangeably. In the following examples, <code class="literal command">pcs list</code> is the format used to display an entire list of all settings for more than one property, while <code class="literal command">pcs show</code> is the format used to display the values of a specific property.
			</p><p>
				To display the values of the property settings that have been set for the cluster, use the following <span class="strong strong"><strong><span class="application application">pcs</span></strong></span> command.
			</p><pre class="literallayout">pcs property list</pre><p>
				To display all of the values of the property settings for the cluster, including the default values of the property settings that have not been explicitly set, use the following command.
			</p><pre class="literallayout">pcs property list --all</pre><p>
				To display the current value of a specific cluster property, use the following command.
			</p><pre class="literallayout">pcs property show <span class="emphasis"><em>property</em></span></pre><p>
				For example, to display the current value of the <code class="literal">cluster-infrastructure</code> property, execute the following command:
			</p><pre class="literallayout"># <code class="literal">pcs property show cluster-infrastructure</code>
Cluster Properties:
 cluster-infrastructure: cman</pre><p>
				For informational purposes, you can display a list of all of the default values for the properties, whether they have been set to a value other than the default or not, by using the following command.
			</p><pre class="literallayout">pcs property [list|show] --defaults</pre></section></section><section class="chapter" id="assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 22. Configuring resources to remain stopped on clean node shutdown (RHEL 8.2 and later)</h1></div></div></div><p>
			When a cluster node shuts down, Pacemaker’s default response is to stop all resources running on that node and recover them elsewhere, even if the shutdown is a clean shutdown. As of RHEL 8.2, you can configure Pacemaker so that when a node shuts down cleanly, the resources attached to the node will be locked to the node and unable to start elsewhere until they start again when the node that has shut down rejoins the cluster. This allows you to power down nodes during maintenance windows when service outages are acceptable without causing that node’s resources to fail over to other nodes in the cluster.
		</p><section class="section" id="ref_cluster-properties-shutdown-lock-configuring-resources-to-remain-stopped"><div class="titlepage"><div><div><h2 class="title">22.1. Cluster properties to configure resources to remain stopped on clean node shutdown</h2></div></div></div><p>
				The ability to prevent resources from failing over on a clean node shutdown is implemented by means of the following cluster properties.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">shutdown-lock</code></span></dt><dd><p class="simpara">
							When this cluster property is set to the default value of <code class="literal">false</code>, the cluster will recover resources that are active on nodes being cleanly shut down. When this property is set to <code class="literal">true</code>, resources that are active on the nodes being cleanly shut down are unable to start elsewhere until they start on the node again after it rejoins the cluster.
						</p><p class="simpara">
							The <code class="literal">shutdown-lock</code> property will work for either cluster nodes or remote nodes, but not guest nodes.
						</p><p class="simpara">
							If <code class="literal">shutdown-lock</code> is set to <code class="literal">true</code>, you can remove the lock on the cluster resources when a node is down so that the resource can start elsewhere by performing a manual refresh on the node with the following command.
						</p><p class="simpara">
							<code class="literal command">pcs resource refresh <span class="emphasis"><em>resource</em></span> --node <span class="emphasis"><em>node</em></span></code>
						</p><p class="simpara">
							Note that once the resources are unlocked, the cluster is free to move the resources elsewhere. You can control the likelihood of this occurring by using stickiness values or location preferences for the resource.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								A manual refresh will work with remote nodes only if you first run the following commands:
							</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
										Run the <code class="literal command">systemctl stop pacemaker_remote</code> command on the remote node to stop the node.
									</li><li class="listitem">
										Run the <code class="literal command">pcs resource disable <span class="emphasis"><em>remote-connection-resource</em></span></code> command.
									</li></ol></div><p>
								You can then perform a manual refresh on the remote node.
							</p></div></div></dd><dt><span class="term"><code class="literal">shutdown-lock-limit</code></span></dt><dd><p class="simpara">
							When this cluster property is set to a time other than the default value of 0, resources will be available for recovery on other nodes if the node does not rejoin within the specified time since the shutdown was initiated.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The <code class="literal">shutdown-lock-limit</code> property will work with remote nodes only if you first run the following commands:
							</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
										Run the <code class="literal command">systemctl stop pacemaker_remote</code> command on the remote node to stop the node.
									</li><li class="listitem">
										Run the <code class="literal command">pcs resource disable <span class="emphasis"><em>remote-connection-resource</em></span></code> command.
									</li></ol></div><p>
								After you run these commands, the resources that had been running on the remote node will be available for recovery on other nodes when the amount of time specified as the <code class="literal">shutdown-lock-limit</code> has passed.
							</p></div></div></dd></dl></div></section><section class="section" id="proc_setting-shutdown-lock-configuring-resources-to-remain-stopped"><div class="titlepage"><div><div><h2 class="title">22.2. Setting the shutdown-lock cluster property</h2></div></div></div><p>
				The following example sets the <code class="literal">shutdown-lock</code> cluster property to <code class="literal">true</code> in an example cluster and shows the effect this has when the node is shut down and started again. This example cluster consists of three nodes: <code class="literal">z1.example.com</code>, <code class="literal">z2.example.com</code>, and <code class="literal">z3.example.com</code>.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Set the <code class="literal">shutdown-lock</code> property to to <code class="literal">true</code> and verify its value. In this example the <code class="literal">shutdown-lock-limit</code> property maintains its default value of 0.
					</p><pre class="literallayout">[<a class="link" href="mailto:root@z3.example.com">root@z3.example.com</a> ~]# <code class="literal">pcs property set shutdown-lock=true</code>
[<a class="link" href="mailto:root@z3.example.com">root@z3.example.com</a> ~]# <code class="literal">pcs property list --all | grep shutdown-lock</code>
 shutdown-lock: true
 shutdown-lock-limit: 0</pre></li><li class="listitem"><p class="simpara">
						Check the status of the cluster. In this example, resources <code class="literal">third</code> and <code class="literal">fifth</code> are running on <code class="literal">z1.example.com</code>.
					</p><pre class="literallayout">[<a class="link" href="mailto:root@z3.example.com">root@z3.example.com</a> ~]# <code class="literal">pcs status</code>
...
Full List of Resources:
...
 * first	(ocf::pacemaker:Dummy):	Started z3.example.com
 * second	(ocf::pacemaker:Dummy):	Started z2.example.com
 * third	(ocf::pacemaker:Dummy):	Started z1.example.com
 * fourth	(ocf::pacemaker:Dummy):	Started z2.example.com
 * fifth	(ocf::pacemaker:Dummy):	Started z1.example.com
...</pre></li><li class="listitem"><p class="simpara">
						Shut down <code class="literal">z1.example.com</code>, which will stop the resources that are running on that node.
					</p><pre class="literallayout">[<a class="link" href="mailto:root@z3.example.com">root@z3.example.com</a> ~] <code class="literal"># pcs cluster stop z1.example.com</code>
Stopping Cluster (pacemaker)...
Stopping Cluster (corosync)...</pre></li><li class="listitem"><p class="simpara">
						Running the <code class="literal">pcs status</code> command shows that node <code class="literal">z1.example.com</code> is offline and that the resources that had been running on <code class="literal">z1.example.com</code> are <code class="literal">LOCKED</code> while the node is down.
					</p><pre class="literallayout">[<a class="link" href="mailto:root@z3.example.com">root@z3.example.com</a> ~]# <code class="literal">pcs status</code>
...

Node List:
 * Online: [ z2.example.com z3.example.com ]
 * OFFLINE: [ z1.example.com ]

Full List of Resources:
...
 * first	(ocf::pacemaker:Dummy):	Started z3.example.com
 * second	(ocf::pacemaker:Dummy):	Started z2.example.com
 * third	(ocf::pacemaker:Dummy):	Stopped z1.example.com (LOCKED)
 * fourth	(ocf::pacemaker:Dummy):	Started z3.example.com
 * fifth	(ocf::pacemaker:Dummy):	Stopped z1.example.com (LOCKED)

...</pre></li><li class="listitem"><p class="simpara">
						Start cluster services again on <code class="literal">z1.example.com</code> so that it rejoins the cluster. Locked resources should get started on that node, although once they start they will not not necessarily remain on the same node.
					</p><pre class="literallayout">[<a class="link" href="mailto:root@z3.example.com">root@z3.example.com</a> ~]# <code class="literal">pcs cluster start z1.example.com</code>
Starting Cluster...</pre></li><li class="listitem"><p class="simpara">
						In this example, resouces <code class="literal">third</code> and <code class="literal">fifth</code> are recovered on node <code class="literal">z1.example.com</code>.
					</p><pre class="literallayout">[<a class="link" href="mailto:root@z3.example.com">root@z3.example.com</a> ~]# <code class="literal">pcs status</code>
...

Node List:
 * Online: [ z1.example.com z2.example.com z3.example.com ]

Full List of Resources:
..
 * first	(ocf::pacemaker:Dummy):	Started z3.example.com
 * second	(ocf::pacemaker:Dummy):	Started z2.example.com
 * third	(ocf::pacemaker:Dummy):	Started z1.example.com
 * fourth	(ocf::pacemaker:Dummy):	Started z3.example.com
 * fifth	(ocf::pacemaker:Dummy):	Started z1.example.com

...</pre></li></ol></div></section></section><section class="chapter" id="assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 23. Configuring a node placement strategy</h1></div></div></div><p>
			Pacemaker decides where to place a resource according to the resource allocation scores on every node. The resource will be allocated to the node where the resource has the highest score. This allocation score is derived from a combination of factors, including resource constraints, <code class="literal">resource-stickiness</code> settings, prior failure history of a resource on each node, and utilization of each node.
		</p><p>
			If the resource allocation scores on all the nodes are equal, by the default placement strategy Pacemaker will choose a node with the least number of allocated resources for balancing the load. If the number of resources on each node is equal, the first eligible node listed in the CIB will be chosen to run the resource.
		</p><p>
			Often, however, different resources use significantly different proportions of a node’s capacities (such as memory or I/O). You cannot always balance the load ideally by taking into account only the number of resources allocated to a node. In addition, if resources are placed such that their combined requirements exceed the provided capacity, they may fail to start completely or they may run run with degraded performance. To take these factors into account, Pacemaker allows you to configure the following components:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					the capacity a particular node provides
				</li><li class="listitem">
					the capacity a particular resource requires
				</li><li class="listitem">
					an overall strategy for placement of resources
				</li></ul></div><section class="section" id="configuring-utilization-attributes-configuring-node-placement-strategy"><div class="titlepage"><div><div><h2 class="title">23.1. Utilization attributes and placement strategy</h2></div></div></div><p>
				To configure the capacity that a node provides or a resource requires, you can use <span class="emphasis"><em>utilization attributes</em></span> for nodes and resources. You do this by setting a utilization variable for a resource and assigning a value to that variable to indicate what the resource requires, and then setting that same utilization variable for a node and assigning a value to that variable to indicate what that node provides.
			</p><p>
				You can name utilization attributes according to your preferences and define as many name and value pairs as your configuration needs. The values of utilization attributes must be integers.
			</p><section class="section" id="configuring_node_and_resource_capacity"><div class="titlepage"><div><div><h3 class="title">23.1.1. Configuring node and resource capacity</h3></div></div></div><p>
					The following example configures a utilization attribute of CPU capacity for two nodes, setting this attribute as the variable <code class="literal">cpu</code>. It also configures a utilization attribute of RAM capacity, setting this attribute as the variable <code class="literal">memory</code>. In this example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Node 1 is defined as providing a CPU capacity of two and a RAM capacity of 2048
						</li><li class="listitem">
							Node 2 is defined as providing a CPU capacity of four and a RAM capacity of 2048
						</li></ul></div><pre class="literallayout"># <code class="literal">pcs node utilization node1 cpu=2 memory=2048</code>
# <code class="literal">pcs node utilization node2 cpu=4 memory=2048</code></pre><p>
					The following example specifies the same utilization attributes that three different resources require. In this example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							resource <code class="literal">dummy-small</code> requires a CPU capacity of 1 and a RAM capacity of 1024
						</li><li class="listitem">
							resource <code class="literal">dummy-medium</code> requires a CPU capacity of 2 and a RAM capacity of 2048
						</li><li class="listitem">
							resource <code class="literal">dummy-large</code> requires a CPU capacity of 1 and a RAM capacity of 3072
						</li></ul></div><pre class="literallayout"># <code class="literal">pcs resource utilization dummy-small cpu=1 memory=1024</code>
# <code class="literal">pcs resource utilization dummy-medium cpu=2 memory=2048</code>
# <code class="literal">pcs resource utilization dummy-large cpu=3 memory=3072</code></pre><p>
					A node is considered eligible for a resource if it has sufficient free capacity to satisfy the resource’s requirements, as defined by the utilization attributes.
				</p></section><section class="section" id="configuring_placement_strategy"><div class="titlepage"><div><div><h3 class="title">23.1.2. Configuring placement strategy</h3></div></div></div><p>
					After you have configured the capacities your nodes provide and the capacities your resources require, you need to set the <code class="literal">placement-strategy</code> cluster property, otherwise the capacity configurations have no effect.
				</p><p>
					Four values are available for the <code class="literal">placement-strategy</code> cluster property:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">default</code> — Utilization values are not taken into account at all. Resources are allocated according to allocation scores. If scores are equal, resources are evenly distributed across nodes.
						</li><li class="listitem">
							<code class="literal">utilization</code> — Utilization values are taken into account only when deciding whether a node is considered eligible (that is, whether it has sufficient free capacity to satisfy the resource’s requirements). Load-balancing is still done based on the number of resources allocated to a node.
						</li><li class="listitem">
							<code class="literal">balanced</code> — Utilization values are taken into account when deciding whether a node is eligible to serve a resource and when load-balancing, so an attempt is made to spread the resources in a way that optimizes resource performance.
						</li><li class="listitem">
							<code class="literal">minimal</code> — Utilization values are taken into account only when deciding whether a node is eligible to serve a resource. For load-balancing, an attempt is made to concentrate the resources on as few nodes as possible, thereby enabling possible power savings on the remaining nodes.
						</li></ul></div><p>
					The following example command sets the value of <code class="literal">placement-strategy</code> to <code class="literal">balanced</code>. After running this command, Pacemaker will ensure the load from your resources will be distributed evenly throughout the cluster, without the need for complicated sets of colocation constraints.
				</p><pre class="literallayout"># <code class="literal">pcs property set placement-strategy=balanced</code></pre></section></section><section class="section" id="pacemaker-resource-allocation-configuring-node-placement-strategy"><div class="titlepage"><div><div><h2 class="title">23.2. Pacemaker resource allocation</h2></div></div></div><p>
				The following subsections summarize how Pacemaker allocates resources.
			</p><section class="section" id="node_preference"><div class="titlepage"><div><div><h3 class="title">23.2.1. Node Preference</h3></div></div></div><p>
					Pacemaker determines which node is preferred when allocating resources according to the following strategy.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The node with the highest node weight gets consumed first. Node weight is a score maintained by the cluster to represent node health.
						</li><li class="listitem"><p class="simpara">
							If multiple nodes have the same node weight:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
									If the <code class="literal">placement-strategy</code> cluster property is <code class="literal">default</code> or <code class="literal">utilization</code>:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem">
											The node that has the least number of allocated resources gets consumed first.
										</li><li class="listitem">
											If the numbers of allocated resources are equal, the first eligible node listed in the CIB gets consumed first.
										</li></ul></div></li><li class="listitem"><p class="simpara">
									If the <code class="literal">placement-strategy</code> cluster property is <code class="literal">balanced</code>:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem">
											The node that has the most free capacity gets consumed first.
										</li><li class="listitem">
											If the free capacities of the nodes are equal, the node that has the least number of allocated resources gets consumed first.
										</li><li class="listitem">
											If the free capacities of the nodes are equal and the number of allocated resources is equal, the first eligible node listed in the CIB gets consumed first.
										</li></ul></div></li><li class="listitem">
									If the <code class="literal">placement-strategy</code> cluster property is <code class="literal">minimal</code>, the first eligible node listed in the CIB gets consumed first.
								</li></ul></div></li></ul></div></section><section class="section" id="node_capacity"><div class="titlepage"><div><div><h3 class="title">23.2.2. Node Capacity</h3></div></div></div><p>
					Pacemaker determines which node has the most free capacity according to the following strategy.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If only one type of utilization attribute has been defined, free capacity is a simple numeric comparison.
						</li><li class="listitem"><p class="simpara">
							If multiple types of utilization attributes have been defined, then the node that is numerically highest in the most attribute types has the most free capacity. For example:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									If NodeA has more free CPUs, and NodeB has more free memory, then their free capacities are equal.
								</li><li class="listitem">
									If NodeA has more free CPUs, while NodeB has more free memory and storage, then NodeB has more free capacity.
								</li></ul></div></li></ul></div></section><section class="section" id="resource_allocation_preference"><div class="titlepage"><div><div><h3 class="title">23.2.3. Resource Allocation Preference</h3></div></div></div><p>
					Pacemaker determines which resource is allocated first according to the following strategy.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The resource that has the highest priority gets allocated first. You can set a resource’s priority when you create the resource.
						</li><li class="listitem">
							If the priorities of the resources are equal, the resource that has the highest score on the node where it is running gets allocated first, to prevent resource shuffling.
						</li><li class="listitem">
							If the resource scores on the nodes where the resources are running are equal or the resources are not running, the resource that has the highest score on the preferred node gets allocated first. If the resource scores on the preferred node are equal in this case, the first runnable resource listed in the CIB gets allocated first.
						</li></ul></div></section></section><section class="section" id="resource-placement-strategy-guidelines-configuring-node-placement-strategy"><div class="titlepage"><div><div><h2 class="title">23.3. Resource placement strategy guidelines</h2></div></div></div><p>
				To ensure that Pacemaker’s placement strategy for resources works most effectively, you should take the following considerations into account when configuring your system.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Make sure that you have sufficient physical capacity.
					</p><p class="simpara">
						If the physical capacity of your nodes is being used to near maximum under normal conditions, then problems could occur during failover. Even without the utilization feature, you may start to experience timeouts and secondary failures.
					</p></li><li class="listitem"><p class="simpara">
						Build some buffer into the capabilities you configure for the nodes.
					</p><p class="simpara">
						Advertise slightly more node resources than you physically have, on the assumption the that a Pacemaker resource will not use 100% of the configured amount of CPU, memory, and so forth all the time. This practice is sometimes called overcommit.
					</p></li><li class="listitem"><p class="simpara">
						Specify resource priorities.
					</p><p class="simpara">
						If the cluster is going to sacrifice services, it should be the ones you care about least. Ensure that resource priorities are properly set so that your most important resources are scheduled first.
					</p></li></ul></div></section><section class="section" id="node-utilization-resource-agent-configuring-node-placement-strategy"><div class="titlepage"><div><div><h2 class="title">23.4. The NodeUtilization resource agent</h2></div></div></div><p>
				The NodeUtilization agent can detect the system parameters of available CPU, host memory availability, and hypervisor memory availability and add these parameters into the CIB. You can run the agent as a clone resource to have it automatically populate these parameters on each node.
			</p><p>
				For information on the <code class="literal">NodeUtilization</code> resource agent and the resource options for this agent, run the <code class="literal command">pcs resource describe NodeUtilization</code> command.
			</p></section></section><section class="chapter" id="assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 24. Configuring a virtual domain as a resource</h1></div></div></div><p>
			You can configure a virtual domain that is managed by the <code class="literal">libvirt</code> virtualization framework as a cluster resource with the <code class="literal command">pcs resource create</code> command, specifying <code class="literal">VirtualDomain</code> as the resource type.
		</p><p>
			When configuring a virtual domain as a resource, take the following considerations into account:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					A virtual domain should be stopped before you configure it as a cluster resource.
				</li><li class="listitem">
					Once a virtual domain is a cluster resource, it should not be started, stopped, or migrated except through the cluster tools.
				</li><li class="listitem">
					Do not configure a virtual domain that you have configured as a cluster resource to start when its host boots.
				</li><li class="listitem">
					All nodes allowed to run a virtual domain must have access to the necessary configuration files and storage devices for that virtual domain.
				</li></ul></div><p>
			If you want the cluster to manage services within the virtual domain itself, you can configure the virtual domain as a guest node.
		</p><section class="section" id="ref_virtual-domain-resource-options-configuring-virtual-domain-as-a-resource"><div class="titlepage"><div><div><h2 class="title">24.1. Virtual domain resource options</h2></div></div></div><p>
				<a class="xref" href="index.html#tb-virtdomain-options-HAAR" title="Table 24.1. Resource Options for Virtual Domain Resources">Table 24.1, “Resource Options for Virtual Domain Resources”</a> describes the resource options you can configure for a <code class="literal">VirtualDomain</code> resource.
			</p><div class="table" id="tb-virtdomain-options-HAAR"><p class="title"><strong>Table 24.1. Resource Options for Virtual Domain Resources</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 38%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 38%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319458736992" scope="col">Field</th><th align="left" valign="top" id="idm140319458735904" scope="col">Default</th><th align="left" valign="top" id="idm140319458734816" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319458736992"> <p>
								<code class="literal">config</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458735904"> </td><td align="left" valign="top" headers="idm140319458734816"> <p>
								(required) Absolute path to the <code class="literal">libvirt</code> configuration file for this virtual domain.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458736992"> <p>
								<code class="literal">hypervisor</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458735904"> <p>
								System dependent
							</p>
							 </td><td align="left" valign="top" headers="idm140319458734816"> <p>
								Hypervisor URI to connect to. You can determine the system’s default URI by running the <code class="literal command">virsh --quiet uri</code> command.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458736992"> <p>
								<code class="literal">force_stop</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458735904"> <p>
								<code class="literal">0</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458734816"> <p>
								Always forcefully shut down ("destroy") the domain on stop. The default behavior is to resort to a forceful shutdown only after a graceful shutdown attempt has failed. You should set this to <code class="literal">true</code> only if your virtual domain (or your virtualization back end) does not support graceful shutdown.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458736992"> <p>
								<code class="literal">migration_transport</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458735904"> <p>
								System dependent
							</p>
							 </td><td align="left" valign="top" headers="idm140319458734816"> <p>
								Transport used to connect to the remote hypervisor while migrating. If this parameter is omitted, the resource will use <code class="literal">libvirt</code>'s default transport to connect to the remote hypervisor.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458736992"> <p>
								<code class="literal">migration_network_suffix</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458735904"> </td><td align="left" valign="top" headers="idm140319458734816"> <p>
								Use a dedicated migration network. The migration URI is composed by adding this parameter’s value to the end of the node name. If the node name is a fully qualified domain name (FQDN), insert the suffix immediately prior to the first period (.) in the FQDN. Ensure that this composed host name is locally resolvable and the associated IP address is reachable through the favored network.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458736992"> <p>
								<code class="literal">monitor_scripts</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458735904"> </td><td align="left" valign="top" headers="idm140319458734816"> <p>
								To additionally monitor services within the virtual domain, add this parameter with a list of scripts to monitor. <span class="emphasis"><em>Note</em></span>: When monitor scripts are used, the <code class="literal">start</code> and <code class="literal">migrate_from</code> operations will complete only when all monitor scripts have completed successfully. Be sure to set the timeout of these operations to accommodate this delay
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458736992"> <p>
								<code class="literal">autoset_utilization_cpu</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458735904"> <p>
								<code class="literal">true</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458734816"> <p>
								If set to <code class="literal">true</code>, the agent will detect the number of <code class="literal">domainU</code>'s <code class="literal">vCPU</code>s from <code class="literal">virsh</code>, and put it into the CPU utilization of the resource when the monitor is executed.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458736992"> <p>
								<code class="literal">autoset_utilization_hv_memory</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458735904"> <p>
								<code class="literal">true</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458734816"> <p>
								If set it true, the agent will detect the number of <code class="literal">Max memory</code> from <code class="literal">virsh</code>, and put it into the <code class="literal">hv_memory</code> utilization of the source when the monitor is executed.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458736992"> <p>
								<code class="literal">migrateport</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458735904"> <p>
								random highport
							</p>
							 </td><td align="left" valign="top" headers="idm140319458734816"> <p>
								This port will be used in the <code class="literal">qemu</code> migrate URI. If unset, the port will be a random highport.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458736992"> <p>
								<code class="literal">snapshot</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458735904"> </td><td align="left" valign="top" headers="idm140319458734816"> <p>
								Path to the snapshot directory where the virtual machine image will be stored. When this parameter is set, the virtual machine’s RAM state will be saved to a file in the snapshot directory when stopped. If on start a state file is present for the domain, the domain will be restored to the same state it was in right before it stopped last. This option is incompatible with the <code class="literal">force_stop</code> option.
							</p>
							 </td></tr></tbody></table></div></div><p>
				In addition to the <code class="literal">VirtualDomain</code> resource options, you can configure the <code class="literal">allow-migrate</code> metadata option to allow live migration of the resource to another node. When this option is set to <code class="literal">true</code>, the resource can be migrated without loss of state. When this option is set to <code class="literal">false</code>, which is the default state, the virtual domain will be shut down on the first node and then restarted on the second node when it is moved from one node to the other.
			</p></section><section class="section" id="proc_creating-virtual-domain-resource-configuring-virtual-domain-as-a-resource"><div class="titlepage"><div><div><h2 class="title">24.2. Creating the virtual domain resource</h2></div></div></div><p>
				Use the following procedure to create a <code class="literal">VirtualDomain</code> resource in a cluster for a virtual machine you have previously created:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To create the <code class="literal">VirtualDomain</code> resource agent for the management of the virtual machine, Pacemaker requires the virtual machine’s <code class="literal">xml</code> configuration file to be dumped to a file on disk. For example, if you created a virtual machine named <code class="literal">guest1</code>, dump the <code class="literal">xml</code> file to a file somewhere on one of the cluster nodes that will be allowed to run the guest. You can use a file name of your choosing; this example uses <code class="literal">/etc/pacemaker/guest1.xml</code>.
					</p><pre class="literallayout"># <code class="literal">virsh dumpxml guest1 &gt; /etc/pacemaker/guest1.xml</code></pre></li><li class="listitem">
						Copy the virtual machine’s <code class="literal">xml</code> configuration file to all of the other cluster nodes that will be allowed to run the guest, in the same location on each node.
					</li><li class="listitem">
						Ensure that all of the nodes allowed to run the virtual domain have access to the necessary storage devices for that virtual domain.
					</li><li class="listitem">
						Separately test that the virtual domain can start and stop on each node that will run the virtual domain.
					</li><li class="listitem">
						If it is running, shut down the guest node. Pacemaker will start the node when it is configured in the cluster. The virtual machine should not be configured to start automatically when the host boots.
					</li><li class="listitem"><p class="simpara">
						Configure the <code class="literal">VirtualDomain</code> resource with the <code class="literal command">pcs resource create</code> command. For example, the following command configures a <code class="literal">VirtualDomain</code> resource named <code class="literal">VM</code>. Since the <code class="literal">allow-migrate</code> option is set to <code class="literal">true</code> a <code class="literal">pcs move VM <span class="emphasis"><em>nodeX</em></span></code> command would be done as a live migration.
					</p><p class="simpara">
						In this example <code class="literal">migration_transport</code> is set to <code class="literal">ssh</code>. Note that for SSH migration to work properly, keyless logging must work between nodes.
					</p><pre class="literallayout"># <code class="literal">pcs resource create VM VirtualDomain config=/etc/pacemaker/guest1.xml migration_transport=ssh meta allow-migrate=true</code></pre></li></ol></div></section></section><section class="chapter" id="assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 25. Cluster quorum</h1></div></div></div><p>
			A Red Hat Enterprise Linux High Availability Add-On cluster uses the <code class="literal">votequorum</code> service, in conjunction with fencing, to avoid split brain situations. A number of votes is assigned to each system in the cluster, and cluster operations are allowed to proceed only when a majority of votes is present. The service must be loaded into all nodes or none; if it is loaded into a subset of cluster nodes, the results will be unpredictable. For information on the configuration and operation of the <code class="literal">votequorum</code> service, see the <code class="literal command">votequorum</code>(5) man page.
		</p><section class="section" id="ref_quorum-options-configuring-cluster-quorum"><div class="titlepage"><div><div><h2 class="title">25.1. Configuring quorum options</h2></div></div></div><p>
				There are some special features of quorum configuration that you can set when you create a cluster with the <code class="literal command">pcs cluster setup</code> command. <a class="xref" href="index.html#tb-quorumoptions-HAAR" title="Table 25.1. Quorum Options">Table 25.1, “Quorum Options”</a> summarizes these options.
			</p><div class="table" id="tb-quorumoptions-HAAR"><p class="title"><strong>Table 25.1. Quorum Options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 44%; " class="col_1"><!--Empty--></col><col style="width: 56%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319458647152" scope="col">Option</th><th align="left" valign="top" id="idm140319458646064" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319458647152"> <p>
								<code class="literal">auto_tie_breaker</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458646064"> <p>
								When enabled, the cluster can suffer up to 50% of the nodes failing at the same time, in a deterministic fashion. The cluster partition, or the set of nodes that are still in contact with the <code class="literal">nodeid</code> configured in <code class="literal">auto_tie_breaker_node</code> (or lowest <code class="literal">nodeid</code> if not set), will remain quorate. The other nodes will be inquorate.
							</p>
							 <p>
								The <code class="literal option">auto_tie_breaker</code> option is principally used for clusters with an even number of nodes, as it allows the cluster to continue operation with an even split. For more complex failures, such as multiple, uneven splits, it is recommended that you use a quorum device, as described in <a class="link" href="index.html#assembly_configuring-quorum-devices-configuring-cluster-quorum" title="25.5. Quorum devices">Quorum devices</a>.
							</p>
							 <p>
								The <code class="literal option">auto_tie_breaker</code> option is incompatible with quorum devices.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458647152"> <p>
								<code class="literal">wait_for_all</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458646064"> <p>
								When enabled, the cluster will be quorate for the first time only after all nodes have been visible at least once at the same time.
							</p>
							 <p>
								The <code class="literal option">wait_for_all</code> option is primarily used for two-node clusters and for even-node clusters using the quorum device <code class="literal">lms</code> (last man standing) algorithm.
							</p>
							 <p>
								The <code class="literal option">wait_for_all</code> option is automatically enabled when a cluster has two nodes, does not use a quorum device, and <code class="literal option">auto_tie_breaker</code> is disabled. You can override this by explicitly setting <code class="literal option">wait_for_all</code> to 0.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458647152"> <p>
								<code class="literal">last_man_standing</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458646064"> <p>
								When enabled, the cluster can dynamically recalculate <code class="literal">expected_votes</code> and quorum under specific circumstances. You must enable <code class="literal">wait_for_all</code> when you enable this option. The <code class="literal">last_man_standing</code> option is incompatible with quorum devices.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458647152"> <p>
								<code class="literal">last_man_standing_window</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319458646064"> <p>
								The time, in milliseconds, to wait before recalculating <code class="literal">expected_votes</code> and quorum after a cluster loses nodes.
							</p>
							 </td></tr></tbody></table></div></div><p>
				For further information about configuring and using these options, see the <code class="literal command">votequorum</code>(5) man page.
			</p></section><section class="section" id="proc_modifying-quorum-options-configuring-cluster-quorum"><div class="titlepage"><div><div><h2 class="title">25.2. Modifying quorum options</h2></div></div></div><p>
				You can modify general quorum options for your cluster with the <code class="literal command">pcs quorum update</code> command. Executing this command requires that the cluster be stopped. For information on the quorum options, see the <code class="literal command">votequorum</code>(5) man page.
			</p><p>
				The format of the <code class="literal command">pcs quorum update</code> command is as follows.
			</p><pre class="literallayout">pcs quorum update [auto_tie_breaker=[0|1]] [last_man_standing=[0|1]] [last_man_standing_window=[<span class="emphasis"><em>time-in-ms</em></span>] [wait_for_all=[0|1]]</pre><p>
				The following series of commands modifies the <code class="literal">wait_for_all</code> quorum option and displays the updated status of the option. Note that the system does not allow you to execute this command while the cluster is running.
			</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum update wait_for_all=1</code>
Checking corosync is not running on nodes...
Error: node1: corosync is running
Error: node2: corosync is running

[root@node1:~]# <code class="literal">pcs cluster stop --all</code>
node2: Stopping Cluster (pacemaker)...
node1: Stopping Cluster (pacemaker)...
node1: Stopping Cluster (corosync)...
node2: Stopping Cluster (corosync)...

[root@node1:~]# <code class="literal">pcs quorum update wait_for_all=1</code>
Checking corosync is not running on nodes...
node2: corosync is not running
node1: corosync is not running
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded

[root@node1:~]# <code class="literal">pcs quorum config</code>
Options:
  wait_for_all: 1</pre></section><section class="section" id="proc_displaying-quorum-configuration-status-configuring-cluster-quorum"><div class="titlepage"><div><div><h2 class="title">25.3. Displaying quorum configuration and status</h2></div></div></div><p>
				Once a cluster is running, you can enter the following cluster quorum commands.
			</p><p>
				The following command shows the quorum configuration.
			</p><pre class="literallayout">pcs quorum [config]</pre><p>
				The following command shows the quorum runtime status.
			</p><pre class="literallayout">pcs quorum status</pre></section><section class="section" id="proc_running-inquorate-clusters-configuring-cluster-quorum"><div class="titlepage"><div><div><h2 class="title">25.4. Running inquorate clusters</h2></div></div></div><p>
				If you take nodes out of a cluster for a long period of time and the loss of those nodes would cause quorum loss, you can change the value of the <code class="literal">expected_votes</code> parameter for the live cluster with the <code class="literal command">pcs quorum expected-votes</code> command. This allows the cluster to continue operation when it does not have quorum.
			</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					Changing the expected votes in a live cluster should be done with extreme caution. If less than 50% of the cluster is running because you have manually changed the expected votes, then the other nodes in the cluster could be started separately and run cluster services, causing data corruption and other unexpected results. If you change this value, you should ensure that the <code class="literal">wait_for_all</code> parameter is enabled.
				</p></div></div><p>
				The following command sets the expected votes in the live cluster to the specified value. This affects the live cluster only and does not change the configuration file; the value of <code class="literal">expected_votes</code> is reset to the value in the configuration file in the event of a reload.
			</p><pre class="literallayout">pcs quorum expected-votes <span class="emphasis"><em>votes</em></span></pre><p>
				In a situation in which you know that the cluster is inquorate but you want the cluster to proceed with resource management, you can use the <code class="literal command">pcs quorum unblock</code> command to prevent the cluster from waiting for all nodes when establishing quorum.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					This command should be used with extreme caution. Before issuing this command, it is imperative that you ensure that nodes that are not currently in the cluster are switched off and have no access to shared resources.
				</p></div></div><pre class="literallayout"># <code class="literal">pcs quorum unblock</code></pre></section><section class="section" id="assembly_configuring-quorum-devices-configuring-cluster-quorum"><div class="titlepage"><div><div><h2 class="title">25.5. Quorum devices</h2></div></div></div><p>
				You can allow a cluster to sustain more node failures than standard quorum rules allows by configuring a separate quorum device which acts as a third-party arbitration device for the cluster. A quorum device is recommended for clusters with an even number of nodes and highly recommended for two-node clusters.
			</p><p>
				You must take the following into account when configuring a quorum device.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						It is recommended that a quorum device be run on a different physical network at the same site as the cluster that uses the quorum device. Ideally, the quorum device host should be in a separate rack than the main cluster, or at least on a separate PSU and not on the same network segment as the corosync ring or rings.
					</li><li class="listitem">
						You cannot use more than one quorum device in a cluster at the same time.
					</li><li class="listitem">
						Although you cannot use more than one quorum device in a cluster at the same time, a single quorum device may be used by several clusters at the same time. Each cluster using that quorum device can use different algorithms and quorum options, as those are stored on the cluster nodes themselves. For example, a single quorum device can be used by one cluster with an <code class="literal">ffsplit</code> (fifty/fifty split) algorithm and by a second cluster with an <code class="literal">lms</code> (last man standing) algorithm.
					</li><li class="listitem">
						A quorum device should not be run on an existing cluster node.
					</li></ul></div><section class="section" id="proc_installing-quorum-device-packages-configuring-quorum-devices"><div class="titlepage"><div><div><h3 class="title">25.5.1. Installing quorum device packages</h3></div></div></div><p>
					Configuring a quorum device for a cluster requires that you install the following packages:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Install <code class="literal">corosync-qdevice</code> on the nodes of an existing cluster.
						</p><pre class="literallayout">[root@node1:~]# <code class="literal">yum install corosync-qdevice</code>
[root@node2:~]# <code class="literal">yum install corosync-qdevice</code></pre></li><li class="listitem"><p class="simpara">
							Install <code class="literal">pcs</code> and <code class="literal">corosync-qnetd</code> on the quorum device host.
						</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">yum install pcs corosync-qnetd</code></pre></li><li class="listitem"><p class="simpara">
							Start the <code class="literal">pcsd</code> service and enable <code class="literal">pcsd</code> at system start on the quorum device host.
						</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">systemctl start pcsd.service</code>
[root@qdevice:~]# <code class="literal">systemctl enable pcsd.service</code></pre></li></ul></div></section><section class="section" id="proc_configuring-quorum-device-configuring-quorum-devices"><div class="titlepage"><div><div><h3 class="title">25.5.2. Configuring a quorum device</h3></div></div></div><p>
					The following procedure configures a quorum device and adds it to the cluster. In this example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The node used for a quorum device is <code class="literal">qdevice</code>.
						</li><li class="listitem"><p class="simpara">
							The quorum device model is <code class="literal">net</code>, which is currently the only supported model. The <code class="literal">net</code> model supports the following algorithms:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">ffsplit</code>: fifty-fifty split. This provides exactly one vote to the partition with the highest number of active nodes.
								</li><li class="listitem"><p class="simpara">
									<code class="literal">lms</code>: last-man-standing. If the node is the only one left in the cluster that can see the <code class="literal">qnetd</code> server, then it returns a vote.
								</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
										The LMS algorithm allows the cluster to remain quorate even with only one remaining node, but it also means that the voting power of the quorum device is great since it is the same as number_of_nodes - 1. Losing connection with the quorum device means losing number_of_nodes - 1 votes, which means that only a cluster with all nodes active can remain quorate (by overvoting the quorum device); any other cluster becomes inquorate.
									</p></div></div><p class="simpara">
									For more detailed information on the implementation of these algorithms, see the <code class="literal">corosync-qdevice</code>(8) man page.
								</p></li></ul></div></li><li class="listitem">
							The cluster nodes are <code class="literal">node1</code> and <code class="literal">node2</code>.
						</li></ul></div><p>
					The following procedure configures a quorum device and adds that quorum device to a cluster.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On the node that you will use to host your quorum device, configure the quorum device with the following command. This command configures and starts the quorum device model <code class="literal">net</code> and configures the device to start on boot.
						</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">pcs qdevice setup model net --enable --start</code>
Quorum device 'net' initialized
quorum device enabled
Starting quorum device...
quorum device started</pre><p class="simpara">
							After configuring the quorum device, you can check its status. This should show that the <code class="literal">corosync-qnetd</code> daemon is running and, at this point, there are no clients connected to it. The <code class="literal option">--full</code> command option provides detailed output.
						</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">pcs qdevice status net --full</code>
QNetd address:                  *:5403
TLS:                            Supported (client certificate required)
Connected clients:              0
Connected clusters:             0
Maximum send/receive size:      32768/32768 bytes</pre></li><li class="listitem"><p class="simpara">
							Enable the ports on the firewall needed by the <code class="literal">pcsd</code> daemon and the <code class="literal">net</code> quorum device by enabling the <code class="literal">high-availability</code> service on <code class="literal">firewalld</code> with following commands.
						</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">firewall-cmd --permanent --add-service=high-availability</code>
[root@qdevice:~]# <code class="literal">firewall-cmd --add-service=high-availability</code></pre></li><li class="listitem"><p class="simpara">
							From one of the nodes in the existing cluster, authenticate user <code class="literal">hacluster</code> on the node that is hosting the quorum device. This allows <code class="literal">pcs</code> on the cluster to connect to <code class="literal">pcs</code> on the <code class="literal">qdevice</code> host, but does not allow <code class="literal">pcs</code> on the <code class="literal">qdevice</code> host to connect to <code class="literal">pcs</code> on the cluster.
						</p><pre class="literallayout">[root@node1:~] # <code class="literal">pcs host auth qdevice</code>
Username: hacluster
Password:
qdevice: Authorized</pre></li><li class="listitem"><p class="simpara">
							Add the quorum device to the cluster.
						</p><p class="simpara">
							Before adding the quorum device, you can check the current configuration and status for the quorum device for later comparison. The output for these commands indicates that the cluster is not yet using a quorum device.
						</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum config</code>
Options:</pre><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum status</code>
Quorum information
------------------
Date:             Wed Jun 29 13:15:36 2016
Quorum provider:  corosync_votequorum
Nodes:            2
Node ID:          1
Ring ID:          1/8272
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   2
Highest expected: 2
Total votes:      2
Quorum:           1
Flags:            2Node Quorate

Membership information
----------------------
    Nodeid      Votes    Qdevice Name
         1          1         NR node1 (local)
         2          1         NR node2</pre><p class="simpara">
							The following command adds the quorum device that you have previously created to the cluster. You cannot use more than one quorum device in a cluster at the same time. However, one quorum device can be used by several clusters at the same time. This example command configures the quorum device to use the <code class="literal">ffsplit</code> algorithm. For information on the configuration options for the quorum device, see the <code class="literal">corosync-qdevice</code>(8) man page.
						</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum device add model net host=qdevice algorithm=ffsplit</code>
Setting up qdevice certificates on nodes...
node2: Succeeded
node1: Succeeded
Enabling corosync-qdevice...
node1: corosync-qdevice enabled
node2: corosync-qdevice enabled
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded
Corosync configuration reloaded
Starting corosync-qdevice...
node1: corosync-qdevice started
node2: corosync-qdevice started</pre></li><li class="listitem"><p class="simpara">
							Check the configuration status of the quorum device.
						</p><p class="simpara">
							From the cluster side, you can execute the following commands to see how the configuration has changed.
						</p><p class="simpara">
							The <code class="literal command">pcs quorum config</code> shows the quorum device that has been configured.
						</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum config</code>
Options:
Device:
  Model: net
    algorithm: ffsplit
    host: qdevice</pre><p class="simpara">
							The <code class="literal command">pcs quorum status</code> command shows the quorum runtime status, indicating that the quorum device is in use.
						</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum status</code>
Quorum information
------------------
Date:             Wed Jun 29 13:17:02 2016
Quorum provider:  corosync_votequorum
Nodes:            2
Node ID:          1
Ring ID:          1/8272
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   3
Highest expected: 3
Total votes:      3
Quorum:           2
Flags:            Quorate Qdevice

Membership information
----------------------
    Nodeid      Votes    Qdevice Name
         1          1    A,V,NMW node1 (local)
         2          1    A,V,NMW node2
         0          1            Qdevice</pre><p class="simpara">
							The <code class="literal command">pcs quorum device status</code> shows the quorum device runtime status.
						</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum device status</code>
Qdevice information
-------------------
Model:                  Net
Node ID:                1
Configured node list:
    0   Node ID = 1
    1   Node ID = 2
Membership node list:   1, 2

Qdevice-net information
----------------------
Cluster name:           mycluster
QNetd host:             qdevice:5403
Algorithm:              ffsplit
Tie-breaker:            Node with lowest node ID
State:                  Connected</pre><p class="simpara">
							From the quorum device side, you can execute the following status command, which shows the status of the <code class="literal">corosync-qnetd</code> daemon.
						</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">pcs qdevice status net --full</code>
QNetd address:                  *:5403
TLS:                            Supported (client certificate required)
Connected clients:              2
Connected clusters:             1
Maximum send/receive size:      32768/32768 bytes
Cluster "mycluster":
    Algorithm:          ffsplit
    Tie-breaker:        Node with lowest node ID
    Node ID 2:
        Client address:         ::ffff:192.168.122.122:50028
        HB interval:            8000ms
        Configured node list:   1, 2
        Ring ID:                1.2050
        Membership node list:   1, 2
        TLS active:             Yes (client certificate verified)
        Vote:                   ACK (ACK)
    Node ID 1:
        Client address:         ::ffff:192.168.122.121:48786
        HB interval:            8000ms
        Configured node list:   1, 2
        Ring ID:                1.2050
        Membership node list:   1, 2
        TLS active:             Yes (client certificate verified)
        Vote:                   ACK (ACK)</pre></li></ol></div></section><section class="section" id="proc_managing-quorum-device-service-configuring-quorum-devices"><div class="titlepage"><div><div><h3 class="title">25.5.3. Managing the Quorum Device Service</h3></div></div></div><p>
					PCS provides the ability to manage the quorum device service on the local host (<code class="literal command">corosync-qnetd</code>), as shown in the following example commands. Note that these commands affect only the <code class="literal">corosync-qnetd</code> service.
				</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">pcs qdevice start net</code>
[root@qdevice:~]# <code class="literal">pcs qdevice stop net</code>
[root@qdevice:~]# <code class="literal">pcs qdevice enable net</code>
[root@qdevice:~]# <code class="literal">pcs qdevice disable net</code>
[root@qdevice:~]# <code class="literal">pcs qdevice kill net</code></pre></section><section class="section" id="proc_managing-quorum-device-settingsconfiguring-quorum-devices"><div class="titlepage"><div><div><h3 class="title">25.5.4. Managing the quorum device settings in a cluster</h3></div></div></div><p>
					The following sections describe the PCS commands that you can use to manage the quorum device settings in a cluster.
				</p><section class="section" id="s3-changeqdevice-HAAR"><div class="titlepage"><div><div><h4 class="title">25.5.4.1. Changing quorum device settings</h4></div></div></div><p>
						You can change the setting of a quorum device with the <code class="literal command">pcs quorum device update</code> command.
					</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							To change the <code class="literal">host</code> option of quorum device model <code class="literal">net</code>, use the <code class="literal command">pcs quorum device remove</code> and the <code class="literal command">pcs quorum device add</code> commands to set up the configuration properly, unless the old and the new host are the same machine.
						</p></div></div><p>
						The following command changes the quorum device algorithm to <code class="literal">lms</code>.
					</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum device update model algorithm=lms</code>
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded
Corosync configuration reloaded
Reloading qdevice configuration on nodes...
node1: corosync-qdevice stopped
node2: corosync-qdevice stopped
node1: corosync-qdevice started
node2: corosync-qdevice started</pre></section><section class="section" id="removing_a_quorum_device"><div class="titlepage"><div><div><h4 class="title">25.5.4.2. Removing a quorum device</h4></div></div></div><p>
						Use the following command to remove a quorum device configured on a cluster node.
					</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum device remove</code>
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded
Corosync configuration reloaded
Disabling corosync-qdevice...
node1: corosync-qdevice disabled
node2: corosync-qdevice disabled
Stopping corosync-qdevice...
node1: corosync-qdevice stopped
node2: corosync-qdevice stopped
Removing qdevice certificates from nodes...
node1: Succeeded
node2: Succeeded</pre><p>
						After you have removed a quorum device, you should see the following error message when displaying the quorum device status.
					</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum device status</code>
Error: Unable to get quorum status: corosync-qdevice-tool: Can't connect to QDevice socket (is QDevice running?): No such file or directory</pre></section><section class="section" id="destroying_a_quorum_device"><div class="titlepage"><div><div><h4 class="title">25.5.4.3. Destroying a quorum device</h4></div></div></div><p>
						To disable and stop a quorum device on the quorum device host and delete all of its configuration files, use the following command.
					</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">pcs qdevice destroy net</code>
Stopping quorum device...
quorum device stopped
quorum device disabled
Quorum device 'net' configuration files removed</pre></section></section></section></section><section class="chapter" id="assembly_configuring-pacemaker-alert-agents_configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 26. Triggering scripts for cluster events</h1></div></div></div><p>
			A Pacemaker cluster is an event-driven system, where an event might be a resource or node failure, a configuration change, or a resource starting or stopping. You can configure Pacemaker cluster alerts to take some external action when a cluster event occurs by means of alert agents, which are external programs that the cluster calls in the same manner as the cluster calls resource agents to handle resource configuration and operation.
		</p><p>
			The cluster passes information about the event to the agent by means of environment variables. Agents can do anything with this information, such as send an email message or log to a file or update a monitoring system.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Pacemaker provides several sample alert agents, which are installed in <code class="literal">/usr/share/pacemaker/alerts</code> by default. These sample scripts may be copied and used as is, or they may be used as templates to be edited to suit your purposes. Refer to the source code of the sample agents for the full set of attributes they support.
				</li><li class="listitem">
					If the sample alert agents do not meet your needs, you can write your own alert agents for a Pacemaker alert to call.
				</li></ul></div><section class="section" id="using-sample-alert-agents-configuring-pacemaker-alert-agents"><div class="titlepage"><div><div><h2 class="title">26.1. Installing and configuring sample alert agents</h2></div></div></div><p>
				When you use one of the sample alert agents, you should review the script to ensure that it suits your needs. These sample agents are provided as a starting point for custom scripts for specific cluster environments. Note that while Red Hat supports the interfaces that the alert agents scripts use to communicate with Pacemaker, Red Hat does not provide support for the custom agents themselves.
			</p><p>
				To use one of the sample alert agents, you must install the agent on each node in the cluster. For example, the following command installs the <code class="literal">alert_file.sh.sample</code> script as <code class="literal">alert_file.sh</code>.
			</p><pre class="literallayout"># <code class="literal">install --mode=0755 /usr/share/pacemaker/alerts/alert_file.sh.sample /var/lib/pacemaker/alert_file.sh</code></pre><p>
				After you have installed the script, you can create an alert that uses the script.
			</p><p>
				The following example configures an alert that uses the installed <code class="literal">alert_file.sh</code> alert agent to log events to a file. Alert agents run as the user <code class="literal">hacluster</code>, which has a minimal set of permissions.
			</p><p>
				This example creates the log file <code class="literal">pcmk_alert_file.log</code> that will be used to record the events. It then creates the alert agent and adds the path to the log file as its recipient.
			</p><pre class="literallayout"># <code class="literal">touch /var/log/pcmk_alert_file.log</code>
# <code class="literal">chown hacluster:haclient /var/log/pcmk_alert_file.log</code>
# <code class="literal">chmod 600 /var/log/pcmk_alert_file.log</code>
# <code class="literal">pcs alert create id=alert_file description="Log events to a file." path=/var/lib/pacemaker/alert_file.sh</code>
# <code class="literal">pcs alert recipient add alert_file id=my-alert_logfile value=/var/log/pcmk_alert_file.log</code></pre><p>
				The following example installs the <code class="literal">alert_snmp.sh.sample</code> script as <code class="literal">alert_snmp.sh</code> and configures an alert that uses the installed <code class="literal">alert_snmp.sh</code> alert agent to send cluster events as SNMP traps. By default, the script will send all events except successful monitor calls to the SNMP server. This example configures the timestamp format as a meta option. After configuring the alert, this example configures a recipient for the alert and displays the alert configuration.
			</p><pre class="literallayout"># <code class="literal">install --mode=0755 /usr/share/pacemaker/alerts/alert_snmp.sh.sample /var/lib/pacemaker/alert_snmp.sh</code>
# <code class="literal">pcs alert create id=snmp_alert path=/var/lib/pacemaker/alert_snmp.sh meta timestamp-format="%Y-%m-%d,%H:%M:%S.%01N"</code>
# <code class="literal">pcs alert recipient add snmp_alert value=192.168.1.2</code>
# <code class="literal">pcs alert</code>
Alerts:
 Alert: snmp_alert (path=/var/lib/pacemaker/alert_snmp.sh)
  Meta options: timestamp-format=%Y-%m-%d,%H:%M:%S.%01N.
  Recipients:
   Recipient: snmp_alert-recipient (value=192.168.1.2)</pre><p>
				The following example installs the <code class="literal">alert_smtp.sh</code> agent and then configures an alert that uses the installed alert agent to send cluster events as email messages. After configuring the alert, this example configures a recipient and displays the alert configuration.
			</p><pre class="literallayout"># <code class="literal">install --mode=0755 /usr/share/pacemaker/alerts/alert_smtp.sh.sample /var/lib/pacemaker/alert_smtp.sh</code>
# <code class="literal">pcs alert create id=smtp_alert path=/var/lib/pacemaker/alert_smtp.sh options email_sender=donotreply@example.com</code>
# <code class="literal">pcs alert recipient add smtp_alert value=admin@example.com</code>
# <code class="literal">pcs alert</code>
Alerts:
 Alert: smtp_alert (path=/var/lib/pacemaker/alert_smtp.sh)
  Options: email_sender=<a class="link" href="mailto:donotreply@example.com">donotreply@example.com</a>
  Recipients:
   Recipient: smtp_alert-recipient (value=<a class="link" href="mailto:admin@example.com">admin@example.com</a>)</pre></section><section class="section" id="creating_a_cluster_alert"><div class="titlepage"><div><div><h2 class="title">26.2. Creating a cluster alert</h2></div></div></div><p>
				The following command creates a cluster alert. The options that you configure are agent-specific configuration values that are passed to the alert agent script at the path you specify as additional environment variables. If you do not specify a value for <code class="literal">id</code>, one will be generated.
			</p><pre class="literallayout">pcs alert create path=<span class="emphasis"><em>path</em></span> [id=<span class="emphasis"><em>alert-id</em></span>] [description=<span class="emphasis"><em>description</em></span>] [options [<span class="emphasis"><em>option</em></span>=<span class="emphasis"><em>value</em></span>]...] [meta [<span class="emphasis"><em>meta-option</em></span>=<span class="emphasis"><em>value</em></span>]...]</pre><p>
				Multiple alert agents may be configured; the cluster will call all of them for each event. Alert agents will be called only on cluster nodes. They will be called for events involving Pacemaker Remote nodes, but they will never be called on those nodes.
			</p><p>
				The following example creates a simple alert that will call <code class="literal">myscript.sh</code> for each event.
			</p><pre class="literallayout"># <code class="literal">pcs alert create id=my_alert path=/path/to/myscript.sh</code></pre></section><section class="section" id="displaying_modifying_and_removing_cluster_alerts"><div class="titlepage"><div><div><h2 class="title">26.3. Displaying, modifying, and removing cluster alerts</h2></div></div></div><p>
				The following command shows all configured alerts along with the values of the configured options.
			</p><pre class="literallayout">pcs alert [config|show]</pre><p>
				The following command updates an existing alert with the specified <span class="emphasis"><em>alert-id</em></span> value.
			</p><pre class="literallayout">pcs alert update <span class="emphasis"><em>alert-id</em></span> [path=<span class="emphasis"><em>path</em></span>] [description=<span class="emphasis"><em>description</em></span>] [options [<span class="emphasis"><em>option</em></span>=<span class="emphasis"><em>value</em></span>]...] [meta [<span class="emphasis"><em>meta-option</em></span>=<span class="emphasis"><em>value</em></span>]...]</pre><p>
				The following command removes an alert with the specified <span class="emphasis"><em>alert-id</em></span> value.
			</p><pre class="literallayout">pcs alert remove <span class="emphasis"><em>alert-id</em></span></pre><p>
				Alternately, you can run the <code class="literal">pcs alert delete</code> command, which is identical to the <code class="literal">pcs alert remove</code> command. Both the <code class="literal">pcs alert delete</code> and the <code class="literal">pcs alert remove</code> commands allow you to specify more than one alert to be deleted.
			</p></section><section class="section" id="configuring_alert_recipients"><div class="titlepage"><div><div><h2 class="title">26.4. Configuring alert recipients</h2></div></div></div><p>
				Usually alerts are directed towards a recipient. Thus each alert may be additionally configured with one or more recipients. The cluster will call the agent separately for each recipient.
			</p><p>
				The recipient may be anything the alert agent can recognize: an IP address, an email address, a file name, or whatever the particular agent supports.
			</p><p>
				The following command adds a new recipient to the specified alert.
			</p><pre class="literallayout">pcs alert recipient add <span class="emphasis"><em>alert-id</em></span> value=<span class="emphasis"><em>recipient-value</em></span> [id=<span class="emphasis"><em>recipient-id</em></span>] [description=<span class="emphasis"><em>description</em></span>] [options [<span class="emphasis"><em>option</em></span>=<span class="emphasis"><em>value</em></span>]...] [meta [<span class="emphasis"><em>meta-option</em></span>=<span class="emphasis"><em>value</em></span>]...]</pre><p>
				The following command updates an existing alert recipient.
			</p><pre class="literallayout">pcs alert recipient update <span class="emphasis"><em>recipient-id</em></span> [value=<span class="emphasis"><em>recipient-value</em></span>] [description=<span class="emphasis"><em>description</em></span>] [options [<span class="emphasis"><em>option</em></span>=<span class="emphasis"><em>value</em></span>]...] [meta [<span class="emphasis"><em>meta-option</em></span>=<span class="emphasis"><em>value</em></span>]...]</pre><p>
				The following command removes the specified alert recipient.
			</p><pre class="literallayout">pcs alert recipient remove <span class="emphasis"><em>recipient-id</em></span></pre><p>
				Alternately, you can run the <code class="literal">pcs alert recipient delete</code> command, which is identical to the <code class="literal">pcs alert recipient remove</code> command. Both the <code class="literal">pcs alert recipient remove</code> and the <code class="literal">pcs alert recipient delete</code> commands allow you to remove more than one alert recipient.
			</p><p>
				The following example command adds the alert recipient <code class="literal">my-alert-recipient</code> with a recipient ID of <code class="literal">my-recipient-id</code> to the alert <code class="literal">my-alert</code>. This will configure the cluster to call the alert script that has been configured for <code class="literal">my-alert</code> for each event, passing the recipient <code class="literal">some-address</code> as an environment variable.
			</p><pre class="literallayout">#  <code class="literal">pcs alert recipient add my-alert value=my-alert-recipient id=my-recipient-id options value=some-address</code></pre></section><section class="section" id="alert_meta_options"><div class="titlepage"><div><div><h2 class="title">26.5. Alert meta options</h2></div></div></div><p>
				As with resource agents, meta options can be configured for alert agents to affect how Pacemaker calls them. <a class="xref" href="index.html#tb-alert-meta-HAAR" title="Table 26.1. Alert Meta Options">Table 26.1, “Alert Meta Options”</a> describes the alert meta options. Meta options can be configured per alert agent as well as per recipient.
			</p><div class="table" id="tb-alert-meta-HAAR"><p class="title"><strong>Table 26.1. Alert Meta Options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319457851072" scope="col">Meta-Attribute</th><th align="left" valign="top" id="idm140319457849984" scope="col">Default</th><th align="left" valign="top" id="idm140319457848896" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319457851072"> <p>
								<code class="literal">timestamp-format</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319457849984"> <p>
								%H:%M:%S.%06N
							</p>
							 </td><td align="left" valign="top" headers="idm140319457848896"> <p>
								Format the cluster will use when sending the event’s timestamp to the agent. This is a string as used with the <code class="literal">date</code>(1) command.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319457851072"> <p>
								<code class="literal">timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319457849984"> <p>
								30s
							</p>
							 </td><td align="left" valign="top" headers="idm140319457848896"> <p>
								If the alert agent does not complete within this amount of time, it will be terminated.
							</p>
							 </td></tr></tbody></table></div></div><p>
				The following example configures an alert that calls the script <code class="literal">myscript.sh</code> and then adds two recipients to the alert. The first recipient has an ID of <code class="literal">my-alert-recipient1</code> and the second recipient has an ID of <code class="literal">my-alert-recipient2</code>. The script will get called twice for each event, with each call using a 15-second timeout. One call will be passed to the recipient <code class="literal">someuser@example.com</code> with a timestamp in the format %D %H:%M, while the other call will be passed to the recipient <code class="literal">otheruser@example.com</code> with a timestamp in the format %c.
			</p><pre class="literallayout"># <code class="literal">pcs alert create id=my-alert path=/path/to/myscript.sh meta timeout=15s</code>
# <code class="literal">pcs alert recipient add my-alert value=someuser@example.com id=my-alert-recipient1 meta timestamp-format="%D %H:%M"</code>
# <code class="literal">pcs alert recipient add my-alert value=otheruser@example.com id=my-alert-recipient2 meta timestamp-format="%c"</code></pre></section><section class="section" id="alert_configuration_command_examples"><div class="titlepage"><div><div><h2 class="title">26.6. Alert configuration command examples</h2></div></div></div><p>
				The following sequential examples show some basic alert configuration commands to show the format to use to create alerts, add recipients, and display the configured alerts.
			</p><p>
				Note that while you must install the alert agents themselves on each node in a cluster, you need to run the <code class="literal">pcs</code> commands only once.
			</p><p>
				The following commands create a simple alert, add two recipients to the alert, and display the configured values.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Since no alert ID value is specified, the system creates an alert ID value of <code class="literal">alert</code>.
					</li><li class="listitem">
						The first recipient creation command specifies a recipient of <code class="literal">rec_value</code>. Since this command does not specify a recipient ID, the value of <code class="literal">alert-recipient</code> is used as the recipient ID.
					</li><li class="listitem">
						The second recipient creation command specifies a recipient of <code class="literal">rec_value2</code>. This command specifies a recipient ID of <code class="literal">my-recipient</code> for the recipient.
					</li></ul></div><pre class="literallayout"># <code class="literal">pcs alert create path=/my/path</code>
# <code class="literal">pcs alert recipient add alert value=rec_value</code>
# <code class="literal">pcs alert recipient add alert value=rec_value2 id=my-recipient</code>
# <code class="literal">pcs alert config</code>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)
   Recipient: my-recipient (value=rec_value2)</pre><p>
				This following commands add a second alert and a recipient for that alert. The alert ID for the second alert is <code class="literal">my-alert</code> and the recipient value is <code class="literal">my-other-recipient</code>. Since no recipient ID is specified, the system provides a recipient id of <code class="literal">my-alert-recipient</code>.
			</p><pre class="literallayout"># <code class="literal">pcs alert create id=my-alert path=/path/to/script description=alert_description options option1=value1 opt=val meta timeout=50s timestamp-format="%H%B%S"</code>
# <code class="literal">pcs alert recipient add my-alert value=my-other-recipient</code>
# <code class="literal">pcs alert</code>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)
   Recipient: my-recipient (value=rec_value2)
 Alert: my-alert (path=/path/to/script)
  Description: alert_description
  Options: opt=val option1=value1
  Meta options: timestamp-format=%H%B%S timeout=50s
  Recipients:
   Recipient: my-alert-recipient (value=my-other-recipient)</pre><p>
				The following commands modify the alert values for the alert <code class="literal">my-alert</code> and for the recipient <code class="literal">my-alert-recipient</code>.
			</p><pre class="literallayout"># <code class="literal">pcs alert update my-alert options option1=newvalue1 meta timestamp-format="%H%M%S"</code>
# <code class="literal">pcs alert recipient update my-alert-recipient options option1=new meta timeout=60s</code>
# <code class="literal">pcs alert</code>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)
   Recipient: my-recipient (value=rec_value2)
 Alert: my-alert (path=/path/to/script)
  Description: alert_description
  Options: opt=val option1=newvalue1
  Meta options: timestamp-format=%H%M%S timeout=50s
  Recipients:
   Recipient: my-alert-recipient (value=my-other-recipient)
    Options: option1=new
    Meta options: timeout=60s</pre><p>
				The following command removes the recipient <code class="literal">my-alert-recipient</code> from <code class="literal">alert</code>.
			</p><pre class="literallayout"># <code class="literal">pcs alert recipient remove my-recipient</code>
# <code class="literal">pcs alert</code>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)
 Alert: my-alert (path=/path/to/script)
  Description: alert_description
  Options: opt=val option1=newvalue1
  Meta options: timestamp-format="%M%B%S" timeout=50s
  Recipients:
   Recipient: my-alert-recipient (value=my-other-recipient)
    Options: option1=new
    Meta options: timeout=60s</pre><p>
				The following command removes <code class="literal">myalert</code> from the configuration.
			</p><pre class="literallayout"># <code class="literal">pcs alert remove myalert</code>
# <code class="literal">pcs alert</code>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)</pre></section><section class="section" id="writing_an_alert_agent"><div class="titlepage"><div><div><h2 class="title">26.7. Writing an alert agent</h2></div></div></div><p>
				There are three types of Pacemaker alerts: node alerts, fencing alerts, and resource alerts. The environment variables that are passed to the alert agents can differ, depending on the type of alert. <a class="xref" href="index.html#tb-alert-environmentvariables-HAAR" title="Table 26.2. Environment Variables Passed to Alert Agents">Table 26.2, “Environment Variables Passed to Alert Agents”</a> describes the environment variables that are passed to alert agents and specifies when the environment variable is associated with a specific alert type.
			</p><div class="table" id="tb-alert-environmentvariables-HAAR"><p class="title"><strong>Table 26.2. Environment Variables Passed to Alert Agents</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 44%; " class="col_1"><!--Empty--></col><col style="width: 56%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319458739728" scope="col">Environment Variable</th><th align="left" valign="top" id="idm140319459354896" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_kind</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								The type of alert (node, fencing, or resource)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_version</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								The version of Pacemaker sending the alert
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_recipient</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								The configured recipient
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_node_sequence</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								A sequence number increased whenever an alert is being issued on the local node, which can be used to reference the order in which alerts have been issued by Pacemaker. An alert for an event that happened later in time reliably has a higher sequence number than alerts for earlier events. Be aware that this number has no cluster-wide meaning.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_timestamp</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								A timestamp created prior to executing the agent, in the format specified by the <code class="literal">timestamp-format</code> meta option. This allows the agent to have a reliable, high-precision time of when the event occurred, regardless of when the agent itself was invoked (which could potentially be delayed due to system load or other circumstances).
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_node</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								Name of affected node
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_desc</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								Detail about event. For node alerts, this is the node’s current state (member or lost). For fencing alerts, this is a summary of the requested fencing operation, including origin, target, and fencing operation error code, if any. For resource alerts, this is a readable string equivalent of <code class="literal">CRM_alert_status</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_nodeid</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								ID of node whose status changed (provided with node alerts only)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_task</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								The requested fencing or resource operation (provided with fencing and resource alerts only)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_rc</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								The numerical return code of the fencing or resource operation (provided with fencing and resource alerts only)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_rsc</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								The name of the affected resource (resource alerts only)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_interval</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								The interval of the resource operation (resource alerts only)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_target_rc</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								The expected numerical return code of the operation (resource alerts only)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140319458739728"> <p>
								<code class="literal">CRM_alert_status</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140319459354896"> <p>
								A numerical code used by Pacemaker to represent the operation result (resource alerts only)
							</p>
							 </td></tr></tbody></table></div></div><p>
				When writing an alert agent, you must take the following concerns into account.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Alert agents may be called with no recipient (if none is configured), so the agent must be able to handle this situation, even if it only exits in that case. Users may modify the configuration in stages, and add a recipient later.
					</li><li class="listitem">
						If more than one recipient is configured for an alert, the alert agent will be called once per recipient. If an agent is not able to run concurrently, it should be configured with only a single recipient. The agent is free, however, to interpret the recipient as a list.
					</li><li class="listitem">
						When a cluster event occurs, all alerts are fired off at the same time as separate processes. Depending on how many alerts and recipients are configured and on what is done within the alert agents, a significant load burst may occur. The agent could be written to take this into consideration, for example by queueing resource-intensive actions into some other instance, instead of directly executing them.
					</li><li class="listitem">
						Alert agents are run as the <code class="literal">hacluster</code> user, which has a minimal set of permissions. If an agent requires additional privileges, it is recommended to configure <code class="literal command">sudo</code> to allow the agent to run the necessary commands as another user with the appropriate privileges.
					</li><li class="listitem">
						Take care to validate and sanitize user-configured parameters, such as <code class="literal">CRM_alert_timestamp</code> (whose content is specified by the user-configured <code class="literal">timestamp-format</code>), <code class="literal">CRM_alert_recipient</code>, and all alert options. This is necessary to protect against configuration errors. In addition, if some user can modify the CIB without having <code class="literal">hacluster</code>-level access to the cluster nodes, this is a potential security concern as well, and you should avoid the possibility of code injection.
					</li><li class="listitem">
						If a cluster contains resources with operations for which the <code class="literal">on-fail</code> parameter is set to <code class="literal">fence</code>, there will be multiple fence notifications on failure, one for each resource for which this parameter is set plus one additional notification. Both the <code class="literal">pacemaker-fenced</code> and <code class="literal">pacemaker-controld</code> will send notifications. Pacemaker performs only one actual fence operation in this case, however, no matter how many notifications are sent.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The alerts interface is designed to be backward compatible with the external scripts interface used by the <code class="literal">ocf:pacemaker:ClusterMon</code> resource. To preserve this compatibility, the environment variables passed to alert agents are available prepended with <code class="literal">CRM_notify_</code> as well as <code class="literal">CRM_alert_</code>. One break in compatibility is that the <code class="literal">ClusterMon</code> resource ran external scripts as the root user, while alert agents are run as the <code class="literal">hacluster</code> user.
				</p></div></div></section></section><section class="chapter" id="assembly_configuring-multisite-cluster-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 27. Configuring multi-site clusters with Pacemaker</h1></div></div></div><p>
			When a cluster spans more than one site, issues with network connectivity between the sites can lead to split-brain situations. When connectivity drops, there is no way for a node on one site to determine whether a node on another site has failed or is still functioning with a failed site interlink. In addition, it can be problematic to provide high availability services across two sites which are too far apart to keep synchronous.
		</p><p>
			To address these issues, Pacemaker provides full support for the ability to configure high availability clusters that span multiple sites through the use of a Booth cluster ticket manager.
		</p><section class="section" id="con_booth-cluster-ticket-manager-configuring-multisite-cluster"><div class="titlepage"><div><div><h2 class="title">27.1. Overview of Booth cluster ticket manager</h2></div></div></div><p>
				The Booth <span class="emphasis"><em>ticket manager</em></span> is a distributed service that is meant to be run on a different physical network than the networks that connect the cluster nodes at particular sites. It yields another, loose cluster, a <span class="emphasis"><em>Booth formation</em></span>, that sits on top of the regular clusters at the sites. This aggregated communication layer facilitates consensus-based decision processes for individual Booth tickets.
			</p><p>
				A Booth <span class="emphasis"><em>ticket</em></span> is a singleton in the Booth formation and represents a time-sensitive, movable unit of authorization. Resources can be configured to require a certain ticket to run. This can ensure that resources are run at only one site at a time, for which a ticket or tickets have been granted.
			</p><p>
				You can think of a Booth formation as an overlay cluster consisting of clusters running at different sites, where all the original clusters are independent of each other. It is the Booth service which communicates to the clusters whether they have been granted a ticket, and it is Pacemaker that determines whether to run resources in a cluster based on a Pacemaker ticket constraint. This means that when using the ticket manager, each of the clusters can run its own resources as well as shared resources. For example there can be resources A, B and C running only in one cluster, resources D, E, and F running only in the other cluster, and resources G and H running in either of the two clusters as determined by a ticket. It is also possible to have an additional resource J that could run in either of the two clusters as determined by a separate ticket.
			</p></section><section class="section" id="proc-configuring-multisite-with-booth-configuring-multisite-cluster"><div class="titlepage"><div><div><h2 class="title">27.2. Configuring multi-site clusters with Pacemaker</h2></div></div></div><p>
				The following procedure provides an outline of the steps you follow to configure a multi-site configuration that uses the Booth ticket manager.
			</p><p>
				These example commands use the following arrangement:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Cluster 1 consists of the nodes <code class="literal">cluster1-node1</code> and <code class="literal">cluster1-node2</code>
					</li><li class="listitem">
						Cluster 1 has a floating IP address assigned to it of 192.168.11.100
					</li><li class="listitem">
						Cluster 2 consists of <code class="literal">cluster2-node1</code> and <code class="literal">cluster2-node2</code>
					</li><li class="listitem">
						Cluster 2 has a floating IP address assigned to it of 192.168.22.100
					</li><li class="listitem">
						The arbitrator node is <code class="literal">arbitrator-node</code> with an ip address of 192.168.99.100
					</li><li class="listitem">
						The name of the Booth ticket that this configuration uses is <code class="literal">apacheticket</code>
					</li></ul></div><p>
				These example commands assume that the cluster resources for an Apache service have been configured as part of the resource group <code class="literal">apachegroup</code> for each cluster. It is not required that the resources and resource groups be the same on each cluster to configure a ticket constraint for those resources, since the Pacemaker instance for each cluster is independent, but that is a common failover scenario.
			</p><p>
				Note that at any time in the configuration procedure you can enter the <code class="literal command">pcs booth config</code> command to display the booth configuration for the current node or cluster or the <code class="literal command">pcs booth status</code> command to display the current status of booth on the local node.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Install the <code class="literal">booth-site</code> Booth ticket manager package on each node of both clusters.
					</p><pre class="literallayout">[root@cluster1-node1 ~]# <code class="literal">yum install -y booth-site</code>
[root@cluster1-node2 ~]# <code class="literal">yum install -y booth-site</code>
[root@cluster2-node1 ~]# <code class="literal">yum install -y booth-site</code>
[root@cluster2-node2 ~]# <code class="literal">yum install -y booth-site</code></pre></li><li class="listitem"><p class="simpara">
						Install the <code class="literal">pcs</code>, <code class="literal">booth-core</code>, and <code class="literal">booth-arbitrator</code> packages on the arbitrator node.
					</p><pre class="literallayout">[root@arbitrator-node ~]# <code class="literal">yum install -y pcs booth-core booth-arbitrator</code></pre></li><li class="listitem"><p class="simpara">
						Ensure that ports 9929/tcp and 9929/udp are open on all cluster nodes and on the arbitrator node.
					</p><p class="simpara">
						For example, running the following commands on all nodes in both clusters as well as on the arbitrator node allows access to ports 9929/tcp and 9929/udp on those nodes.
					</p><pre class="literallayout"># <code class="literal">firewall-cmd --add-port=9929/udp</code>
# <code class="literal">firewall-cmd --add-port=9929/tcp</code>
# <code class="literal">firewall-cmd --add-port=9929/udp --permanent</code>
# <code class="literal">firewall-cmd --add-port=9929/tcp --permanent</code></pre><p class="simpara">
						Note that this procedure in itself allows any machine anywhere to access port 9929 on the nodes. You should ensure that on your site the nodes are open only to the nodes that require them.
					</p></li><li class="listitem"><p class="simpara">
						Create a Booth configuration on one node of one cluster. The addresses you specify for each cluster and for the arbitrator must be IP addresses. For each cluster, you specify a floating IP address.
					</p><pre class="literallayout">[cluster1-node1 ~] # <code class="literal">pcs booth setup sites 192.168.11.100 192.168.22.100 arbitrators 192.168.99.100</code></pre><p class="simpara">
						This command creates the configuration files <code class="literal">/etc/booth/booth.conf</code> and <code class="literal">/etc/booth/booth.key</code> on the node from which it is run.
					</p></li><li class="listitem"><p class="simpara">
						Create a ticket for the Booth configuration. This is the ticket that you will use to define the resource constraint that will allow resources to run only when this ticket has been granted to the cluster.
					</p><p class="simpara">
						This basic failover configuration procedure uses only one ticket, but you can create additional tickets for more complicated scenarios where each ticket is associated with a different resource or resources.
					</p><pre class="literallayout">[cluster1-node1 ~] # <code class="literal">pcs booth ticket add apacheticket</code></pre></li><li class="listitem"><p class="simpara">
						Synchronize the Booth configuration to all nodes in the current cluster.
					</p><pre class="literallayout">[cluster1-node1 ~] # <code class="literal">pcs booth sync</code></pre></li><li class="listitem"><p class="simpara">
						From the arbitrator node, pull the Booth configuration to the arbitrator. If you have not previously done so, you must first authenticate <code class="literal">pcs</code> to the node from which you are pulling the configuration.
					</p><pre class="literallayout">[arbitrator-node ~] # <code class="literal">pcs host auth cluster1-node1</code>
[arbitrator-node ~] # <code class="literal">pcs booth pull cluster1-node1</code></pre></li><li class="listitem"><p class="simpara">
						Pull the Booth configuration to the other cluster and synchronize to all the nodes of that cluster. As with the arbitrator node, if you have not previously done so, you must first authenticate <code class="literal">pcs</code> to the node from which you are pulling the configuration.
					</p><pre class="literallayout">[cluster2-node1 ~] # <code class="literal">pcs host auth cluster1-node1</code>
[cluster2-node1 ~] # <code class="literal">pcs booth pull cluster1-node1</code>
[cluster2-node1 ~] # <code class="literal">pcs booth sync</code></pre></li><li class="listitem"><p class="simpara">
						Start and enable Booth on the arbitrator.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You must not manually start or enable Booth on any of the nodes of the clusters since Booth runs as a Pacemaker resource in those clusters.
						</p></div></div><pre class="literallayout">[arbitrator-node ~] # <code class="literal">pcs booth start</code>
[arbitrator-node ~] # <code class="literal">pcs booth enable</code></pre></li><li class="listitem"><p class="simpara">
						Configure Booth to run as a cluster resource on both cluster sites. This creates a resource group with <code class="literal">booth-ip</code> and <code class="literal">booth-service</code> as members of that group.
					</p><pre class="literallayout">[cluster1-node1 ~] # <code class="literal">pcs booth create ip 192.168.11.100</code>
[cluster2-node1 ~] # <code class="literal">pcs booth create ip 192.168.22.100</code></pre></li><li class="listitem"><p class="simpara">
						Add a ticket constraint to the resource group you have defined for each cluster.
					</p><pre class="literallayout">[cluster1-node1 ~] # <code class="literal">pcs constraint ticket add apacheticket apachegroup</code>
[cluster2-node1 ~] # <code class="literal">pcs constraint ticket add apacheticket apachegroup</code></pre><p class="simpara">
						You can enter the following command to display the currently configured ticket constraints.
					</p><pre class="literallayout">pcs constraint ticket [show]</pre></li><li class="listitem"><p class="simpara">
						Grant the ticket you created for this setup to the first cluster.
					</p><p class="simpara">
						Note that it is not necessary to have defined ticket constraints before granting a ticket. Once you have initially granted a ticket to a cluster, then Booth takes over ticket management unless you override this manually with the <code class="literal command">pcs booth ticket revoke</code> command. For information on the <code class="literal command">pcs booth</code> administration commands, see the PCS help screen for the <code class="literal command">pcs booth</code> command.
					</p><pre class="literallayout">[cluster1-node1 ~] # <code class="literal">pcs booth ticket grant apacheticket</code></pre></li></ol></div><p>
				It is possible to add or remove tickets at any time, even after completing this procedure. After adding or removing a ticket, however, you must synchronize the configuration files to the other nodes and clusters as well as to the arbitrator and grant the ticket as is shown in this procedure.
			</p><p>
				For information on additional Booth administration commands that you can use for cleaning up and removing Booth configuration files, tickets, and resources, see the PCS help screen for the <code class="literal command">pcs booth</code> command.
			</p></section></section><section class="chapter" id="assembly_remote-node-management-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 28. Integrating non-corosync nodes into a cluster: the pacemaker_remote service</h1></div></div></div><p>
			The <code class="literal">pacemaker_remote</code> service allows nodes not running <code class="literal">corosync</code> to integrate into the cluster and have the cluster manage their resources just as if they were real cluster nodes.
		</p><p>
			Among the capabilities that the <code class="literal">pacemaker_remote</code> service provides are the following:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					The <code class="literal">pacemaker_remote</code> service allows you to scale beyond the Red Hat support limit of 32 nodes for RHEL 8.1.
				</li><li class="listitem">
					The <code class="literal">pacemaker_remote</code> service allows you to manage a virtual environment as a cluster resource and also to manage individual services within the virtual environment as cluster resources.
				</li></ul></div><p>
			The following terms are used to describe the <code class="literal">pacemaker_remote</code> service.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<span class="emphasis"><em>cluster node</em></span> — A node running the High Availability services (<code class="literal">pacemaker</code> and <code class="literal">corosync</code>).
				</li><li class="listitem">
					<span class="emphasis"><em>remote node</em></span> — A node running <code class="literal">pacemaker_remote</code> to remotely integrate into the cluster without requiring <code class="literal">corosync</code> cluster membership. A remote node is configured as a cluster resource that uses the <code class="literal">ocf:pacemaker:remote</code> resource agent.
				</li><li class="listitem">
					<span class="emphasis"><em>guest node</em></span> — A virtual guest node running the <code class="literal">pacemaker_remote</code> service. The virtual guest resource is managed by the cluster; it is both started by the cluster and integrated into the cluster as a remote node.
				</li><li class="listitem">
					<span class="emphasis"><em>pacemaker_remote</em></span> — A service daemon capable of performing remote application management within remote nodes and KVM guest nodes in a Pacemaker cluster environment. This service is an enhanced version of Pacemaker’s local executor daemon (<code class="literal">pacemaker-execd</code>) that is capable of managing resources remotely on a node not running corosync.
				</li></ul></div><p>
			A Pacemaker cluster running the <code class="literal">pacemaker_remote</code> service has the following characteristics.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Remote nodes and guest nodes run the <code class="literal">pacemaker_remote</code> service (with very little configuration required on the virtual machine side).
				</li><li class="listitem">
					The cluster stack (<code class="literal">pacemaker</code> and <code class="literal">corosync</code>), running on the cluster nodes, connects to the <code class="literal">pacemaker_remote</code> service on the remote nodes, allowing them to integrate into the cluster.
				</li><li class="listitem">
					The cluster stack (<code class="literal">pacemaker</code> and <code class="literal">corosync</code>), running on the cluster nodes, launches the guest nodes and immediately connects to the <code class="literal">pacemaker_remote</code> service on the guest nodes, allowing them to integrate into the cluster.
				</li></ul></div><p>
			The key difference between the cluster nodes and the remote and guest nodes that the cluster nodes manage is that the remote and guest nodes are not running the cluster stack. This means the remote and guest nodes have the following limitations:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					they do not take place in quorum
				</li><li class="listitem">
					they do not execute fencing device actions
				</li><li class="listitem">
					they are not eligible to be the cluster’s Designated Controller (DC)
				</li><li class="listitem">
					they do not themselves run the full range of <code class="literal command">pcs</code> commands
				</li></ul></div><p>
			On the other hand, remote nodes and guest nodes are not bound to the scalability limits associated with the cluster stack.
		</p><p>
			Other than these noted limitations, the remote and guest nodes behave just like cluster nodes in respect to resource management, and the remote and guest nodes can themselves be fenced. The cluster is fully capable of managing and monitoring resources on each remote and guest node: You can build constraints against them, put them in standby, or perform any other action you perform on cluster nodes with the <code class="literal command">pcs</code> commands. Remote and guest nodes appear in cluster status output just as cluster nodes do.
		</p><section class="section" id="ref_host-and-guest-authentication-of-remote-nodes-remote-node-management"><div class="titlepage"><div><div><h2 class="title">28.1. Host and guest authentication of pacemaker_remote nodes</h2></div></div></div><p>
				The connection between cluster nodes and pacemaker_remote is secured using Transport Layer Security (TLS) with pre-shared key (PSK) encryption and authentication over TCP (using port 3121 by default). This means both the cluster node and the node running <code class="literal">pacemaker_remote</code> must share the same private key. By default this key must be placed at <code class="literal">/etc/pacemaker/authkey</code> on both cluster nodes and remote nodes.
			</p><p>
				The <code class="literal command">pcs cluster node add-guest</code> command sets up the <code class="literal">authkey</code> for guest nodes and the <code class="literal command">pcs cluster node add-remote</code> command sets up the <code class="literal">authkey</code> for remote nodes.
			</p></section><section class="section" id="assembly_configuring-kvm-guest-nodes-remote-node-management"><div class="titlepage"><div><div><h2 class="title">28.2. Configuring KVM guest nodes</h2></div></div></div><p>
				A Pacemaker guest node is a virtual guest node running the <code class="literal">pacemaker_remote</code> service. The virtual guest node is managed by the cluster.
			</p><section class="section" id="ref_guest-node-resource-options-configuring-kvm-guest-nodes"><div class="titlepage"><div><div><h3 class="title">28.2.1. Guest node resource options</h3></div></div></div><p>
					When configuring a virtual machine to act as a guest node, you create a <code class="literal">VirtualDomain</code> resource, which manages the virtual machine. For descriptions of the options you can set for a <code class="literal">VirtualDomain</code> resource, see <a class="xref" href="index.html#tb-virtdomain-options-HAAR" title="Table 24.1. Resource Options for Virtual Domain Resources">Table 24.1, “Resource Options for Virtual Domain Resources”</a>.
				</p><p>
					In addition to the <code class="literal">VirtualDomain</code> resource options, metadata options define the resource as a guest node and define the connection parameters. You set these resource options with the <code class="literal command">pcs cluster node add-guest</code> command. <a class="xref" href="index.html#tb-remoteklm-options-HAAR" title="Table 28.1. Metadata Options for Configuring KVM Resources as Remote Nodes">Table 28.1, “Metadata Options for Configuring KVM Resources as Remote Nodes”</a> describes these metadata options.
				</p><div class="table" id="tb-remoteklm-options-HAAR"><p class="title"><strong>Table 28.1. Metadata Options for Configuring KVM Resources as Remote Nodes</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319459240144" scope="col">Field</th><th align="left" valign="top" id="idm140319459239056" scope="col">Default</th><th align="left" valign="top" id="idm140319459237968" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319459240144"> <p>
									<code class="literal">remote-node</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319459239056"> <p>
									&lt;none&gt;
								</p>
								 </td><td align="left" valign="top" headers="idm140319459237968"> <p>
									The name of the guest node this resource defines. This both enables the resource as a guest node and defines the unique name used to identify the guest node. <span class="emphasis"><em>WARNING</em></span>: This value cannot overlap with any resource or node IDs.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319459240144"> <p>
									<code class="literal">remote-port</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319459239056"> <p>
									3121
								</p>
								 </td><td align="left" valign="top" headers="idm140319459237968"> <p>
									Configures a custom port to use for the guest connection to <code class="literal">pacemaker_remote</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319459240144"> <p>
									<code class="literal">remote-addr</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319459239056"> <p>
									The address provided in the <code class="literal">pcs host auth</code> command
								</p>
								 </td><td align="left" valign="top" headers="idm140319459237968"> <p>
									The IP address or host name to connect to
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319459240144"> <p>
									<code class="literal">remote-connect-timeout</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319459239056"> <p>
									60s
								</p>
								 </td><td align="left" valign="top" headers="idm140319459237968"> <p>
									Amount of time before a pending guest connection will time out
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="proc_integrating-vm-as-guest-node-configuring-kvm-guest-nodes"><div class="titlepage"><div><div><h3 class="title">28.2.2. Integrating a virtual machine as a guest node</h3></div></div></div><p>
					The following procedure is a high-level summary overview of the steps to perform to have Pacemaker launch a virtual machine and to integrate that machine as a guest node, using <code class="literal">libvirt</code> and KVM virtual guests.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Configure the <code class="literal">VirtualDomain</code> resources.
						</li><li class="listitem"><p class="simpara">
							Enter the following commands on every virtual machine to install <code class="literal">pacemaker_remote</code> packages, start the <code class="literal">pcsd</code> service and enable it to run on startup, and allow TCP port 3121 through the firewall.
						</p><pre class="literallayout"># <code class="literal">yum install pacemaker-remote resource-agents pcs</code>
# <code class="literal">systemctl start pcsd.service</code>
# <code class="literal">systemctl enable pcsd.service</code>
# <code class="literal">firewall-cmd --add-port 3121/tcp --permanent</code>
# <code class="literal">firewall-cmd --add-port 2224/tcp --permanent</code>
# <code class="literal">firewall-cmd --reload</code></pre></li><li class="listitem">
							Give each virtual machine a static network address and unique host name, which should be known to all nodes. For information on setting a static IP address for the guest virtual machine, see the <span class="emphasis"><em><span class="citetitle citetitle">Virtualization Deployment and Administration Guide</span></em></span>.
						</li><li class="listitem"><p class="simpara">
							If you have not already done so, authenticate <code class="literal">pcs</code> to the node you will be integrating as a quest node.
						</p><pre class="literallayout"># <code class="literal">pcs host auth <span class="emphasis"><em>nodename</em></span></code></pre></li><li class="listitem"><p class="simpara">
							Use the following command to convert an existing <code class="literal">VirtualDomain</code> resource into a guest node. This command must be run on a cluster node and not on the guest node which is being added. In addition to converting the resource, this command copies the <code class="literal">/etc/pacemaker/authkey</code> to the guest node and starts and enables the <code class="literal">pacemaker_remote</code> daemon on the guest node. The node name for the guest node, which you can define arbitrarily, can differ from the host name for the node.
						</p><pre class="literallayout"># <code class="literal">pcs cluster node add-guest <span class="emphasis"><em>nodename</em></span> <span class="emphasis"><em>resource_id</em></span></code> [<code class="literal"><span class="emphasis"><em>options</em></span></code>]</pre></li><li class="listitem"><p class="simpara">
							After creating the <code class="literal">VirtualDomain</code> resource, you can treat the guest node just as you would treat any other node in the cluster. For example, you can create a resource and place a resource constraint on the resource to run on the guest node as in the following commands, which are run from a cluster node. You can include guest nodes in groups, which allows you to group a storage device, file system, and VM.
						</p><pre class="literallayout"># <code class="literal">pcs resource create webserver apache configfile=/etc/httpd/conf/httpd.conf op monitor interval=30s</code>
# <code class="literal">pcs constraint location webserver prefers</code> <code class="literal"><span class="emphasis"><em>nodename</em></span></code></pre></li></ol></div></section></section><section class="section" id="assembly_configuring-remote-nodes-remote-node-management"><div class="titlepage"><div><div><h2 class="title">28.3. Configuring Pacemaker remote nodes</h2></div></div></div><p>
				A remote node is defined as a cluster resource with <code class="literal">ocf:pacemaker:remote</code> as the resource agent. You create this resource with the <code class="literal command">pcs cluster node add-remote</code> command.
			</p><section class="section" id="ref_remote-node-resource-options-configuring-remote-nodes"><div class="titlepage"><div><div><h3 class="title">28.3.1. Remote node resource options</h3></div></div></div><p>
					<a class="xref" href="index.html#tb-remotenode-options-HAAR" title="Table 28.2. Resource Options for Remote Nodes">Table 28.2, “Resource Options for Remote Nodes”</a> describes the resource options you can configure for a <code class="literal">remote</code> resource.
				</p><div class="table" id="tb-remotenode-options-HAAR"><p class="title"><strong>Table 28.2. Resource Options for Remote Nodes</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 38%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 38%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140319462217344" scope="col">Field</th><th align="left" valign="top" id="idm140319462216256" scope="col">Default</th><th align="left" valign="top" id="idm140319462215168" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140319462217344"> <p>
									<code class="literal">reconnect_interval</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319462216256"> <p>
									0
								</p>
								 </td><td align="left" valign="top" headers="idm140319462215168"> <p>
									Time in seconds to wait before attempting to reconnect to a remote node after an active connection to the remote node has been severed. This wait is recurring. If reconnect fails after the wait period, a new reconnect attempt will be made after observing the wait time. When this option is in use, Pacemaker will keep attempting to reach out and connect to the remote node indefinitely after each wait interval.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319462217344"> <p>
									<code class="literal">server</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319462216256"> <p>
									Address specified with <code class="literal">pcs host auth</code> command
								</p>
								 </td><td align="left" valign="top" headers="idm140319462215168"> <p>
									Server to connect to. This can be an IP address or host name.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140319462217344"> <p>
									<code class="literal">port</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140319462216256"> </td><td align="left" valign="top" headers="idm140319462215168"> <p>
									TCP port to connect to.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="proc-integrating-remote-nodes-configuring-remote-nodes"><div class="titlepage"><div><div><h3 class="title">28.3.2. Remote node configuration overview</h3></div></div></div><p>
					This section provides a high-level summary overview of the steps to perform to configure a Pacemaker Remote node and to integrate that node into an existing Pacemaker cluster environment.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On the node that you will be configuring as a remote node, allow cluster-related services through the local firewall.
						</p><pre class="literallayout"># <code class="literal">firewall-cmd --permanent --add-service=high-availability</code>
success
# <code class="literal">firewall-cmd --reload</code>
success</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you are using <code class="literal">iptables</code> directly, or some other firewall solution besides <code class="literal">firewalld</code>, simply open the following ports: TCP ports 2224 and 3121.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Install the <code class="literal command">pacemaker_remote</code> daemon on the remote node.
						</p><pre class="literallayout"># <code class="literal">yum install -y pacemaker-remote resource-agents pcs</code></pre></li><li class="listitem"><p class="simpara">
							Start and enable <code class="literal">pcsd</code> on the remote node.
						</p><pre class="literallayout"># <code class="literal">systemctl start pcsd.service</code>
# <code class="literal">systemctl enable pcsd.service</code></pre></li><li class="listitem"><p class="simpara">
							If you have not already done so, authenticate <code class="literal">pcs</code> to the node you will be adding as a remote node.
						</p><pre class="literallayout"># <code class="literal">pcs host auth remote1</code></pre></li><li class="listitem"><p class="simpara">
							Add the remote node resource to the cluster with the following command. This command also syncs all relevant configuration files to the new node, starts the node, and configures it to start <code class="literal">pacemaker_remote</code> on boot. This command must be run on a cluster node and not on the remote node which is being added.
						</p><pre class="literallayout"># <code class="literal">pcs cluster node add-remote remote1</code></pre></li><li class="listitem"><p class="simpara">
							After adding the <code class="literal">remote</code> resource to the cluster, you can treat the remote node just as you would treat any other node in the cluster. For example, you can create a resource and place a resource constraint on the resource to run on the remote node as in the following commands, which are run from a cluster node.
						</p><pre class="literallayout"># <code class="literal">pcs resource create webserver apache configfile=/etc/httpd/conf/httpd.conf op monitor interval=30s</code>
# <code class="literal">pcs constraint location webserver prefers remote1</code></pre><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
								Never involve a remote node connection resource in a resource group, colocation constraint, or order constraint.
							</p></div></div></li><li class="listitem">
							Configure fencing resources for the remote node. Remote nodes are fenced the same way as cluster nodes. Configure fencing resources for use with remote nodes the same as you would with cluster nodes. Note, however, that remote nodes can never initiate a fencing action. Only cluster nodes are capable of actually executing a fencing operation against another node.
						</li></ol></div></section></section><section class="section" id="proc_changing-default-port-location-remote-node-management"><div class="titlepage"><div><div><h2 class="title">28.4. Changing the default port location</h2></div></div></div><p>
				If you need to change the default port location for either Pacemaker or <code class="literal">pacemaker_remote</code>, you can set the <code class="literal">PCMK_remote_port</code> environment variable that affects both of these daemons. This environment variable can be enabled by placing it in the <code class="literal">/etc/sysconfig/pacemaker</code> file as follows.
			</p><pre class="literallayout"><span class="marked marked">==</span>==# Pacemaker Remote
...
#
# Specify a custom port for Pacemaker Remote connections
PCMK_remote_port=3121</pre><p>
				When changing the default port used by a particular guest node or remote node, the <code class="literal">PCMK_remote_port</code> variable must be set in that node’s <code class="literal">/etc/sysconfig/pacemaker</code> file, and the cluster resource creating the guest node or remote node connection must also be configured with the same port number (using the <code class="literal">remote-port</code> metadata option for guest nodes, or the <code class="literal">port</code> option for remote nodes).
			</p></section><section class="section" id="proc_upgrading-systems-with-pacemaker-remote-remote-node-management"><div class="titlepage"><div><div><h2 class="title">28.5. Upgrading systems with pacemaker_remote nodes</h2></div></div></div><p>
				If the <code class="literal">pacemaker_remote</code> service is stopped on an active Pacemaker Remote node, the cluster will gracefully migrate resources off the node before stopping the node. This allows you to perform software upgrades and other routine maintenance procedures without removing the node from the cluster. Once <code class="literal">pacemaker_remote</code> is shut down, however, the cluster will immediately try to reconnect. If <code class="literal">pacemaker_remote</code> is not restarted within the resource’s monitor timeout, the cluster will consider the monitor operation as failed.
			</p><p>
				If you wish to avoid monitor failures when the <code class="literal">pacemaker_remote</code> service is stopped on an active Pacemaker Remote node, you can use the following procedure to take the node out of the cluster before performing any system administration that might stop <code class="literal">pacemaker_remote</code>
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Stop the node’s connection resource with the <code class="literal command">pcs resource disable <span class="emphasis"><em>resourcename</em></span></code>, which will move all services off the node. For guest nodes, this will also stop the VM, so the VM must be started outside the cluster (for example, using <code class="literal command">virsh</code>) to perform any maintenance.
					</li><li class="listitem">
						Perform the required maintenance.
					</li><li class="listitem">
						When ready to return the node to the cluster, re-enable the resource with the <code class="literal command">pcs resource enable</code>.
					</li></ol></div></section></section><section class="chapter" id="assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 29. Performing cluster maintenance</h1></div></div></div><p>
			In order to perform maintenance on the nodes of your cluster, you may need to stop or move the resources and services running on that cluster. Or you may need to stop the cluster software while leaving the services untouched. Pacemaker provides a variety of methods for performing system maintenance.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					If you need to stop a node in a cluster while continuing to provide the services running on that cluster on another node, you can put the cluster node in standby mode. A node that is in standby mode is no longer able to host resources. Any resource currently active on the node will be moved to another node, or stopped if no other node is eligible to run the resource. For information on standby mode, see <a class="link" href="index.html#proc_stopping-individual-node-cluster-maintenance" title="29.1. Putting a node into standby mode">Putting a node into standby mode</a>.
				</li><li class="listitem"><p class="simpara">
					If you need to move an individual resource off the node on which it is currently running without stopping that resource, you can use the <code class="literal command">pcs resource move</code> command to move the resource to a different node. For information on the <code class="literal command">pcs resource move</code> command, see <a class="link" href="index.html#assembly_manually-move-resources-cluster-maintenance" title="29.2. Manually moving cluster resources">Manually moving cluster resources</a>.
				</p><p class="simpara">
					When you execute the <code class="literal command">pcs resource move</code> command, this adds a constraint to the resource to prevent it from running on the node on which it is currently running. When you are ready to move the resource back, you can execute the <code class="literal command">pcs resource clear</code> or the <code class="literal command">pcs constraint delete</code> command to remove the constraint. This does not necessarily move the resources back to the original node, however, since where the resources can run at that point depends on how you have configured your resources initially. You can relocate a resource to its preferred node with the <code class="literal command">pcs resource relocate run</code> command, as described in <a class="link" href="index.html#proc_moving-resource-to-preferred-node-manually-move-resources" title="29.2.2. Moving a resource to its preferred node">Moving a resource to its preferred node</a>.
				</p></li><li class="listitem">
					If you need to stop a running resource entirely and prevent the cluster from starting it again, you can use the <code class="literal command">pcs resource disable</code> command. For information on the <code class="literal command">pcs resource disable</code> command, see <a class="link" href="index.html#proc_disabling-resources-cluster-maintenance" title="29.3. Disabling, enabling, and banning cluster resources">Enabling, disabling, and banning cluster resources</a>.
				</li><li class="listitem">
					If you want to prevent Pacemaker from taking any action for a resource (for example, if you want to disable recovery actions while performing maintenance on the resource, or if you need to reload the <code class="literal">/etc/sysconfig/pacemaker</code> settings), use the <code class="literal command">pcs resource unmanage</code> command, as described in <a class="link" href="index.html#proc_unmanaging-resources-cluster-maintenance" title="29.4. Setting a resource to unmanaged mode">Setting a resource to unmanaged mode</a>. Pacemaker Remote connection resources should never be unmanaged.
				</li><li class="listitem">
					If you need to put the cluster in a state where no services will be started or stopped, you can set the <code class="literal">maintenance-mode</code> cluster property. Putting the cluster into maintenance mode automatically unmanages all resources. For information on putting the cluster in maintenance mode, see <a class="link" href="index.html#proc_setting-maintenance-mode-cluster-maintenance" title="29.5. Putting a cluster in maintenance mode">Putting a cluster in maintenance mode</a>.
				</li><li class="listitem">
					If you need to update the packages that make up the RHEL High Availability and Resilient Storage Add-Ons, you can update the packages on one node at a time or on the entire cluster as a whole, as summarized in <a class="link" href="index.html#proc_updating-cluster-packages-cluster-maintenance" title="29.6. Updating a RHEL high availability cluster">Updating a Red Hat Enterprise Linux high availability cluster</a>.
				</li><li class="listitem">
					If you need to perform maintenance on a Pacemaker remote node, you can remove that node from the cluster by disabling the remote node resource, as described in <a class="link" href="index.html#proc_upgrading-remote-nodes-cluster-maintenance" title="29.7. Upgrading remote nodes and guest nodes">Upgrading remote nodes and guest nodes</a>.
				</li></ul></div><section class="section" id="proc_stopping-individual-node-cluster-maintenance"><div class="titlepage"><div><div><h2 class="title">29.1. Putting a node into standby mode</h2></div></div></div><p>
				When a cluster node is in standby mode, the node is no longer able to host resources. Any resources currently active on the node will be moved to another node.
			</p><p>
				The following command puts the specified node into standby mode. If you specify the <code class="literal option">--all</code>, this command puts all nodes into standby mode.
			</p><p>
				You can use this command when updating a resource’s packages. You can also use this command when testing a configuration, to simulate recovery without actually shutting down a node.
			</p><pre class="literallayout">pcs node standby <span class="emphasis"><em>node</em></span> | --all</pre><p>
				The following command removes the specified node from standby mode. After running this command, the specified node is then able to host resources. If you specify the <code class="literal option">--all</code>, this command removes all nodes from standby mode.
			</p><pre class="literallayout">pcs node unstandby <span class="emphasis"><em>node</em></span> | --all</pre><p>
				Note that when you execute the <code class="literal command">pcs node standby</code> command, this prevents resources from running on the indicated node. When you execute the <code class="literal command">pcs node unstandby</code> command, this allows resources to run on the indicated node. This does not necessarily move the resources back to the indicated node; where the resources can run at that point depends on how you have configured your resources initially.
			</p></section><section class="section" id="assembly_manually-move-resources-cluster-maintenance"><div class="titlepage"><div><div><h2 class="title">29.2. Manually moving cluster resources</h2></div></div></div><p>
				You can override the cluster and force resources to move from their current location. There are two occasions when you would want to do this:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						When a node is under maintenance, and you need to move all resources running on that node to a different node
					</li><li class="listitem">
						When individually specified resources needs to be moved
					</li></ul></div><p>
				To move all resources running on a node to a different node, you put the node in standby mode.
			</p><p>
				You can move individually specified resources in either of the following ways.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You can use the <code class="literal command">pcs resource move</code> command to move a resource off a node on which it is currently running.
					</li><li class="listitem">
						You can use the <code class="literal command">pcs resource relocate run</code> command to move a resource to its preferred node, as determined by current cluster status, constraints, location of resources and other settings.
					</li></ul></div><section class="section" id="proc_moving-resource-from-node-manually-move-resources"><div class="titlepage"><div><div><h3 class="title">29.2.1. Moving a resource from its current node</h3></div></div></div><p>
					To move a resource off the node on which it is currently running, use the following command, specifying the <span class="emphasis"><em>resource_id</em></span> of the resource as defined. Specify the <code class="literal">destination_node</code> if you want to indicate on which node to run the resource that you are moving.
				</p><pre class="literallayout">pcs resource move <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>destination_node</em></span>] [--master] [lifetime=<span class="emphasis"><em>lifetime</em></span>]</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When you execute the <code class="literal command">pcs resource move</code> command, this adds a constraint to the resource to prevent it from running on the node on which it is currently running. You can execute the <code class="literal command">pcs resource clear</code> or the <code class="literal command">pcs constraint delete</code> command to remove the constraint. This does not necessarily move the resources back to the original node; where the resources can run at that point depends on how you have configured your resources initially.
					</p></div></div><p>
					If you specify the <code class="literal">--master</code> parameter of the <code class="literal command">pcs resource move</code> command, the scope of the constraint is limited to the master role and you must specify <span class="emphasis"><em>master_id</em></span> rather than <span class="emphasis"><em>resource_id</em></span>.
				</p><p>
					You can optionally configure a <code class="literal">lifetime</code> parameter for the <code class="literal">pcs resource move</code> command to indicate a period of time the constraint should remain. You specify the units of a <code class="literal">lifetime</code> parameter according to the format defined in ISO 8601, which requires that you specify the unit as a capital letter such as Y (for years), M (for months), W (for weeks), D (for days), H (for hours), M (for minutes), and S (for seconds).
				</p><p>
					To distinguish a unit of minutes(M) from a unit of months(M), you must specify PT before indicating the value in minutes. For example, a <code class="literal">lifetime</code> parameter of 5M indicates an interval of five months, while a <code class="literal">lifetime</code> parameter of PT5M indicates an interval of five minutes.
				</p><p>
					The <code class="literal">lifetime</code> parameter is checked at intervals defined by the <code class="literal">cluster-recheck-interval</code> cluster property. By default this value is 15 minutes. If your configuration requires that you check this parameter more frequently, you can reset this value with the following command.
				</p><pre class="literallayout">pcs property set cluster-recheck-interval=<span class="emphasis"><em>value</em></span></pre><p>
					You can optionally configure a <code class="literal">--wait[=<span class="emphasis"><em>n</em></span>]</code> parameter for the <code class="literal">pcs resource move</code> command to indicate the number of seconds to wait for the resource to start on the destination node before returning 0 if the resource is started or 1 if the resource has not yet started. If you do not specify n, the default resource timeout will be used.
				</p><p>
					The following command moves the resource <code class="literal">resource1</code> to node <code class="literal">example-node2</code> and prevents it from moving back to the node on which it was originally running for one hour and thirty minutes.
				</p><pre class="literallayout">pcs resource move resource1 example-node2 lifetime=PT1H30M</pre><p>
					The following command moves the resource <code class="literal">resource1</code> to node <code class="literal">example-node2</code> and prevents it from moving back to the node on which it was originally running for thirty minutes.
				</p><pre class="literallayout">pcs resource move resource1 example-node2 lifetime=PT30M</pre></section><section class="section" id="proc_moving-resource-to-preferred-node-manually-move-resources"><div class="titlepage"><div><div><h3 class="title">29.2.2. Moving a resource to its preferred node</h3></div></div></div><p>
					After a resource has moved, either due to a failover or to an administrator manually moving the node, it will not necessarily move back to its original node even after the circumstances that caused the failover have been corrected. To relocate resources to their preferred node, use the following command. A preferred node is determined by the current cluster status, constraints, resource location, and other settings and may change over time.
				</p><pre class="literallayout">pcs resource relocate run [<span class="emphasis"><em>resource1</em></span>] [<span class="emphasis"><em>resource2</em></span>] ...</pre><p>
					If you do not specify any resources, all resource are relocated to their preferred nodes.
				</p><p>
					This command calculates the preferred node for each resource while ignoring resource stickiness. After calculating the preferred node, it creates location constraints which will cause the resources to move to their preferred nodes. Once the resources have been moved, the constraints are deleted automatically. To remove all constraints created by the <code class="literal command">pcs resource relocate run</code> command, you can enter the <code class="literal command">pcs resource relocate clear</code> command. To display the current status of resources and their optimal node ignoring resource stickiness, enter the <code class="literal command">pcs resource relocate show</code> command.
				</p></section></section><section class="section" id="proc_disabling-resources-cluster-maintenance"><div class="titlepage"><div><div><h2 class="title">29.3. Disabling, enabling, and banning cluster resources</h2></div></div></div><p>
				In addition to the <code class="literal command">pcs resource move</code> and <code class="literal command">pcs resource relocate</code> commands, there are a variety of other commands you can use to control the behavior of cluster resources.
			</p><h4 id="disabling_a_cluster_resource">Disabling a cluster resource</h4><p>
				You can manually stop a running resource and prevent the cluster from starting it again with the following command. Depending on the rest of the configuration (constraints, options, failures, and so on), the resource may remain started. If you specify the <code class="literal option">--wait</code> option, <span class="strong strong"><strong><span class="application application">pcs</span></strong></span> will wait up to 'n' seconds for the resource to stop and then return 0 if the resource is stopped or 1 if the resource has not stopped. If 'n' is not specified it defaults to 60 minutes.
			</p><pre class="literallayout">pcs resource disable <span class="emphasis"><em>resource_id</em></span> [--wait[=<span class="emphasis"><em>n</em></span>]]</pre><p>
				As of Red Hat Enterprise Linux 8.2, you can specify that a resource be disabled only if disabling the resource would not have an effect on other resources. Ensuring that this would be the case can be impossible to do by hand when complex resource relations are set up.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The <code class="literal command">pcs resource disable --simulate</code> command shows the effects of disabling a resource while not changing the cluster configuration.
					</li><li class="listitem">
						The <code class="literal command">pcs resource disable --safe</code> command disables a resource only if no other resources would be affected in any way, such as being migrated from one node to another. The <code class="literal command">pcs resource safe-disable</code> command is an alias for the <code class="literal">pcs resource disable --safe</code> command.
					</li><li class="listitem">
						The <code class="literal command">pcs resource disable --safe --no-strict</code> command disables a resource only if no other resources would be stopped or demoted
					</li></ul></div><h4 id="enabling_a_cluster_resource">Enabling a cluster resource</h4><p>
				Use the following command to allow the cluster to start a resource. Depending on the rest of the configuration, the resource may remain stopped. If you specify the <code class="literal option">--wait</code> option, <span class="strong strong"><strong><span class="application application">pcs</span></strong></span> will wait up to 'n' seconds for the resource to start and then return 0 if the resource is started or 1 if the resource has not started. If 'n' is not specified it defaults to 60 minutes.
			</p><pre class="literallayout">pcs resource enable <span class="emphasis"><em>resource_id</em></span> [--wait[=<span class="emphasis"><em>n</em></span>]]</pre><h4 id="preventing_a_resource_from_running_on_a_particular_node">Preventing a resource from running on a particular node</h4><p>
				Use the following command to prevent a resource from running on a specified node, or on the current node if no node is specified.
			</p><pre class="literallayout">pcs resource ban <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>node</em></span>] [--master] [lifetime=<span class="emphasis"><em>lifetime</em></span>] [--wait[=<span class="emphasis"><em>n</em></span>]]</pre><p>
				Note that when you execute the <code class="literal command">pcs resource ban</code> command, this adds a -INFINITY location constraint to the resource to prevent it from running on the indicated node. You can execute the <code class="literal command">pcs resource clear</code> or the <code class="literal command">pcs constraint delete</code> command to remove the constraint. This does not necessarily move the resources back to the indicated node; where the resources can run at that point depends on how you have configured your resources initially.
			</p><p>
				If you specify the <code class="literal">--master</code> parameter of the <code class="literal command">pcs resource ban</code> command, the scope of the constraint is limited to the master role and you must specify <span class="emphasis"><em>master_id</em></span> rather than <span class="emphasis"><em>resource_id</em></span>.
			</p><p>
				You can optionally configure a <code class="literal">lifetime</code> parameter for the <code class="literal">pcs resource ban</code> command to indicate a period of time the constraint should remain.
			</p><p>
				You can optionally configure a <code class="literal">--wait[=<span class="emphasis"><em>n</em></span>]</code> parameter for the <code class="literal">pcs resource ban</code> command to indicate the number of seconds to wait for the resource to start on the destination node before returning 0 if the resource is started or 1 if the resource has not yet started. If you do not specify n, the default resource timeout will be used.
			</p><h4 id="forcing_a_resource_to_start_on_the_current_node">Forcing a resource to start on the current node</h4><p>
				Use the <code class="literal command">debug-start</code> parameter of the <code class="literal command">pcs resource</code> command to force a specified resource to start on the current node, ignoring the cluster recommendations and printing the output from starting the resource. This is mainly used for debugging resources; starting resources on a cluster is (almost) always done by Pacemaker and not directly with a <code class="literal command">pcs</code> command. If your resource is not starting, it is usually due to either a misconfiguration of the resource (which you debug in the system log), constraints that prevent the resource from starting, or the resource being disabled. You can use this command to test resource configuration, but it should not normally be used to start resources in a cluster.
			</p><p>
				The format of the <code class="literal command">debug-start</code> command is as follows.
			</p><pre class="literallayout">pcs resource debug-start <span class="emphasis"><em>resource_id</em></span></pre></section><section class="section" id="proc_unmanaging-resources-cluster-maintenance"><div class="titlepage"><div><div><h2 class="title">29.4. Setting a resource to unmanaged mode</h2></div></div></div><p>
				When a resource is in <code class="literal">unmanaged</code> mode, the resource is still in the configuration but Pacemaker does not manage the resource.
			</p><p>
				The following command sets the indicated resources to <code class="literal">unmanaged</code> mode.
			</p><pre class="literallayout">pcs resource unmanage <span class="emphasis"><em>resource1</em></span>  [<span class="emphasis"><em>resource2</em></span>] ...</pre><p>
				The following command sets resources to <code class="literal">managed</code> mode, which is the default state.
			</p><pre class="literallayout">pcs resource manage <span class="emphasis"><em>resource1</em></span>  [<span class="emphasis"><em>resource2</em></span>] ...</pre><p>
				You can specify the name of a resource group with the <code class="literal command">pcs resource manage</code> or <code class="literal command">pcs resource unmanage</code> command. The command will act on all of the resources in the group, so that you can set all of the resources in a group to <code class="literal">managed</code> or <code class="literal">unmanaged</code> mode with a single command and then manage the contained resources individually.
			</p></section><section class="section" id="proc_setting-maintenance-mode-cluster-maintenance"><div class="titlepage"><div><div><h2 class="title">29.5. Putting a cluster in maintenance mode</h2></div></div></div><p>
				When a cluster is in maintenance mode, the cluster does not start or stop any services until told otherwise. When maintenance mode is completed, the cluster does a sanity check of the current state of any services, and then stops or starts any that need it.
			</p><p>
				To put a cluster in maintenance mode, use the following command to set the <code class="literal">maintenance-mode</code> cluster property to <code class="literal">true</code>.
			</p><pre class="literallayout"># <code class="literal">pcs property set maintenance-mode=true</code></pre><p>
				To remove a cluster from maintenance mode, use the following command to set the <code class="literal">maintenance-mode</code> cluster property to <code class="literal">false</code>.
			</p><pre class="literallayout"># <code class="literal">pcs property set maintenance-mode=false</code></pre><p>
				u can remove a cluster property from the configuration with the following command.
			</p><pre class="literallayout">pcs property unset <span class="emphasis"><em>property</em></span></pre><p>
				Alternately, you can remove a cluster property from a configuration by leaving the value field of the <code class="literal command">pcs property set</code> command blank. This restores that property to its default value. For example, if you have previously set the <code class="literal">symmetric-cluster</code> property to <code class="literal">false</code>, the following command removes the value you have set from the configuration and restores the value of <code class="literal">symmetric-cluster</code> to <code class="literal">true</code>, which is its default value.
			</p><pre class="literallayout"># <code class="literal">pcs property set symmetric-cluster=</code></pre></section><section class="section" id="proc_updating-cluster-packages-cluster-maintenance"><div class="titlepage"><div><div><h2 class="title">29.6. Updating a RHEL high availability cluster</h2></div></div></div><p>
				Updating packages that make up the RHEL High Availability and Resilient Storage Add-Ons, either individually or as a whole, can be done in one of two general ways:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<span class="emphasis"><em>Rolling Updates</em></span>: Remove one node at a time from service, update its software, then integrate it back into the cluster. This allows the cluster to continue providing service and managing resources while each node is updated.
					</li><li class="listitem">
						<span class="emphasis"><em>Entire Cluster Update</em></span>: Stop the entire cluster, apply updates to all nodes, then start the cluster back up.
					</li></ul></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					It is critical that when performing software update procedures for Red Hat Enterprise LInux High Availability and Resilient Storage clusters, you ensure that any node that will undergo updates is not an active member of the cluster before those updates are initiated.
				</p></div></div><p>
				For a full description of each of these methods and the procedures to follow for the updates, see <a class="link" href="https://access.redhat.com/articles/2059253/">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster</a>.
			</p></section><section class="section" id="proc_upgrading-remote-nodes-cluster-maintenance"><div class="titlepage"><div><div><h2 class="title">29.7. Upgrading remote nodes and guest nodes</h2></div></div></div><p>
				If the <code class="literal">pacemaker_remote</code> service is stopped on an active remote node or guest node, the cluster will gracefully migrate resources off the node before stopping the node. This allows you to perform software upgrades and other routine maintenance procedures without removing the node from the cluster. Once <code class="literal">pacemaker_remote</code> is shut down, however, the cluster will immediately try to reconnect. If <code class="literal">pacemaker_remote</code> is not restarted within the resource’s monitor timeout, the cluster will consider the monitor operation as failed.
			</p><p>
				If you wish to avoid monitor failures when the <code class="literal">pacemaker_remote</code> service is stopped on an active Pacemaker Remote node, you can use the following procedure to take the node out of the cluster before performing any system administration that might stop <code class="literal">pacemaker_remote</code>
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Stop the node’s connection resource with the <code class="literal command">pcs resource disable <span class="emphasis"><em>resourcename</em></span></code>, which will move all services off the node. For guest nodes, this will also stop the VM, so the VM must be started outside the cluster (for example, using <code class="literal command">virsh</code>) to perform any maintenance.
					</li><li class="listitem">
						Perform the required maintenance.
					</li><li class="listitem">
						When ready to return the node to the cluster, re-enable the resource with the <code class="literal command">pcs resource enable</code>.
					</li></ol></div></section></section><section class="chapter" id="assembly_configuring-disaster-recovery-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 30. Configuring disaster recovery clusters</h1></div></div></div><p>
			One method of providing disaster recovery for a high availability cluster is to configure two clusters. You can then configure one cluster as your primary site cluster, and the second cluster as your disaster recovery cluster.
		</p><p>
			In normal circumstances, the primary cluster is running resources in production mode. The disaster recovery cluster has all the resources configured as well and is either running them in demoted mode or not at all. For example, there may be a database running in the primary cluster in promoted mode and running in the disaster recovery cluster in demoted mode. The database in this setup would be configured so that data is synchronized from the primary to disaster recovery site. This is done through the database configuration itself rather than through the <code class="literal">pcs</code> command interface.
		</p><p>
			When the primary cluster goes down, users can use the <code class="literal">pcs</code> command interface to manually fail the resources over to the disaster recovery site. They can then log in to the disaster site and promote and start the resources there. Once the primary cluster has recovered, users can use the <code class="literal">pcs</code> command interface to manually move resources back to the primary site.
		</p><p>
			As of Red Hat Enterprise Linux 8.2, you can use the <code class="literal">pcs</code> command to display the status of both the primary and the disaster recovery site cluster from a single node on either site.
		</p><section class="section" id="ref_recovery-considerations-configuring-disaster-recovery"><div class="titlepage"><div><div><h2 class="title">30.1. Considerations for disaster recovery clusters</h2></div></div></div><p>
				When planning and configuring a disaster recovery site that you will manage and monitor with the <code class="literal">pcs</code> command interface, note the following considerations.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The disaster recovery site must be a cluster. This makes it possible to configure it with same tools and similar procedures as the primary site.
					</li><li class="listitem">
						The primary and disaster recovery clusters are created by independent <code class="literal">pcs cluster setup</code> commands.
					</li><li class="listitem">
						The clusters and their resources must be configured so that that the data is synchronized and failover is possible.
					</li><li class="listitem">
						The cluster nodes in the recovery site can not have the same names as the nodes in the primary site.
					</li><li class="listitem">
						The pcs user <code class="literal">hacluster</code> must be authenticated for each node in both clusters on the node from which you will be running <code class="literal">pcs</code> commands.
					</li></ul></div></section><section class="section" id="proc_disaster-recovery-display-configuring-disaster-recovery"><div class="titlepage"><div><div><h2 class="title">30.2. Displaying status of recovery clusters (RHEL 8.2 and later)</h2></div></div></div><p>
				To configure a primary and a disaster recovery cluster so that you can display the status of both clusters, perform the following procedure.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Setting up a disaster recovery cluster does not automatically configure resources or replicate data. Those items must be configured manually by the user.
				</p></div></div><p>
				In this example:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The primary cluster will be named <code class="literal">PrimarySite</code> and will consist of the nodes <code class="literal">z1.example.com</code>. and <code class="literal">z2.example.com</code>.
					</li><li class="listitem">
						The disaster recovery site cluster will be named <code class="literal">DRsite</code> and will consist of the nodes <code class="literal">z3.example.com</code> and <code class="literal">z4.example.com</code>.
					</li></ul></div><p>
				This example sets up a basic cluster with no resources or fencing configured.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Authenticate all of the nodes that will be used for both clusters.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs host auth z1.example.com z2.example.com z3.example.com z4.example.com -u hacluster -p password</code>
z1.example.com: Authorized
z2.example.com: Authorized
z3.example.com: Authorized
z4.example.com: Authorized</pre></li><li class="listitem"><p class="simpara">
						Create the cluster that will be used as the primary cluster and start cluster services for the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs cluster setup PrimarySite z1.example.com z2.example.com --start</code>
{...}
Cluster has been successfully set up.
Starting cluster on hosts: 'z1.example.com', 'z2.example.com'...</pre></li><li class="listitem"><p class="simpara">
						Create the cluster that will be used as the disaster recovery cluster and start cluster services for the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs cluster setup DRSite z3.example.com z4.example.com --start</code>
{...}
Cluster has been successfully set up.
Starting cluster on hosts: 'z3.example.com', 'z4.example.com'...</pre></li><li class="listitem"><p class="simpara">
						From a node in the primary cluster, set up the second cluster as the recovery site. The recovery site is defined by a name of one of its nodes.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs dr set-recovery-site z3.example.com</code>
Sending 'disaster-recovery config' to 'z3.example.com', 'z4.example.com'
z3.example.com: successful distribution of the file 'disaster-recovery config'
z4.example.com: successful distribution of the file 'disaster-recovery config'
Sending 'disaster-recovery config' to 'z1.example.com', 'z2.example.com'
z1.example.com: successful distribution of the file 'disaster-recovery config'
z2.example.com: successful distribution of the file 'disaster-recovery config'</pre></li><li class="listitem"><p class="simpara">
						Check the disaster recovery configuration.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs dr config</code>
Local site:
  Role: Primary
Remote site:
  Role: Recovery
  Nodes:
    z1.example.com
    z2.example.com</pre></li><li class="listitem"><p class="simpara">
						Check the status of the primary cluster and the disaster recovery cluster from a node in the primary cluster.
					</p><pre class="literallayout">[root@z1 ~]# <code class="literal">pcs dr status</code>
--- Local cluster - Primary site ---
Cluster name: PrimarySite

WARNINGS:
No stonith devices and stonith-enabled is not false

Cluster Summary:
  * Stack: corosync
  * Current DC: z2.example.com (version 2.0.3-2.el8-2c9cea563e) - partition with quorum
  * Last updated: Mon Dec  9 04:10:31 2019
  * Last change:  Mon Dec  9 04:06:10 2019 by hacluster via crmd on z2.example.com
  * 2 nodes configured
  * 0 resource instances configured

Node List:
  * Online: [ z1.example.com z2.example.com ]

Full List of Resources:
  * No resources

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled


--- Remote cluster - Recovery site ---
Cluster name: DRSite

WARNINGS:
No stonith devices and stonith-enabled is not false

Cluster Summary:
  * Stack: corosync
  * Current DC: z4.example.com (version 2.0.3-2.el8-2c9cea563e) - partition with quorum
  * Last updated: Mon Dec  9 04:10:34 2019
  * Last change:  Mon Dec  9 04:09:55 2019 by hacluster via crmd on z4.example.com
  * 2 nodes configured
  * 0 resource instances configured

Node List:
  * Online: [ z3.example.com z4.example.com ]

Full List of Resources:
  * No resources

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled</pre></li></ol></div><p>
				For additional display options for a disaster recovery configuration, see the help screen for the <code class="literal">pcs dr</code> command.
			</p></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm140319475256848"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2020 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></div>
  
  </div>
  </div>
</div>
<div id="comments-footer" class="book-comments">
  </div>
<meta itemscope="" itemref="md1">

    <!-- Display: Next/Previous Nav -->
          
      </div>
</article>


  

            </div>
        </main>
    </div>
    <!--googleoff: all-->
    <div id="to-top"><a class="btn_slideto" href="index.html#masthead" aria-label="Back to Top"><span class="web-icon-upload"></span></a></div>
    <footer class="footer-main">
        <div class="footer-top">
            <div class="container">

              <div class="brand">
                <a href="https://redhat.com">
                  <svg class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 613 145">
                    <defs>
                      <style>
                        .rh-logo-hat {
                          fill: #e00;
                        }
                        .rh-logo-type {
                          fill: #fff;
                        }
                      </style>
                    </defs>
                    <title>Red Hat</title>
                    <path
                      class="rh-logo-hat"
                      d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/>
                      <path class="rh-logo-band"
                      d="M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45,12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/>
                      <path
                      class="rh-logo-type"
                      d="M579.74,92.8c0,11.89,7.15,17.67,20.19,17.67a52.11,52.11,0,0,0,11.89-1.68V95a24.84,24.84,0,0,1-7.68,1.16c-5.37,0-7.36-1.68-7.36-6.73V68.3h15.56V54.1H596.78v-18l-17,3.68V54.1H568.49V68.3h11.25Zm-53,.32c0-3.68,3.69-5.47,9.26-5.47a43.12,43.12,0,0,1,10.1,1.26v7.15a21.51,21.51,0,0,1-10.63,2.63c-5.46,0-8.73-2.1-8.73-5.57m5.2,17.56c6,0,10.84-1.26,15.36-4.31v3.37h16.82V74.08c0-13.56-9.14-21-24.39-21-8.52,0-16.94,2-26,6.1l6.1,12.52c6.52-2.74,12-4.42,16.83-4.42,7,0,10.62,2.73,10.62,8.31v2.73a49.53,49.53,0,0,0-12.62-1.58c-14.31,0-22.93,6-22.93,16.73,0,9.78,7.78,17.24,20.19,17.24m-92.44-.94h18.09V80.92h30.29v28.82H506V36.12H487.93V64.41H457.64V36.12H439.55ZM370.62,81.87c0-8,6.31-14.1,14.62-14.1A17.22,17.22,0,0,1,397,72.09V91.54A16.36,16.36,0,0,1,385.24,96c-8.2,0-14.62-6.1-14.62-14.09m26.61,27.87h16.83V32.44l-17,3.68V57.05a28.3,28.3,0,0,0-14.2-3.68c-16.19,0-28.92,12.51-28.92,28.5a28.25,28.25,0,0,0,28.4,28.6,25.12,25.12,0,0,0,14.93-4.83ZM320,67c5.36,0,9.88,3.47,11.67,8.83H308.47C310.15,70.3,314.36,67,320,67M291.33,82c0,16.2,13.25,28.82,30.28,28.82,9.36,0,16.2-2.53,23.25-8.42l-11.26-10c-2.63,2.74-6.52,4.21-11.14,4.21a14.39,14.39,0,0,1-13.68-8.83h39.65V83.55c0-17.67-11.88-30.39-28.08-30.39a28.57,28.57,0,0,0-29,28.81M262,51.58c6,0,9.36,3.78,9.36,8.31S268,68.2,262,68.2H244.11V51.58Zm-36,58.16h18.09V82.92h13.77l13.89,26.82H292l-16.2-29.45a22.27,22.27,0,0,0,13.88-20.72c0-13.25-10.41-23.45-26-23.45H226Z"/>
                  </svg>
                </a>
              </div>

            <div role="navigation">
                <h3>Quick Links</h3>
                <ul>
                    <li><a class="download-software" href="https://access.redhat.com/downloads/">Downloads</a></li>
                    <li><a class="manage-subscriptions" href="https://access.redhat.com/management/subscriptions/#active">Subscriptions</a></li>
                    <li><a class="support-cases" href="https://access.redhat.com/support">Support Cases</a></li>
                    <li><a class="customer-service" href="https://access.redhat.com/support/customer-service">Customer Service</a></li>
                    <li><a class="quick-docs" href="https://access.redhat.com/documentation">Product Documentation</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>Help</h3>
                <ul>
                    <li><a class="contact-us" href="https://access.redhat.com/support/contact/">Contact Us</a></li>
                    <li><a class="cp-faqs" href="https://access.redhat.com/articles/33844">Customer Portal FAQ</a></li>
                    <li><a class="login-problems" href="https://access.redhat.com/help/login_assistance">Log-in Assistance</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>Site Info</h3>
                <ul>
                  <li><a class="trust-red-hat" href="https://www.redhat.com/en/trust">Trust Red Hat</a></li>
                  <li><a class="browser-support-policy" href="https://access.redhat.com/help/browsers/">Browser Support Policy</a></li>
                  <li><a class="accessibility" href="https://access.redhat.com/help/accessibility/">Accessibility</a></li>
                  <li><a class="recognition" href="https://access.redhat.com/recognition/">Awards and Recognition</a></li>
                  <li><a class="colophon" href="https://access.redhat.com/help/colophon/">Colophon</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>Related Sites</h3>
                <ul>
                    <li><a href="https://www.redhat.com/" class="red-hat-com">redhat.com</a></li>
                    <li><a href="https://www.openshift.com" class="openshift-com">openshift.com</a></li>
                    <li><a href="http://developers.redhat.com/" class="red-hat-developers">developers.redhat.com</a></li>
                    <li><a href="https://connect.redhat.com/" class="partner-connect">connect.redhat.com</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>About</h3>
                <ul>
                    <li><a href="https://access.redhat.com/subscription-value" class="subscription-value">Red Hat Subscription Value</a></li>
                    <li><a href="https://www.redhat.com/about/" class="about-red-hat">About Red Hat</a></li>
                    <li><a href="http://jobs.redhat.com" class="about-jobs">Red Hat Jobs</a></li>
                </ul>
            </div>

            </div>
        </div>

        <div class="anchor">
            <div class="container">
                <div class="status-legal">
                    <a hidden href="https://status.redhat.com" class="status-page-widget">
                          <span class="status-description"></span>
                          <span class="status-dot shape-circle"></span>
                    </a>
                    <div class="legal-copyright">
                        <div class="copyright">Copyright © 2020 Red Hat, Inc.</div>

                        <div role="navigation" class="legal">
                            <ul>
                                <li><a href="http://www.redhat.com/en/about/privacy-policy" class="privacy-policy">Privacy Statement</a></li>
                                <li><a href="https://access.redhat.com/help/terms/" class="terms-of-use">Customer Portal Terms of Use</a></li>
                                <li><a href="http://www.redhat.com/en/about/all-policies-guidelines" class="all-policies">All Policies and Guidelines</a></li>
                            </ul>
                        </div>
                    </div>
                </div>
                <div class="social">
                    <a href="http://www.redhat.com/summit/" class="summit">
                      <img src="l_summit.png" alt="Red Hat Summit" />
                    </a>

                    <div class="social-media">
                        <a href="https://twitter.com/RedHatSupport" class="sm-icon twitter"><span class="nicon-twitter"></span><span class="offscreen">Twitter</span></a>
                        <a href="https://www.facebook.com/RedHatSupport" class="sm-icon facebook"><span class="nicon-facebook"></span><span class="offscreen">Facebook</span></a>
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!--googleon: all-->
</div>
  
  <div id="formatHelp" class="modal fade" tabindex="-1" role="dialog" aria-labelledby="formatTitle" aria-hidden="true"><div class="modal-dialog"><div class="modal-content">
    <div class="modal-header">
      <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
      <h3 id="formatTitle">Formatting Tips</h3>
    </div>
    <div class="modal-body">
      <p>Here are the common uses of Markdown.</p><dl class="formatting-help">
        <dt class="codeblock">Code blocks</dt><dd class="codeblock"><pre><code>~~~
Code surrounded in tildes is easier to read
~~~</code></pre></dd>
        <dt class="urls">Links/URLs</dt><dd class="urls"><code>[Red Hat Customer Portal](https://access.redhat.com)</code></dd>
       </dl>
    </div>
    <div class="modal-footer">
      <a target="_blank" href="https://access.redhat.com/help/markdown" class="btn btn-primary">Learn more</a>
      <button class="btn" data-dismiss="modal" aria-hidden="true">Close</button>
    </div>
  </div></div></div></body>
</html>
